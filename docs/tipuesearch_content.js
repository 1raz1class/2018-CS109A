var tipuesearch = {"pages":[{"title":"Schedule","text":"Week Lecture (Mon) Lecture (Wed) Lab A-Section Assignment (release and due) 1 Lecture 0: What is Data Science? Data, Visualizations Lab 1: Intro to Python (numpy, graphing libraries, program structure, Jupyter Notebook) 2 Lecture 1: Data, Summaries, and Visuals Lecture 2: Data Engineering, Grammar of Data Lab 2: Python: numpy, functions, Pandas, matplotlib 3 Lecture 3: Effective Exploratory Data Analysis and Visualization Lecture 4: Linear Regression, kNN Regression and Inference Lab 3: skLearn and simple Linear Regression A-sec1: 4 Lecture 5: kNN Regression and Linear Regression (reading: ISL section 3.1 (page 58-70)) Lecture 6: Multiple Linear Regression, Polynomial Regression and Model Selection Lab 4: Multi-Regression and Polynimial Regression A-sec2: : 5 Lecture 7: Regularization Lecture 8: High Dimensionality & PCA Lab 5: Regularization and Cross-Validation A-sec3: : 6 No Lecture: Hoiliday Lecture 9: Visualization for Communication No Lab 7 Lecture 10: Logistic Regression I Lecture 11: Logistic Regression II Lab 6: Logistic Regression, PCA A-sec4: : 8 Lecture 12: Neural Networks 1 - Perceptron, Back Propagation Lecture 13: k-NN Classification and Dealing with Missingness Lab 7: Neural Networks (NN1) A-sec5: : 9 Lecture 14: LDA and QDA Lecture 15: Classification Trees Lab 8: Discriminant Analysis & Classification Trees A-sec6:: [CANCELED] 10 Lecture 16: Regression Trees Bagging RF Lecture 17: Boosting Methods Lab 9: Random Forests and Boosting A-sec7: 11 Lecture 18: Neural Networks 2 Lecture 19: Neural Networks 2. NN as a function approximation, universal approximation and design choices Lab 10: NN2 : Introduction to Keras A-sec8: : 12 Lecture 20: SVM No Lecture: Thankgiving No Lab 13 Lecture 21: Stacking Lecture 22: Responsible DS [guest lecture] A-sec9: 14 Lecture 23: AB Testing Lecture 24: Final 15 Reading Period 16 Finals Week","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"CS 109A, STAT 121A, AC 209A, CSCI E-109A Introduction to Data Science Syllabus - Fall 2018 Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year introduction to data science. The course focuses on the analysis of messy, real life data to perform predictions using statistical and machine learning methods. The material of the course is divided 3 modules. Each module will integrate the five key facets of an investigation using data: data collection ‐ data wrangling, cleaning, and sampling to get a suitable data set; data management ‐ accessing data quickly and reliably; exploratory data analysis – generating hypotheses and building intuition; prediction or statistical learning; and communication – summarizing results through visualization, stories, and interpretable summaries. Only one of CS 109a, AC 209a, or Stat 121a can be taken for credit. Students who have previously taken CS 109, AC 209, or Stat 121 cannot take CS 109a, AC 209a, or Stat 121a for credit. Course Logistics Prerequisites You are expected to have programming experience at the level of CS 50 or above, and statistics knowledge at the level of Stat 100 or above (Stat 110 recommended). HW0 is designed to test your knowledge on the prerequisites. Successful completion of this assignment will show that this course is suitable for you. HW0 will not be graded but you are required to submit. Course Components Lectures The class consists of two weekly lectures. Lectures are held Mon and Wed 1:30pm ‐ 2:45 pm in Northwest Building (NW), Lecture Hall B-103. Attendance to lectures is mandatory for FAS students. DCE students can view the recorded lectures within 2 days. We will have in class quizzes to assess your understanding of the material and to help us identify gaps. Labs Labs are designed as hands-on in-class activities.. The instructor will go over practice problems similar to the homework problems and review difficult material. Attendance to labs is optional but strongly encouraged . Two lab sessions with identical content are held Thur 4:30-6:00 pm and Fri 10:30-11:45 am in Pierce 301 . You should plan to attend one of the two. Sections Lectures and labs are supplemented by 1 hour sections led by teaching fellows. There are two types of sections: a) Standard Sections : which will be a mix of review of material and practice problems similar to the HW b) Advanced Sections which will cover advanced topics like the mathematical underpinnings of the methods seen in lecture and lab and extensions of those methods. The material covered in the Advanced Sections is required for all AC 209A students. Instructor Office Hours Pavlos : Monday 3-5pm, IACS lobby Kevin : Monday 3-5pm, IACS lobby Assignments There will be an initial self-assessment homework called HW0 and 9 more graded homework assignments. Some of them will be due in a week and some of them in two weeks. You have the option to work and submit in pairs for all the assignments except two which you will do individually. You will be working in Jupyter Notebooks which you can run in your own environment or in the SEAS JupyterHub cloud ( details on this to come ). Quizzes Quizzes will be taken at the end of class and the material will be based on what was discussed in lecture. DCE students will have 72 hours to complete the quizzes. 40% of the quizzes will be dropped from your grade. Final Project There will be a final group project (2-4 students) due during Exams period. See Calendar for specific dates. Recording Lectures will be recorded and made available real time for DCE students and 24 hours later for in-campus students via Canvas. Labs will be videotaped only for distant students. Recommended Textbook An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani. The book is available here: Free electronic version : http://www-bcf.usc.edu/~gareth/ISL/ (Links to an external site). HOLLIS : http://link.springer.com.ezp-prod1.hul.harvard.edu/book/10.1007%2F978-1-4614-7138-7 Amazon : https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370 (Links to an external site) . Course Policies Getting Help For questions about homework, course content, package installation, JupyterHub, and after you have tried to troubleshoot yourselves, the process to get help is: Post the question in Piazza and hopefully your peers will answer. Note that in Piazza questions are visible to everyone. The TFs monitor the posts. Go to Office Hours , this is the best way to get help. For private matters send an email to the Helpline: cs109a2018@gmail.com. The Helpline is monitored by TFs. For personal and confidential matters send an email to the instructors . Grading Guidelines and Regrading Policy Homework will be graded based on 1) how correct your code is (the Notebook cells should run, we are not troubleshooting code), 2) how you have interpreted the results - we want text not just code, it should be a report, and 3) how well you present the results. The scale is 1-5. For more details, check out The CS109A Grade Questions on Graded Homework and Regrading Policy We take great care in making sure all homework are graded properly. However if you feel that your assignment was not fairly graded you may: Contact the grader by emailing the helpline with subject line \"Regrade HW1: Grader=johnsmith\" within 48 hours of the grade release . If still unhappy with the initial response, then submit a reason via email to the Helpline with subject line \"Regrade HW1: Second request\" within 2 days of receiving the initial response. Note: once regrading is done, you may receive a grade that is higher or lower than the initial grade. Collaboration Policy We encourage you to talk and discuss the assignments with your fellow students (and on Piazza), but you are not allowed to look at any other students assignment or code outside of your pair. Discussion is encouraged, copying is not allowed. Please refer to Academic Honesty in The CS109A Grade link above Late Day Policy Homework is due on Tuesdays. You are allowed 1 late day per homework for a total of 5 late days. Communication from Staff to Students Class announcements will be through Canvas . All homework and quizzes will be posted and submitted in Canvas. Also all feedback forms. NOTE: make sure you have your settings set so you can receive emails from Canvas. Submitting an assignment Instructions for turning in assignments will be posted when the semester starts. Grading Score Your final score for the course will be computed using the following weights: Homework 40% Quizzes 10% Project 50% Total 100% Software We will be using Jupyter Notebooks, Python 3 and various python modules. You can access the notebook viewer either in your own machine by installing the Anaconda platform (Links to an external site) which includes Jupyter/IPython as well all packages that will be required for the course, or by using the SEAS JupyterHub from Canvas. Details in class. Accommodations for students with disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with Kevin by the end of the third week of the term: Friday, September 15. Failure to do so may result in us being unable to respond in a timely manner. All discussions will remain confidential. Academic Honesty Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109 we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to Academic Honesty section in The CS109A Grade link above Syllabus 2018.md Displaying Syllabus 2018.md.","tags":"pages","url":"pages/syllabus.html"},{"title":"CS109a: Introduction to Data Science","text":"Fall 2018 Pavlos Protopapas and Kevin A. Rader pre { background-color: #F5F5F5; display: block; font-family: monospace; font-size: 14px; white-space: pre; border-color: #999999; border-width: 1px; border-style: solid; border-radius: 6px; margin: 1em 0; padding: 5px; white-space: pre-wrap; } .containerMain { display: flex; width: 100%; height: 300px; } .contentA { flex: 1; flex-direction:column; } .contentB { flex: 3; } Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year course to data science. We will focus on the analysis of data to perform predictions using statistical and machine learning methods. Topics include data scraping, data management, data visualization, regression and classification methods, and deep neural networks (see the schedule ). You will get ample practice through weekly homework assignments. The class material integrates the five key facets of an investigation using data: 1. data collection ‐ data wrangling, cleaning, and sampling to get a suitable data set 2. data management ‐ accessing data quickly and reliably 3. exploratory data analysis – generating hypotheses and building intuition 4. prediction or statistical learning 5. communication – summarizing results through visualization, stories, and interpretable summaries Only one of CS 109a, AC 209a, or Stat 121a can be taken for credit. Students who have previously taken CS 109, AC 209, or Stat 121 cannot take CS 109a, AC 209a, or Stat 121a for credit. Lectures: Mon and Wed 1:30‐2:45 pm in Harvard Northwest Building, NW B-103 Labs : Thur 4:30-6:00 pm and Fri 10:30-11:45 am in Pierce 301 (content is identical, students should only attend one) Head TFs : Eleni Kaxiras -DCE Head TF : Sol Girouard Office Hours: IACS student lobby in Maxwell-Dworkin's ground. Just follow the signs. Online Office Hours zoom link: https://harvard-dce.zoom.us/j/7607382317 Class meetings have concluded! Thank you all for a great semester! Guest Lecture on Nov 28th: Ethics and Critical Thinking, Julia Stoyanovich , Assistant Professor in the Department of Computer Science and Engineering at the Tandon School of Engineering, and the Center for Data Science. Course material can be viewed in the public GitHub repository . REGULAR SECTIONS Cover the material presented in class. All 2 sessions are identical. Standard Sections have concluded. Thank you! ADVANCED SECTIONS Cover a different topic per week and are required for 209a students. Advanced Sections have concluded. Thank you! Instructor Office Hours Pavlos : Mon. 4:00-5:00 pm Kevin : Mon. 3:00-4:00 pm. TF Office Hours See the Weekly Schedule","tags":"pages","url":"pages/cs109a-introduction-to-data-science/"},{"title":"Lecture 23: AB testing","text":"Slides PDF PPTX Notes Examples","tags":"lectures","url":"lectures/lecture23/"},{"title":"Lecture 22: Responsible DS","text":"Slides PDF PPTX Notes Examples","tags":"lectures","url":"lectures/lecture22/"},{"title":"Lecture 21: Stacking","text":"Slides PDF PPTX Notes %- Examples","tags":"lectures","url":"lectures/lecture21/"},{"title":"Lecture 20: SVM","text":"Slides PDF PPTX Notes %- Examples","tags":"lectures","url":"lectures/lecture20/"},{"title":"Lecture 19: NN design choices","text":"Slides PDF PPTX Notes PDF PPTX %- Examples","tags":"lectures","url":"lectures/lecture19/"},{"title":"Lecture 18: NN design choices","text":"Slides PDF PPTX Notes %- Examples","tags":"lectures","url":"lectures/lecture18/"},{"title":"Lecture 17: Boosting","text":"Slides PDF PPTX Notes %- Examples","tags":"lectures","url":"lectures/lecture17/"},{"title":"Lecture 16: Regression Trees Bagging RF","text":"Slides PDF PPTX Notes %- Examples","tags":"lectures","url":"lectures/lecture16/"},{"title":"Lecture 15: Classification Trees","text":"Slides PDF PPTX Notes Examples","tags":"lectures","url":"lectures/lecture15/"},{"title":"Lecture 14: Discrimant Analysiss","text":"Slides PDF PPTX Notes Examples","tags":"lectures","url":"lectures/lecture14/"},{"title":"Lecture 13: k-NN Classification and Dealing with Missingness","text":"Slides PDF PPTX Notes Examples","tags":"lectures","url":"lectures/lecture13/"},{"title":"Lecture 12: Perceptron and Back Propagation","text":"Slides PDF PPTX Notes","tags":"lectures","url":"lectures/lecture12/"},{"title":"Lecture 11: Logistic Regression 2","text":"Slides PDF PPTX Notes Examples","tags":"lectures","url":"lectures/lecture11/"},{"title":"Lecture 10: Logistic Regression","text":"Slides PDF PPTX Notes Examples","tags":"lectures","url":"lectures/lecture10/"},{"title":"Lecture 9: Visualization for Communication","text":"Slides PDF Notes","tags":"lectures","url":"lectures/lecture9/"},{"title":"Lab 10: Neural Networks using keras","text":"Lab 10","tags":"labs","url":"labs/lab10/"},{"title":"Lab 7: MLP","text":"Lab 7","tags":"labs","url":"labs/lab7/"},{"title":"Lab 7: MLP","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109A Introduction to Data Science Lab 7: Building a Neural Network and Dealing with missing values Harvard University Fall 2018 Instructors: Pavlos Protopapas and Kevin Rader Lab Instructor: Eleni Kaxiras Authors: David Sondak, Pavlos Protopapas, and Eleni Kaxiras In [ ]: import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg import numpy as np import pandas as pd % matplotlib inline Learning Goals In this lab, we'll revise matrix operations and talk about tensors. We will also talk about handling missing data. By the end of this lab, you should: Understand how a simple neural network works and code its functions from scratch. Know how to use simple pandas and scikit-learn functions to handle missing values. Be able to think of vectors and arrays as tensors. Part 1: Neural Networks 101: Starting with a Single Node The simplest way to describe a neural network is that we have some inputs $x$, which get combined into an auxilliary variable $z$. The auxilliary variable is passed through the activation function $\\sigma\\left(z\\right)$ and the result is the output. Here is another image showing each step. Notice that the inputs are linearly combined according to some weights $w$ and a bias $b$. This transformation is also sometimes called an affine transformation. The perceptron transforms the weighted inputs according to the rule of the activation function. For a single perceptron, the output $y$ is just the output from the perceptron. The linear transformation and activation of the neuron occurs within a single layer of the network (shown in the dotted box). Let's see what the single-layer, single neuron network give us. We have a couple of choices to make: We must choose some weights and some biases We must choose an activation function For now, we will manually specify the weights and biases. We choose a sigmoid activation function $$\\sigma\\left(z\\right) = \\dfrac{1}{1 + e&#94;{-z}}.$$ What are the limits $\\displaystyle\\lim_{z\\to\\infty}\\sigma\\left(z\\right)$ and $\\displaystyle\\lim_{z\\to-\\infty}\\sigma\\left(z\\right)$? Actually, the sigmoid we have here is called the logistic function. Sigmoids are really a family of functions and the logistic function is just one member in that family. In class exercise : Plot the sigmoid Plot the sigmoid In [ ]: # your code here In [ ]: # %load solutions/sigmoid.py def sigmoid ( z ): return 1.0 / ( 1.0 + np . exp ( - z )) Generate a list of 500 $x$ points from -5 to 5 and plot both the sigmoid and the tanh (for tanh you may use np.tanh ) What do you observe? In [ ]: # %load solutions/plot_sig.py x = np . linspace ( - 5.0 , 5.0 , 500 ) # input points print ( x [: 5 ]) plt . plot ( x , sigmoid ( x ), label = 'sigmoid' ) plt . legend () Comments We say that the activation occurs when $\\sigma = \\dfrac{1}{2}$. We can show that this corresponds to $x = -\\dfrac{b}{w}$. The \"steepness\" of the sigmoid is controlled by $w$. In class exercise: Approximate a Gaussian function using a node The task is to approximate (or learn) a function $f\\left(x\\right)$ given some input $x$. For demonstration purposes, the function we will try to learn is a Gaussian function \\begin{align} f\\left(x\\right) = e&#94;{-x&#94;{2}} \\textrm{} \\end{align} Even though we represent the input $x$ as a vector on the computer, you should think of it as a single input. Start by plotting the above function using the $x$ dataset you created earlier In [ ]: f = np . exp ( - x * x ) # The real function, x = np.linspace(-5.0, 5.0, 500) input points plt . plot ( x , f , label = 'gaussian' ) plt . legend () Now, let's code the single node as per the image above. Write a function named affine that does the linear transformation. In [ ]: # your code here def affine ( x , w , b ): \"\"\"Return affine transformation of x INPUTS ====== x: A numpy array of points in x w: A float representing the weight of the perceptron b: A float representing the bias of the perceptron RETURN ====== z: A numpy array of points after the affine transformation z = wx + b \"\"\" # Code goes here return z In [ ]: # your code here In [ ]: # %load solutions/affine.py In [ ]: # your code here In [ ]: # %load solutions/sigmoid.py In [ ]: # your code here w = - 4.5 b = 4. h = sigmoid ( affine ( x , w , b )) In [ ]: # %load solutions/perceptron.py And now we plot the activation function and the true function. In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 11 , 7 )) # create axes object SIZE = 16 # Plot ax . plot ( x , f , ls = '-.' , lw = 4 , label = r 'True function' ) ax . plot ( x , h , lw = 4 , label = r 'Single Neuron' ) # Create labels (very important!) ax . set_xlabel ( '$x$' , fontsize = SIZE ) # Notice we make the labels big enough to read ax . set_ylabel ( '$y$' , fontsize = SIZE ) ax . tick_params ( labelsize = SIZE ) # Make the tick labels big enough to read ax . legend ( fontsize = SIZE , loc = 1 ) # Create a legend and make it big enough to read The single perceptron simply turns the output on and off at some point, but that's about it. We see that the neuron is on until about $x=0$ at which point it abruptly turns off. It's able to get \"close\" to the true function. Otherwise, it has nothing in common with the true function. What do you think will happen if you change $w$ and $b$? Important Observation Notice that we wrote the output as sigmoid(affine(x)) . This was not a coincidence. It looks like a composition of functions. In fact, that is what a neural network is doing. It's building up an approximation to a function by creating a composition of functions. For example, a composition of three functions would be written as $$\\varphi_{3}\\left(\\varphi_{2}\\left(\\varphi_{1}\\left(x\\right)\\right)\\right).$$ What happens if we play with the weights and biases? In [ ]: w = [ - 5.0 , 0.1 , 5.0 ] # Create a list of weights b = [ 0.0 , - 1.0 , 1.0 ] # Create a list of biases fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 10 )) ax . plot ( x , f , lw = 4 , ls = '-.' , label = 'True function' ) for wi , bi in zip ( w , b ): h = sigmoid ( affine ( x , wi , bi )) ax . plot ( x , h , lw = 4 , label = r '$w = {0} $, $b = {1} $' . format ( wi , bi )) ax . set_title ( 'Single neuron network' , fontsize = SIZE ) # Create labels (very important!) ax . set_xlabel ( '$x$' , fontsize = SIZE ) # Notice we make the labels big enough to read ax . set_ylabel ( '$y$' , fontsize = SIZE ) ax . tick_params ( labelsize = SIZE ) # Make the tick labels big enough to read ax . legend ( fontsize = SIZE , loc = 'best' ) # Create a legend and make it big enough to read We didn't do an exhaustive search of the weights and biases, but it sure looks like this single perceptron is never going to match the actual function. Again, we shouldn't be suprised about this. The output layer of the network is simple the logistic function, which can only have so much flexibility. Let's try to make our network more flexible by using more nodes ! Multiple Perceptrons in a Single Layer It appears that a single neuron is somewhat limited in what it can accomplish. What if we expand the number of nodes/neurons in our network? We have two obvious choices here. One option is to add depth to the network by putting layers next to each other. The other option is to stack neurons on top of each other in the same layer. Now the network has some width, but is still only one layer deep. The following figure shows a single-layer network with two nodes in one layer. Some observations We still have a single input in this case. Note that this is not necessary in general. We're just keeping things simple with a single input for now. If we have more inputs we will have a matrix for $X$. Each node (or neuron) has a weight and bias associated with it. An affine transformation is performed for each node. Both nodes use the same activation function form $\\sigma\\left(\\cdot\\right)$ on their respective inputs. The outputs of the nodes must be combined to give the overall output of the network. There are a variety of ways of accomplishing this. In the current example, we just take a linear combination of the node outputs to produce the actual prediction. Notice that now we have weights and biases at the output too. Let's see what happens in this case. First, we just compute the outputs of each neuron. In [ ]: x = np . linspace ( - 5.0 , 5.0 , 500 ) # input points f = np . exp ( - x * x ) # data w = np . array ([ 3.5 , - 3.5 ]) b = np . array ([ 3.5 , 3.5 ]) # Affine transformations z1 = w [ 0 ] * x + b [ 0 ] z2 = w [ 1 ] * x + b [ 1 ] # Node outputs h1 = 1.0 / ( 1.0 + np . exp ( - z1 )) h2 = 1.0 / ( 1.0 + np . exp ( - z2 )) Now let's plot things and see what they look like. In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 10 )) ax . plot ( x , f , lw = 4 , ls = '-.' , label = 'True function' ) ax . plot ( x , h1 , lw = 4 , label = 'First neuron' ) ax . plot ( x , h2 , lw = 4 , label = 'Second neuron' ) # Set title ax . set_title ( 'Comparison of Neuron Outputs' , fontsize = SIZE ) # Create labels (very important!) ax . set_xlabel ( '$x$' , fontsize = SIZE ) # Notice we make the labels big enough to read ax . set_ylabel ( '$y$' , fontsize = SIZE ) ax . tick_params ( labelsize = SIZE ) # Make the tick labels big enough to read ax . legend ( fontsize = SIZE , loc = 'best' ) # Create a legend and make it big enough to read Just as we expected. Some sigmoids. Of course, to get the network prediction we must combine these two sigmoid curves somehow. First we'll just add $h_{1}$ and $h_{2}$ without any weights to see what happens. Note We are not doing classification here. We are trying to predict an actual function. The sigmoid activation is convenient when doing classification because you need to go from $0$ to $1$. However, when learning a function, we don't have as good of a reason to choose a sigmoid. In [ ]: # Network output wout = np . ones ( 2 ) # Set the output weights to unity to begin bout = - 1 # No bias yet yout = wout [ 0 ] * h1 + wout [ 1 ] * h2 + bout And plot. In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 10 )) ax . plot ( x , f , ls = '-.' , lw = 4 , label = r 'True function' ) ax . plot ( x , yout , lw = 4 , label = r '$y_ {out} = h_ {1} + h_ {2} $' ) # Create labels (very important!) ax . set_xlabel ( '$x$' , fontsize = SIZE ) # Notice we make the labels big enough to read ax . set_ylabel ( '$y$' , fontsize = SIZE ) ax . tick_params ( labelsize = SIZE ) # Make the tick labels big enough to read ax . legend ( fontsize = SIZE , loc = 'best' ) # Create a legend and make it big enough to read Observations The network prediction is still not good. But , it is pretty sophisticated. We just have two neurons, but we get some pretty interesting behavior. We didn't do anything with the output weights. Those are probably important. Now let's see what happens when we change the weights on the output. In [ ]: # Network output wout = np . array ([ - 1.5 , - 1.5 ]) bout = np . array ( 1.5 ) yout = wout [ 0 ] * h1 + wout [ 1 ] * h2 + bout In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 10 )) ax . plot ( x , f , lw = 4 , ls = '-.' , label = 'True function' ) ax . plot ( x , yout , lw = 4 , label = r '$y_ {out} = w_ {1} &#94; {o} h_ {1} + w_ {2} &#94; {o} h_ {2} + b&#94; {o} $' ) # Create labels (very important!) ax . set_xlabel ( '$x$' , fontsize = SIZE ) # Notice we make the labels big enough to read ax . set_ylabel ( '$y$' , fontsize = SIZE ) ax . tick_params ( labelsize = SIZE ) # Make the tick labels big enough to read ax . legend ( fontsize = SIZE , loc = 1 ) # Create a legend and make it big enough to read Very cool! The two nodes interact with each other to produce a pretty complicated-looking function. It still doesn't match the true function, but now we have some hope. In fact, it's starting to look a little bit like a Gaussian! We can do better. There are three obvious options at this point: Change the number of nodes Change the activation functions Change the weights Some Mathematical Notation Before proceeding, let's learn a more succint way of doing the calculations. If you have a network with a lot of nodes, then you probably don't want to manually determine the output of each node. It will take forever. Instead, you can package the computations up using a more compact notation. We'll illustrate the ideas with the two-node network. Suppose we have a single input $x$ to a single-layer two-node network. We can store the weights from each node in a vector $\\mathbf{w} \\in \\mathbb{R}&#94;{2}$. Similarly, we can store the biases from each node in a vector $\\mathbf{b} \\in \\mathbb{R}&#94;{2}$. The affine transformation is then written as $$\\mathbf{z} = \\mathbf{w}x + \\mathbf{b}$$ where the usual laws of vector addition and multiplication by a scalar apply. Of course, we have that $\\mathbf{z} \\in \\mathbb{R}&#94;{2}$ as well. Next we evaluate the output from each node. Formally, we write $$\\mathbf{h} = \\sigma\\left(\\mathbf{z}\\right)$$ where, once again, $\\mathbf{h}\\in\\mathbb{R}&#94;{2}$. Moreover, it is understood that $\\sigma$ operates on each individual element of $\\mathbf{z}$ separately. If we denote each component of $\\mathbf{z}$ by $z_{j}, \\quad j = 1, 2$ then we can write $$h_{j} = \\sigma\\left(z_{j}\\right), \\quad j = 1, 2.$$ Lastly, we must do something about the output layer. Mathematically we write $$y_{out} = \\mathbf{w}_{out} \\cdot \\mathbf{h} + b_{out}$$ where $\\mathbf{w}_{out} \\in \\mathbb{R}&#94;{2}$ and $b_{out} \\in \\mathbb{R}$. Part 2. Handling Missing Variables Imputation in pandas Pandas has chosen to use the floating point NaN internally to denote missing data and that was largely for simplicity and performance reasons. The library will also recognize the Python None as \"missing\" or \"not available\" or \"NA\". When doing descriptive statistics, pandas excludes missing values by default. In [ ]: s = pd . Series ([ 0 , 1 ,[] , np . nan , 4 , 6 , 8 , 15 , 23 , 25 , np . nan ]) s In [ ]: # look for missing values s . isna () # or s.isnull() pandas can interpolate according to different methods. In [ ]: # how many? s . isna () . sum () In [ ]: s . isnull () . sum () In [ ]: # fill with zeros s . fillna ( 0 ) # s['one'].fillna('missing') In [ ]: # You may wish to simply exclude labels from a data set which refer to missing data. To do this, use s = pd . Series ([ 0 , 1 , np . nan , 4 , 6 , 8 , 15 , 23 , 25 , np . nan ]) s = s . dropna () s In [ ]: s = pd . Series ([ 0 , 1 , np . nan , 4 , 6 , 8 , 15 , 23 , 25 , np . nan , 27 ]) s . interpolate () Imputation in scikit-learn In [ ]: from sklearn.impute import SimpleImputer imp = SimpleImputer ( copy = True , missing_values = np . nan , strategy = 'mean' ) s1 = pd . Series ([ 0 , 1 , np . nan , 4 , 6 , 8 , 15 , 23 , 25 , np . nan ]) s2 = pd . Series ([ 0 , 1 , np . nan , 4 , 6 , 8 , 15 , 23 , 25 , np . nan ]) d = { 'col1' : s1 , 'col2' : s2 } df = pd . DataFrame ( data = d ) df In [ ]: imp . fit ( df ) df = imp . transform ( df ) df In [ ]: SimpleImputer ( copy = True , fill_value = None , missing_values = np . nan , strategy = 'mean' , verbose = 0 ) print ( imp . transform ( df )) In [ ]: imp2 = SimpleImputer ( strategy = \"most_frequent\" ) s1 = pd . Series ([ 'eleni' , 'john' , np . nan , 'john' , 'mary' , 'john' , 'george' , 'eleni' , 'john' , np . nan ]) d = { 'name' : s1 } df = pd . DataFrame ( data = d ) df In [ ]: imp2 . fit ( df ) df = imp2 . transform ( df ) df Part 3. Reese Witherspoon as a rank 3 Tensor We can think of tensors as multidimensional arrays of numerical values; their job is to generalize matrices to multiple dimensions. While tensors first emerged in the 20th century, they have since been applied to numerous other disciplines, including machine learning. Tensor decomposition/factorization can solve, among other, problems in unsupervised learning settings, temporal and multirelational data. When we get to handling images for Convolutional Neural Networks, it's a good idea to have the understanding of tensors of rank 3. We will use the following naming conventions: scalar = just a number = rank 0 tensor ($a$ ∈ $F$,) vector = 1D array = rank 1 tensor ( $x = (\\;x_1,...,x_i\\;)⊤$ ∈ $F&#94;n$ ) matrix = 2D array = rank 2 tensor ( $\\textbf{X} = [a_{ij}] ∈ F&#94;{m×n}$ ) 3D array = rank 3 tensor ( $\\mathscr{X} =[t_{i,j,k}]∈F&#94;{m×n×l}$ ) $\\mathscr{N}$D array = rank $\\mathscr{N}$ tensor ( $\\mathscr{T} =[t_{i1},...,t_{i\\mathscr{N}}]∈F&#94;{n_1×...×n_\\mathscr{N}}$ ) Tensor indexing We can create subarrays by fixing some of the given tensor's indices. We can create a vector by fixing all but one index. A 2D matrix is created when fixing all but two indices. For example, for a third order tensor the vectors are $\\mathscr{X}[:,j,k]$ = $\\mathscr{X}[j,k]$ (column), $\\mathscr{X}[i,:,k]$ = $\\mathscr{X}[i,k]$ (row), and $\\mathscr{X}[i,j,:]$ = $\\mathscr{X}[i,j]$ (tube); the matrices are $\\mathscr{X}[:,:,k]$ (frontal), $\\mathscr{X}[:,j,:]$ (lateral), $\\mathscr{X}[i,:,:]$ (horizontal). Tensor multiplication We can multiply one matrix with another as long as the sizes are compatible ((n × m) × (m × p) = n × p), and also multiply an entire matrix by a constant. Numpy numpy.dot performs a matrix multiplication which is straightforward when we have 2D or 1D arrays. But what about > 3D arrays? The function will choose according to the matching dimentions but if we want to choose we should use tensordot , but, again, we do not need tensordot for this class. (photo from Labeled Faces in the Wild ) In [ ]: # load and show the image img = mpimg . imread ( 'Reese_Witherspoon.jpg' ) imgplot = plt . imshow ( img ) What type is this image? We have a suspicion that it's an array so let's find out its shape. In [ ]: type ( img ), img . shape In [ ]: img It's a 24-bit RGB PNG image (height, width, channels) with 8 bits for each of R, G, B. Explore and print the array. In [ ]: img [:,:,: 3 ] . shape # selects all rows, 1st column, and all slices In [ ]: img [ 130 ] In [ ]: img [ 130 ][ 100 ] . shape In [ ]: img [ 130 , 100 ] . shape In [ ]: # we are down to a single pixel img [ 130 ][ 3 ][ 0 ] In [ ]: type ( img ) Slicing tensors: slice along each axis In [ ]: R_img = img [:,:, 0 ] G_img = img [:,:, 1 ] B_img = img [:,:, 2 ] plt . imshow ( R_img ) plt . colorbar () In [ ]: plt . imshow ( G_img ) plt . colorbar () In [ ]: plt . imshow ( B_img ) plt . colorbar () In [ ]: # practice matrix by scalar img = img * 2 plt . imshow ( img ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab7/notebook/"},{"title":"Lab 8: Discriminant Analysis","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS 109A/STAT 121A/AC 209A/CSCI E-109A Discriminant Analysis, a Tale of 2 cities. Harvard University Fall 2018 Instructors: Pavlos Protopapas, Kevin Rader Work by Rahul Dave and Margo Levine. Table of Contents Learning Goals Temporal patterns in urban demographics Geographic patterns in urban demographics Part 0: Learning Goals In this lab we'll work with demographics of a region of the cities of Pavlopolis and Kevinsville from the years 2000 to 2010. We'll use the data to predict household economic status from its geographical location. By the end of this lab, you will be able to: Implement different classifiers and calculate predictive accuracy of these classifiers, Compare different classifiers for general predictive accuracy. In [1]: import numpy as np import pandas as pd import scipy as sp from scipy.stats import mode from sklearn import linear_model import matplotlib import matplotlib.pyplot as plt from sklearn import discriminant_analysis as da from sklearn import preprocessing from sklearn.neighbors import KNeighborsClassifier as KNN from sklearn.model_selection import train_test_split % matplotlib inline Predicting Urban Demographic Changes Part 1: Temporal patterns in urban demographics For this first part of lab, we aim to build a model for the city of Pavlopolis that accurately predicts household economic status from its geographical location. We want the model to be robust, that is, the model should be accurate over many years (through 2010 and beyond). The data is contained in dataset_1_year_2000.txt , ..., dataset_1_year_2010.txt . The first two columns of each dataset are the adjusted latitude and longitude of some randomly sampled houses. The last column contains economic status of a household: 0: low-income, 1: middle-class, 2: high-income You are given some additional information Due to the migration of people in and out of the city, the distribution of each economic group over this region changes over the years. The city of Pavlopolis estimates that in this region there is approximately a 25% yearly increase in high-income households; and a 25% decrease in the remaining population, with the decrease being roughly the same amongst both the middle class and lower income households. Visualization We start by importing and visualizing the data. In [2]: #Load one file, check the data type, check that the data matches the description above data2000_glance = np . loadtxt ( 'datasets/dataset_1_year_2000.txt' ) print ( type ( data2000_glance )) data2000_glance [: 10 , :] Out[2]: array([[ 0.54432757, 0.6245105 , 2. ], [ 0.59468476, 0.72391292, 2. ], [ 0.70018033, 0.78249208, 2. ], [ 0.60126202, 0.97181171, 2. ], [ 0.63199522, 0.74850249, 2. ], [ 0.49427826, 0.60880108, 2. ], [ 0.6096247 , 0.67672357, 2. ], [ 0.76899244, 0.86092404, 2. ], [ 0.59429045, 0.82042128, 2. ], [ 0.45593116, 0.74428878, 2. ]]) In [3]: #Visualize data for every other year fig , ax = plt . subplots ( 2 , 3 , figsize = ( 12 , 5 )) #Create index for subplots ind = 0 #Iterate from year 0 to 10, with steps of 2 for i in range ( 0 , 11 , 2 ): #Load data data = np . loadtxt ( 'datasets/dataset_1_year_' + str ( 2000 + i ) + '.txt' ) #Split into predictor/response x = data [:, : - 1 ] y = data [:, - 1 ] #Plot each class for the current year in different colors ax [ ind // 3 , ind % 3 ] . scatter ( x [ y == 2 , 0 ], x [ y == 2 , 1 ], color = 'green' , alpha = 0.4 , label = '2-High income' ) ax [ ind // 3 , ind % 3 ] . scatter ( x [ y == 1 , 0 ], x [ y == 1 , 1 ], color = 'blue' , alpha = 0.4 , label = '1-Middle class' ) ax [ ind // 3 , ind % 3 ] . scatter ( x [ y == 0 , 0 ], x [ y == 0 , 1 ], color = 'red' , alpha = 0.4 , label = '0-Low income' ) # LABEL AXIS, TITLE ax [ ind // 3 , ind % 3 ] . set_xlabel ( 'Latitude' ) ax [ ind // 3 , ind % 3 ] . set_ylabel ( 'Longitude' ) ax [ ind // 3 , ind % 3 ] . set_title ( str ( 2000 + i )) #Update index ind = ind + 1 plt . legend ( bbox_to_anchor = ( 1 , 1 ), loc = 'upper left' , ncol = 1 ) plt . tight_layout () EXERCISE: What do you observe from the data visualizations? Model comparison Let's compare the the following four classification schemes (on predictive accuracy) for addressing the problem of predicting household economic status from geographic location: Training a logistic regression model on one year and use it repeatedly Training a Q/LDA model on one year and use it repeatedly Training a KNN model on one year and use it repeatedly Using Bayes Theorem to update LDA/QDA classifiers on a year-by-year basis We'll now train three single models (logistic regression, LDA, and KNN) once, and explore the accuracy of these models over time. EXERCISE: Fill in the code below to fit a logistic regression (variable logreg , with C=1000), KNN (variable knn , for the default k=5), and lda (variable lda ) models on the 2000 data. In [31]: #Load data for year 2000 data = np . loadtxt ( 'datasets/dataset_1_year_2000.txt' ) #Split predictors, response x = data [:, : - 1 ] y = data [:, - 1 ] #your code begins here #your code ends here logregtrain = logreg . score ( x , y ) ldatrain = lda . score ( x , y ) knntrain = knn . score ( x , y ) acc_log = [] acc_lda = [] acc_knn = [] #Iterate through all the years for i in range ( 1 , 11 ): #Load data for year 2000 + i data_i = np . loadtxt ( 'datasets/dataset_1_year_' + str ( 2000 + i ) + '.txt' ) #Split predictors, response x_i = data_i [:, : - 1 ] y_i = data_i [:, - 1 ] #Compute predictive accuracies of various models trained on 2000 data acc_log . append ( logreg . score ( x_i , y_i )) acc_lda . append ( lda . score ( x_i , y_i )) acc_knn . append ( knn . score ( x_i , y_i )) #Plot accuracy over years years = np . arange ( 1 , 11 ) + 2000 # x-axis is years 2001-2010 width = 0.25 # width of bar plt . figure ( figsize = ( 10 , 3 )) plt . bar ( years , acc_log , width , color = 'blue' , alpha = 0.5 , label = 'LogReg' ) plt . axhline ( logregtrain , 0 , 1 , color = \"blue\" , label = \"LogReg 2000\" ) plt . bar ( years + width , acc_lda , width , color = 'red' , alpha = 0.5 , label = 'LDA' ) plt . axhline ( ldatrain , 0 , 1 , color = \"red\" , label = \"LDA 2000\" ) plt . bar ( years + 2 * width , acc_knn , width , color = 'green' , alpha = 0.5 , label = 'KNN, k=5' ) plt . axhline ( knntrain , 0 , 1 , color = \"green\" , label = \"KNN 2000\" ) #Labels plt . xlabel ( 'Year' ) plt . ylabel ( 'Prediction Accuracy' ) plt . legend ( loc = 'upper left' , bbox_to_anchor = ( 1 , 1 )) # legend upper left outside plt . xticks ( years + width + 0.125 , years ); # set x-ticks spacing EXERCISE: Are these three models robust? That is, do these models perform well over the years? What about Overfitting? Bias? What do you see? We'll now consider the remaining modeling option: Training a Q/LDA model yearly What does it mean to train a QDA or LDA model for each year? Recall that Q/LDA classifiers first model the distribution of the data within each class using a normal distribution, $\\mathcal{N}(\\mu, \\Sigma)$. Then the Q/LDA models the distribution of the data amongst the classes. For us, since we have three classes, this means that our \"model\" consists of the following 9 pieces of information (three for each class, two to define the distribution of data in the class and one to define the class proportion): Class 0: Distribution: $p(x | y = 0) = \\mathcal{N}(\\mu_0, \\Sigma_0)$ Proportion: $p(y=0)=\\pi_0 = \\frac{\\text{data in Class 0}}{\\text{total number of data}}$ Class 1: Distribution: $p(x | y = 1) = \\mathcal{N}(\\mu_1, \\Sigma_1)$ Proportion: $p(y=1)=\\pi_1 = \\frac{\\text{data in Class 1}}{\\text{total number of data}}$ Class 2: Distribution: $p(x | y = 2) = \\mathcal{N}(\\mu_2, \\Sigma_2)$ Proportion: $p(y=2)=\\pi_2 = \\frac{\\text{data in Class 2}}{\\text{total number of data}}$ Retraining a Q/LDA model means re-estimating $\\mu, \\Sigma$ and $\\pi$ for each class. EXERCISE: At the beginning of lab we were given information on the projected yearly growth rate of the classes, so we don't need the entire set of population data to re-estimate $\\pi$ for each class. Use this information to write down estimates for $\\pi_2&#94;{\\text{year i + 1}}$. Then, assuming that the total population is approximately constant over the years, estimate $\\pi_0&#94;{\\text{year i + 1}}$ and $\\pi_1&#94;{\\text{year i + 1}}$. On the other hand, if the distribution of the data within each class changes (i.e. $\\mu$ and $\\Sigma$ changes within each class) then we will need re-estimate them using new data each year. Let's check, using data visualization, to see if this is indeed the case. In [8]: #Visualize data for every other year fig , ax = plt . subplots ( 3 , 6 , figsize = ( 23 , 10 )) #Iterate from year 0 to 10, with steps of 2 for i in range ( 0 , 11 , 2 ): #Load data data = np . loadtxt ( 'datasets/dataset_1_year_' + str ( 2000 + i ) + '.txt' ) #Split into predictor/response x = data [:, : - 1 ] y = data [:, - 1 ] #Plot each class for the current year in different colors ax [ 0 , i // 2 ] . scatter ( x [ y == 2 , 0 ], x [ y == 2 , 1 ], color = 'g' , alpha = 0.6 , label = '2-High income' ) ax [ 1 , i // 2 ] . scatter ( x [ y == 1 , 0 ], x [ y == 1 , 1 ], color = 'b' , alpha = 0.6 , label = '1-Middle class' ) ax [ 2 , i // 2 ] . scatter ( x [ y == 0 , 0 ], x [ y == 0 , 1 ], color = 'r' , alpha = 0.6 , label = '0-Low income' ) #Labels ax [ 0 , i // 2 ] . set_xlabel ( 'Latitude' ) ax [ 0 , i // 2 ] . set_ylabel ( 'Longitude' ) ax [ 0 , i // 2 ] . set_title ( str ( 2000 + i )) #Labels ax [ 1 , i // 2 ] . set_xlabel ( 'Latitude' ) ax [ 1 , i // 2 ] . set_ylabel ( 'Longitude' ) ax [ 1 , i // 2 ] . set_title ( str ( 2000 + i )) # LABEL AXIS, TITLE ax [ 2 , i // 2 ] . set_xlabel ( 'Latitude' ) ax [ 2 , i // 2 ] . set_ylabel ( 'Longitude' ) ax [ 2 , i // 2 ] . set_title ( str ( 2000 + i )) plt . legend ( bbox_to_anchor = ( 1 , 1 ), loc = 'upper left' , ncol = 1 ) plt . tight_layout () EXERCISE: Comment on how the distribution in each class appears to change over the years. Does this effect the efficiency of Q/LDA models? So how efficient is our Q/LDA model? We train the model once on a single year's worth of data. Then, for any subsequent year, the update is just a few lines of arithmetic. For example, say we train our Q/LDA model on 2000 data, given a point from 2001, $x&#94;{2001}$, we predict the class label for $x&#94;{2001}$ by: find the probability that $x&#94;{2001}$ will be labeled Class 0 to 2 using our model from 2000: $$ \\begin{aligned} p_{2000}(y=0 | x&#94;{2001}) &= p_{2000}(x&#94;{2001} | y=0) p_{2000}(y=0) = \\mathcal{N}(x&#94;{2001}; \\mu_0, \\Sigma_0) \\pi_0&#94;{2000}\\\\ p_{2000}(y=1 | x&#94;{2001}) &= p_{2000}(x&#94;{2001} | y=1) p_{2000}(y=1) = \\mathcal{N}(x&#94;{2001}; \\mu_1, \\Sigma_1) \\pi_1&#94;{2000}\\\\ p_{2000}(y=2 | x&#94;{2001}) &= p_{2000}(x&#94;{2001} | y=2) p_{2000}(y=2) = \\mathcal{N}(x&#94;{2001}; \\mu_2, \\Sigma_2) \\pi_2&#94;{2000} \\end{aligned} $$ Now note that we are assuming that $p(data | class)$ is constant from year to year, but the PRIOR $p(class)$ is changing. So let us adjust the probabilities predicted using the 2000 model, by updating $\\pi_i$ with 2001 class proportions: $$ \\begin{aligned} p_{2001}(y=0 | x&#94;{2001}) &= p_{2000}(y=0 | x&#94;{2001}) \\frac{\\pi_0&#94;{2001}}{\\pi_0&#94;{2000}}\\\\ p_{2001}(y=1 | x&#94;{2001}) &= p_{2000}(y=1 | x&#94;{2001}) \\frac{\\pi_1&#94;{2001}}{\\pi_1&#94;{2000}}\\\\ p_{2001}(y=2 | x&#94;{2001}) &= p_{2000}(y=2 | x&#94;{2001}) \\frac{\\pi_2&#94;{2001}}{\\pi_2&#94;{2000}}\\\\ \\end{aligned} $$ take the maximum of $\\{p_{2001}(y=0 | x&#94;{2001}), p_{2001}(y=1 | x&#94;{2001}), p_{2001}(y=2 | x&#94;{2001}) \\}$ and predict the class label corresponding to the maximum probability. EXERCISE: Fill in the code below to re-estimate the class proportions (see equations from two exercises ago). In [11]: #Load data for year 2000 data = np . loadtxt ( 'datasets/dataset_1_year_2000.txt' ) #Split predictors, response x = data [:, : - 1 ] y = data [:, - 1 ] #Fit a lda model on year 2000 data lda = da . LinearDiscriminantAnalysis () lda . fit ( x , y ) ldatrain = lda . score ( x , y ) acc_lda_corrected = [] # Store corrected LDA accuracies #Store class proportions in 2000 p0_2000 = ( y == 0 ) . mean () p1_2000 = ( y == 1 ) . mean () p2_2000 = ( y == 2 ) . mean () #Store class proportions in current year p0_current = ( y == 0 ) . mean () p1_current = ( y == 1 ) . mean () p2_current = ( y == 2 ) . mean () for k in range ( 1 , 11 ): #Load data data_i = np . loadtxt ( 'datasets/dataset_1_year_' + str ( 2000 + k ) + '.txt' ) #Split into predictor, response x_i = data_i [:, : - 1 ] y_i = data_i [:, - 1 ] #x_train, x_test, y_train, y_test = train_test_split(x_i, y_i, test_size=0.30, random_state=0) #your code begins here #Re-estimate class proportions (25% increase in p2, adjust p0, p1 accordingly) #your code ends here #Re-estimate class label probabilities pred_logprob = lda . predict_log_proba ( x_i ) # compute log-probabilities pred_logprob [:, 0 ] = pred_logprob [:, 0 ] + np . log ( p0_current / p0_2000 ) # correction for class 0 pred_logprob [:, 1 ] = pred_logprob [:, 1 ] + np . log ( p1_current / p1_2000 ) # correction for class 1 pred_logprob [:, 2 ] = pred_logprob [:, 2 ] + np . log ( p2_current / p2_2000 ) # correction for class 2 #Predict class label using re-estimated probabilities y_pred = pred_logprob . argmax ( axis = 1 ) #Compute accuracy acc_lda_corrected . append ( np . mean ( y_i == y_pred )) #Plot accuracy over years years = np . arange ( 1 , 11 ) + 2000 # x-axis is years 2001-2010 width = 0.25 # width of bar plt . figure ( figsize = ( 10 , 3 )) plt . bar ( years + width , acc_lda , width , color = 'red' , alpha = 0.5 , label = 'LDA' ) plt . bar ( years + 2 * width , acc_lda_corrected , width , color = 'green' , alpha = 0.5 , label = 'LDA yearly' ) #Labels plt . xlabel ( 'Year' ) plt . ylabel ( 'Prediction Accuracy' ) plt . legend ( loc = 'upper left' , bbox_to_anchor = ( 1 , 1 )) # legend upper left outside plt . title ( 'Overall predictive accuracy of LDA models' ) plt . xticks ( years + width + 0.125 , years ); # set x-ticks spacing plt . axhline ( ldatrain , 0 , 1 , color = \"red\" , label = \"LDA 2000\" ) Out[11]: Compared to training a model once and using it repeatedly, training the Q/LDA yearly performs better in terms of predictive accuracy. It isn't significantly more expensive though. Note: Why LDA and not QDA? Well since we've already observed that the distributions of data within the three classes are very simliar (in terms of shape) we can assume that the covariance matrices for all three classes will be the same - hence our model is LDA rather than QDA Conclusions: Based on the data visualization from the first 5 years, it's clear that while the class proportions are changing, the class distributions are not (the center and shape of the three classes are constant through time). Thus, updating our LDA on a yearly basis is simple, as this just requires us to adjust our estimates of the class proportions $\\pi$. Part 2: Geographic patterns in urban demographics In dataset_2.txt and dataset_3.txt you have the demographic information for a random sample of houses in two regions in Kevinsville. There are only two economic brackets for the households in these datasets: 0: low-income or middle-class, 1: high-income. The data for each region is shown in the visualizations below. In [12]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 3 )) data = np . loadtxt ( 'datasets/dataset_2.txt' ) x = data [:, 0 : - 1 ] y = data [:, - 1 ] # Plot data ax [ 0 ] . scatter ( x [ y == 1 , 0 ], x [ y == 1 , 1 ], color = 'b' , alpha = 0.2 ) ax [ 0 ] . scatter ( x [ y == 0 , 0 ], x [ y == 0 , 1 ], color = 'r' , alpha = 0.2 ) # Label axes, set title ax [ 0 ] . set_title ( 'dataset_2' ) ax [ 0 ] . set_xlabel ( 'Latitude' ) ax [ 0 ] . set_ylabel ( 'Longitude' ) data = np . loadtxt ( 'datasets/dataset_3.txt' ) x = data [:, 0 : - 1 ] y = data [:, - 1 ] # Plot data ax [ 1 ] . scatter ( x [ y == 1 , 0 ], x [ y == 1 , 1 ], color = 'b' , alpha = 0.2 ) ax [ 1 ] . scatter ( x [ y == 0 , 0 ], x [ y == 0 , 1 ], color = 'r' , alpha = 0.2 ) # Label axes, set title ax [ 1 ] . set_title ( 'dataset_3' ) ax [ 1 ] . set_xlabel ( 'Latitude' ) ax [ 1 ] . set_ylabel ( 'Longitude' ) plt . show () EXERCISE: For each region, comment on the appropriateness of using KNN for classification. Below is code for visualizing decision boundaries. In [27]: #-------- plot_decision_boundary # A function that visualizes the data and the decision boundaries # Input: # x (predictors) # y (labels) # poly_flag (a boolean parameter, fits quadratic model if true, otherwise linear) # title (title for plot) # ax (a set of axes to plot on) # Returns: # ax (axes with data and decision boundaries) def plot_decision_boundary ( x , y , model , poly_flag , title , ax ): # Plot data ax . scatter ( x [ y == 1 , 0 ], x [ y == 1 , 1 ], c = 'red' , label = 'Normal' , cmap = plt . cm . coolwarm , alpha = 0.2 , s = 20 ) ax . scatter ( x [ y == 0 , 0 ], x [ y == 0 , 1 ], c = 'blue' , label = 'Normal' , cmap = plt . cm . coolwarm , alpha = 0.2 , s = 20 ) # Create mesh interval = np . arange ( 0 , 1 , 0.01 ) n = np . size ( interval ) x1 , x2 = np . meshgrid ( interval , interval ) x1 = x1 . reshape ( - 1 , 1 ) x2 = x2 . reshape ( - 1 , 1 ) xx = np . concatenate (( x1 , x2 ), axis = 1 ) # Predict on mesh points if ( poly_flag ): quad_features = preprocessing . PolynomialFeatures ( degree = 2 ) xx = quad_features . fit_transform ( xx ) yy = model . predict ( xx ) yy = yy . reshape (( n , n )) # Plot decision surface x1 = x1 . reshape ( n , n ) x2 = x2 . reshape ( n , n ) ax . contourf ( x1 , x2 , yy , cmap = plt . cm . coolwarm , alpha = 0.1 ) # Label axes, set title ax . set_title ( title ) ax . set_xlabel ( 'Latitude' ) ax . set_ylabel ( 'Longitude' ) return ax In [28]: #-------- fit_and_plot_models # A function that visualizes the data and the decision boundaries # Input: # file_name (name of the file containing dataset) # Returns: # None def fit_and_plot_models ( file_name ): data = np . loadtxt ( file_name ) x = data [:, 0 : - 1 ] y = data [:, - 1 ] fig , ax = plt . subplots ( 1 , 4 , figsize = ( 15 , 3 )) # Plain Logistic Regression logreg = linear_model . LogisticRegression () logreg . fit ( x , y ) acc_logreg = logreg . score ( x , y ) str_title = 'LogReg (acc = ' + str ( acc_logreg ) + ')' ax [ 0 ] = plot_decision_boundary ( x , y , logreg , False , str_title , ax [ 0 ]) # LDA lda = da . LinearDiscriminantAnalysis () lda . fit ( x , y ) acc_lda = lda . score ( x , y ) str_title = 'LDA (acc = ' + str ( acc_lda ) + ')' ax [ 1 ] = plot_decision_boundary ( x , y , lda , False , str_title , ax [ 1 ]) # QDA qda = da . QuadraticDiscriminantAnalysis () qda . fit ( x , y ) acc_qda = qda . score ( x , y ) str_title = 'QDA (acc = ' + str ( acc_qda ) + ')' ax [ 2 ] = plot_decision_boundary ( x , y , qda , False , str_title , ax [ 2 ]) # Logistic Regression with Quadratic Terms quad_features = preprocessing . PolynomialFeatures ( degree = 2 ) x_expanded = quad_features . fit_transform ( x ) logreg_poly = linear_model . LogisticRegression ( C = 1000 ) logreg_poly . fit ( x_expanded , y ) acc_logreg_poly = logreg_poly . score ( x_expanded , y ) str_title = 'LogReg-poly (acc = ' + str ( acc_logreg_poly ) + ')' ax [ 3 ] = plot_decision_boundary ( x , y , logreg_poly , True , str_title , ax [ 3 ]) plt . tight_layout () plt . show () EXERCISE: Choose between Q/LDA and logistic regression by visualizing various types decision boundaries. Discuss. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab8/notebook/"},{"title":"Lecture 0: Notebook","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS109A Introduction to Data Science Lecture 0: Example Harvard University Summer 2018 Instructors : Pavlos Protopapas and Kevin Rader In [2]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING from IPython.core.display import HTML def css_styling (): styles = open ( \"../../../styles/cs109.css\" , \"r\" ) . read () return HTML ( styles ) css_styling () Out[2]: h1 { padding-top: 25px; padding-bottom: 25px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } h2 { padding-top: 10px; padding-bottom: 10px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } div.exercise { background-color: #ffcccc; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; } div.theme { background-color: #DDDDDD; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 18pt; } p.q1 { padding-top: 5px; padding-bottom: 5px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } header { padding-top: 35px; padding-bottom: 35px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } In [3]: import pandas as pd import sys import numpy as np import scipy as sp import matplotlib.pyplot as plt from math import radians , cos , sin , asin , sqrt import datetime from sklearn.linear_model import LinearRegression import seaborn as sns sns . set ( style = \"ticks\" ) % matplotlib inline Download the data from https://drive.google.com/open?id=0B28c493CP9GtMzN1emFoMkJNNlU First Look At The Data In [10]: hubway_data_file = '~/Downloads/hubway_data/hubway_trips.csv' hubway_data = pd . read_csv ( hubway_data_file , low_memory = False ) hubway_data . head () Out[10]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } seq_id hubway_id status duration start_date strt_statn end_date end_statn bike_nr subsc_type zip_code birth_date gender 0 1 8 Closed 9 7/28/2011 10:12:00 23.0 7/28/2011 10:12:00 23.0 B00468 Registered '97217 1976.0 Male 1 2 9 Closed 220 7/28/2011 10:21:00 23.0 7/28/2011 10:25:00 23.0 B00554 Registered '02215 1966.0 Male 2 3 10 Closed 56 7/28/2011 10:33:00 23.0 7/28/2011 10:34:00 23.0 B00456 Registered '02108 1943.0 Male 3 4 11 Closed 64 7/28/2011 10:35:00 23.0 7/28/2011 10:36:00 23.0 B00554 Registered '02116 1981.0 Female 4 5 12 Closed 12 7/28/2011 10:37:00 23.0 7/28/2011 10:37:00 23.0 B00554 Registered '97214 1983.0 Female Who? In [4]: year_to_age = lambda s : 0 if 'N' in s else 2017 - int ( s ) In [15]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 6 )) gender_counts = np . unique ( hubway_data [ 'gender' ] . replace ( np . nan , 'NaN' , regex = True ) . values , return_counts = True ) ax [ 0 ] . bar ( range ( 3 ), gender_counts [ 1 ], align = 'center' , color = [ 'black' , 'green' , 'teal' ], alpha = 0.5 ) ax [ 0 ] . set_xticks ([ 0 , 1 , 2 ]) ax [ 0 ] . set_xticklabels ([ 'none' , 'male' , 'female' , ' ' ]) ax [ 0 ] . set_title ( 'Users by Gender' ) age_col = 2017.0 - hubway_data [ 'birth_date' ] . dropna () . values age_counts = np . unique ( age_col , return_counts = True ) ax [ 1 ] . bar ( age_counts [ 0 ], age_counts [ 1 ], align = 'center' , width = 0.4 , alpha = 0.6 ) ax [ 1 ] . axvline ( x = np . mean ( age_col ), color = 'red' , label = 'average age' ) ax [ 1 ] . axvline ( x = np . percentile ( age_col , 25 ), color = 'red' , linestyle = '--' , label = 'lower quartile' ) ax [ 1 ] . axvline ( x = np . percentile ( age_col , 75 ), color = 'red' , linestyle = '--' , label = 'upper quartile' ) ax [ 1 ] . set_xlim ([ 1 , 90 ]) ax [ 1 ] . set_xlabel ( 'Age' ) ax [ 1 ] . set_ylabel ( 'Number of Checkouts' ) ax [ 1 ] . legend () ax [ 1 ] . set_title ( 'Users by Age' ) plt . tight_layout () plt . savefig ( 'Who.png' , dpi = 300 ) Where In [6]: station_data = pd . read_csv ( 'hubway_stations.csv' , low_memory = False )[[ 'id' , 'lat' , 'lng' ]] station_data . head () Out[6]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } id lat lng 0 3 42.340021 -71.100812 1 4 42.345392 -71.069616 2 5 42.341814 -71.090179 3 6 42.361285 -71.065140 4 7 42.353412 -71.044624 In [7]: hubway_data_with_gps = hubway_data . join ( station_data . set_index ( 'id' ), on = 'strt_statn' ) hubway_data_with_gps . head () Out[7]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } seq_id hubway_id status duration start_date strt_statn end_date end_statn bike_nr subsc_type zip_code birth_date gender lat lng 0 1 8 Closed 9 7/28/2011 10:12:00 23 7/28/2011 10:12:00 23.0 B00468 Registered '97217 1976.0 Male 42.359677 -71.059364 1 2 9 Closed 220 7/28/2011 10:21:00 23 7/28/2011 10:25:00 23.0 B00554 Registered '02215 1966.0 Male 42.359677 -71.059364 2 3 10 Closed 56 7/28/2011 10:33:00 23 7/28/2011 10:34:00 23.0 B00456 Registered '02108 1943.0 Male 42.359677 -71.059364 3 4 11 Closed 64 7/28/2011 10:35:00 23 7/28/2011 10:36:00 23.0 B00554 Registered '02116 1981.0 Female 42.359677 -71.059364 4 5 12 Closed 12 7/28/2011 10:37:00 23 7/28/2011 10:37:00 23.0 B00554 Registered '97214 1983.0 Female 42.359677 -71.059364 When In [8]: #check_out_times = pd.to_datetime(hubway_data['start_date']) check_out_hours = hubway_data [ 'start_date' ] . apply ( lambda s : int ( s [ - 8 : - 6 ])) In [9]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) check_out_counts = np . unique ( check_out_hours , return_counts = True ) ax . bar ( check_out_counts [ 0 ], check_out_counts [ 1 ], align = 'center' , width = 0.4 , alpha = 0.6 ) ax . set_xlim ([ - 1 , 24 ]) ax . set_xticks ( range ( 24 )) ax . set_xlabel ( 'Hour of Day' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . set_title ( 'Time of Day vs Checkouts' ) plt . show () How In [10]: def haversine ( pt , lat2 = 42.355589 , lon2 =- 71.060175 ): \"\"\" Calculate the great circle distance between two points on the earth (specified in decimal degrees) \"\"\" lon1 = pt [ 0 ] lat1 = pt [ 1 ] # convert decimal degrees to radians lon1 , lat1 , lon2 , lat2 = map ( radians , [ lon1 , lat1 , lon2 , lat2 ]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * asin ( sqrt ( a )) r = 3956 # Radius of earth in miles return c * r In [11]: station_counts = np . unique ( hubway_data_with_gps [ 'strt_statn' ] . dropna (), return_counts = True ) counts_df = pd . DataFrame ({ 'id' : station_counts [ 0 ], 'checkouts' : station_counts [ 1 ]}) counts_df = counts_df . join ( station_data . set_index ( 'id' ), on = 'id' ) counts_df . head () Out[11]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } checkouts id lat lng 0 9734 3 42.340021 -71.100812 1 18058 4 42.345392 -71.069616 2 10630 5 42.341814 -71.090179 3 23322 6 42.361285 -71.065140 4 9163 7 42.353412 -71.044624 In [12]: counts_df . loc [:, 'dist_to_center' ] = list ( map ( haversine , counts_df [[ 'lng' , 'lat' ]] . values )) counts_df . head () Out[12]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } checkouts id lat lng dist_to_center 0 9734 3 42.340021 -71.100812 2.335706 1 18058 4 42.345392 -71.069616 0.853095 2 10630 5 42.341814 -71.090179 1.802423 3 23322 6 42.361285 -71.065140 0.467803 4 9163 7 42.353412 -71.044624 0.807582 In [16]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . scatter ( counts_df [ 'dist_to_center' ] . values , counts_df [ 'checkouts' ] . values ) reg_line = LinearRegression () reg_line . fit ( counts_df [ 'dist_to_center' ] . values . reshape (( len ( counts_df [ 'dist_to_center' ]), 1 )), counts_df [ 'checkouts' ] . values ) distances = np . linspace ( counts_df [ 'dist_to_center' ] . min (), counts_df [ 'dist_to_center' ] . max (), 50 ) ax . plot ( distances , reg_line . predict ( distances . reshape (( len ( distances ), 1 ))), color = 'red' , label = 'Regression Line' ) ax . set_xlabel ( 'Distance to City Center (Miles)' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . set_title ( 'Distance to City Center vs Checkouts' ) ax . legend () plt . savefig ( 'How.png' , dpi = 300 ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lecture0/notebook/"},{"title":"Lecture 8:  High Dimensionality & PCA","text":"Slides PDF PPTX Notes Examples","tags":"lectures","url":"lectures/lecture8/"},{"title":"Lecture 7: Pandas and Data Scraping","text":"Slides PDF PPTX","tags":"lectures","url":"lectures/lecture7/"},{"title":"Lecture 6: Multiple Linear, Polynomical Regression","text":"Slides PDF PPTX","tags":"lectures","url":"lectures/lecture6/"},{"title":"Lecture 5: Multiple Linear Regression","text":"Slides PDF PPTX Notebooks Examples","tags":"lectures","url":"lectures/lecture5/"},{"title":"Lecture 3: Effective Exploratory Data Analysis and Visualization","text":"Slides PDF Notebooks Examples","tags":"lectures","url":"lectures/lecture3/"},{"title":"Lecture 2","text":"Lecture 2","tags":"pages","url":"pages/lecture2/"},{"title":"Lecture 2: Data Engineering","text":"Slides PDF PPTX Notebooks Examples","tags":"lectures","url":"lectures/lecture2/"},{"title":"Lecture 1: Data, Summaries, and Visuals","text":"Slides PDF Notebooks Examples","tags":"lectures","url":"lectures/lecture1/"},{"title":"Lab 6:","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109A Introduction to Data Science Lab 6: Missing Data, Classification and Dimensionality Reduction Harvard University Fall 2018 Instructors: Pavlos Protopapas and Kevin Rader Lab Instructors: Pavlos Protopapas. Contributors: Will Claybaugh and David Sondak Zoom I will be using zoom during this lab. You can connect to the room using the link https://harvard-dce.zoom.us/j/7607382317 and when you are ready to share your screen let me know and can share your screen. In [74]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[74]: h1 { padding-top: 25px; padding-bottom: 25px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } h2 { padding-top: 10px; padding-bottom: 10px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } div.exercise { background-color: #ffcccc; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; } div.theme { background-color: #DDDDDD; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 18pt; } p.q1 { padding-top: 5px; padding-bottom: 5px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } header { padding-top: 35px; padding-bottom: 35px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } Learning Goals In this lab, we'll explore different models used to predict which of several labels applies to a new datapoint based on labels observed in the training data. We'll similarly explore PCA as a technique for reducing the number of features in a dataset with as little loss of structure as possible. By the end of this lab, you should: Be familiar with the sklearn implementations of Logistic Regression Be able to make an informed choice of model based on the data at hand Be familiar with the SKlearn implementation of Principle Components Analysis (PCA) Be able to select an appropriate number of PCA components (Bonus) Structure your sklearn code into Pipelines to make building, fitting, and tracking your models easier (Bonus) Apply weights to each class in the model to achieve your desired tradeoffs between discovery and false alarm in various classes In [76]: % matplotlib inline import numpy as np import scipy as sp import matplotlib.pyplot as plt import pandas as pd pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.notebook_repr_html' , True ) from sklearn.model_selection import train_test_split Part 1: The Wine Dataset The dataset contains 11 chemical features of various wines, along with experts' rating of that wine's quality. The quality scale technically runs from 1-10, but only 3-9 are actually used in the data. Our goal will be to distinguish good wines from bad wines based on their chemical properties. Read-in and checking We do the usual read-in and verification of the data: In [75]: wines_df = pd . read_csv ( \"data/wines.csv\" , index_col = 0 ) wines_df . head () Out[75]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality red good 0 8.9 0.590 0.50 2.0 0.337 27.0 81.0 0.99640 3.04 1.61 9.5 6 1 0 1 7.7 0.690 0.22 1.9 0.084 18.0 94.0 0.99610 3.31 0.48 9.5 5 1 0 2 8.8 0.685 0.26 1.6 0.088 16.0 23.0 0.99694 3.32 0.47 9.4 5 1 0 3 11.4 0.460 0.50 2.7 0.122 4.0 17.0 1.00060 3.13 0.70 10.2 5 1 0 4 8.8 0.240 0.54 2.5 0.083 25.0 57.0 0.99830 3.39 0.54 9.2 5 1 0 In [77]: wines_df . describe () Out[77]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality red good count 1000.000000 1000.000000 1000.00000 1000.000000 1000.000000 1000.00000 1000.00000 1000.000000 1000.000000 1000.000000 1000.000000 1000.000000 1000.00000 1000.000000 mean 7.558400 0.397455 0.30676 4.489250 0.067218 25.29650 91.03100 0.995351 3.251980 0.572990 10.489433 5.796000 0.50000 0.189000 std 1.559455 0.189923 0.16783 4.112419 0.046931 17.06237 59.57269 0.002850 0.164416 0.169583 1.151195 0.844451 0.50025 0.391705 min 3.800000 0.080000 0.00000 0.800000 0.009000 1.00000 6.00000 0.987400 2.740000 0.280000 8.500000 3.000000 0.00000 0.000000 25% 6.500000 0.260000 0.22000 1.800000 0.042000 12.00000 37.75000 0.993480 3.140000 0.460000 9.500000 5.000000 0.00000 0.000000 50% 7.200000 0.340000 0.30000 2.400000 0.060000 22.00000 86.00000 0.995690 3.240000 0.550000 10.300000 6.000000 0.50000 0.000000 75% 8.200000 0.520000 0.40000 6.100000 0.080000 35.00000 135.00000 0.997400 3.360000 0.650000 11.300000 6.000000 1.00000 0.000000 max 15.500000 1.580000 1.00000 26.050000 0.611000 131.00000 313.00000 1.003690 3.900000 2.000000 14.000000 8.000000 1.00000 1.000000 Building the training/test data As usual, we split the data before we begin our analysis. Today, we take the 'quality' variable as our target. There's a debate to be had about the best way to handle this variable. It has 10 categories (1-10), though only 3-9 are used. While the variable is definitely ordinal- we can put the categories in an order everyone agrees on- the variable probably isn't a simple numeric feature; it's not clear whether the gap between a 5 and a 6 wine is the same as the gap between an 8 and a 9. Ordinal regression is one possibility for our analysis (beyond the scope of this course), but we'll view the quality variable as categorical. Further, we'll simplify it down to 'good' and 'bad' wines (quality at or above 7, and quality at or below 6, respectively). This binary column already exists in the data, under the name 'good'. In [78]: wines_train , wines_test = train_test_split ( wines_df , test_size = 0.2 , random_state = 8 , stratify = wines_df [ 'good' ]) x_train = wines_train . drop ([ 'quality' , 'good' ], axis = 1 ) y_train = wines_train [ 'good' ] x_test = wines_test . drop ([ 'quality' , 'good' ], axis = 1 ) y_test = wines_test [ 'good' ] x_train . head () Out[78]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol red 744 7.6 0.30 0.37 1.6 0.087 27.0 177.0 0.99438 3.09 0.50 9.8 0 51 7.6 0.29 0.49 2.7 0.092 25.0 60.0 0.99710 3.31 0.61 10.1 1 213 13.2 0.46 0.52 2.2 0.071 12.0 35.0 1.00060 3.10 0.56 9.0 1 883 8.6 0.33 0.34 11.8 0.059 42.0 240.0 0.99882 3.17 0.52 10.0 0 98 7.7 0.41 0.76 1.8 0.611 8.0 45.0 0.99680 3.06 1.26 9.4 1 Now that we've split, let's explore some patterns in the data In [80]: from pandas.plotting import scatter_matrix scatter_matrix ( wines_train , figsize = ( 30 , 20 )); It looks like there aren't any particularly strong correlations among the predictors (maybe sulfur dioxide and free sulfur dioxide) so we're safe to keep them all. It also looks like the different quality categories have roughly the same distribution of most variables, with volatile/fixed acidity and alcohol seeming like the most useful predictors. Part 2 (Introduction): Binary Logistic Regression Linear regression is usually a good baseline model, but since the outcome we're trying to predict only takes values 0 and 1 we'll want to use logistic regression instead of basic linear regression. We'll begin with statsmodels , because cs109 likes confidence intervals and checking that coefficients make sense. In [81]: import statsmodels.api as sm sm_fitted_logit = sm . Logit ( y_train , sm . add_constant ( x_train )) . fit () #sm_fitted_logit.summary() ### ORIGINAL VERSION. GAVE AttributeError: module 'scipy.stats' has no attribute 'chisqprob' sm_fitted_logit . summary2 () ### WORKS Optimization terminated successfully. Current function value: 0.372167 Iterations 10 Out[81]: Model: Logit Pseudo R-squared: 0.232 Dependent Variable: good AIC: 621.4673 Date: 2018-10-18 00:50 BIC: 682.3672 No. Observations: 800 Log-Likelihood: -297.73 Df Model: 12 LL-Null: -387.52 Df Residuals: 787 LLR p-value: 5.1997e-32 Converged: 1.0000 Scale: 1.0000 No. Iterations: 10.0000 Coef. Std.Err. z P>|z| [0.025 0.975] const 296.7882 178.7587 1.6603 0.0969 -53.5724 647.1488 fixed acidity 0.4228 0.1962 2.1546 0.0312 0.0382 0.8073 volatile acidity -3.6301 1.1006 -3.2983 0.0010 -5.7872 -1.4729 citric acid -0.8217 1.0638 -0.7724 0.4399 -2.9066 1.2633 residual sugar 0.1628 0.0759 2.1454 0.0319 0.0141 0.3115 chlorides -18.8903 8.1523 -2.3172 0.0205 -34.8684 -2.9121 free sulfur dioxide 0.0026 0.0089 0.2864 0.7746 -0.0150 0.0201 total sulfur dioxide -0.0036 0.0038 -0.9597 0.3372 -0.0110 0.0038 density -312.9457 182.0829 -1.7187 0.0857 -669.8215 43.9302 pH 1.5112 1.1276 1.3402 0.1802 -0.6989 3.7212 sulphates 2.5571 0.8610 2.9700 0.0030 0.8696 4.2446 alcohol 0.4945 0.2190 2.2580 0.0239 0.0653 0.9236 red 0.6751 0.6490 1.0403 0.2982 -0.5968 1.9470 Let's talk about the output: First, \"optimization terminated successfully\". Recall that linear regression and its simple formula for the optimal betas is a rarity in machine learning and statistics: most models are fit to the data algorithmically, not via a formula. This message is letting us know that the algorithm seems to have worked. Second, the pseudo $R&#94;2$ is rather low (.23). As with regular $R&#94;2$, we might take this as a sign that the model is struggling. Finally, let's look at the coefficients. Several of the coefficients are statistically significant, including Fixed acidity - good Volatile Acidity - bad Residual Sugar - good (judge have a sweet tooth?) Chlorides - bad Sulphates - good Alcohol - good (judges like getting drunk?) The rest only reach a coefficient size we would often observe by chance alone, without any actual effect from the predictor More formal interpretations are of coefficients are long-winded. \"A one unit increase in alcohol (holding all else constant) results in a predicted 0.494 increase in the log odds of a wine being classified as good\". We can't be more precise because the effect of one unit of alcohol depends on how much alcohol there already is. The one unit increase/decrease matters more if the wine is otherwise on the border between good and bad. If the wine is undrinkable (in the far left tail of the sigmoidal curve) one unit of alcohol barely moves the probability, while if the wine is in the middle of the curve that unit of acidity has much more practical impact. Discussion Are there any bones you'd like to pick with the model I've laid out? Can you think of a better logistic regression model? Prediction One of the really cool features of logistic regression is that it hands back probabilities of a given case being 1 or 0, rather than just 1s and 0s. That lets us do neat things like set different cutoffs for what counts as a 1 and do ROC analysis and so on. Here, we'll just set the cutoff at 0.5: if a 1 is reported as more likely, predict a 1. (You can play with the cutoff yourself and see if you can make the model do better by trading false positives and false negatives). Because this is statsmodels, we'll need to import a tool or do the test set score calculation ourselves. Here, it's easy enough to implement: do the predictions compare with .5 find out what percentage of our binary predictions matched the truth In [82]: sm_binary_prediction = sm_fitted_logit . predict ( sm . add_constant ( x_test )) >= . 5 np . sum ( y_test == sm_binary_prediction ) / len ( y_test ) Out[82]: 0.80500000000000005 Wow! 80% is a pretty good performance! We can pretty much tell the bad wines from the good. Here's a sanity check: In [83]: np . sum ( y_test == 0 ) / len ( y_test ) Out[83]: 0.81000000000000005 Oh... no... wait. A model that says \"all wines are bad\" also scores 80% on the test set. Our fancy model isn't really doing that well. Moral of the story : Before you congratulate a model, think of a truly trivial model to compare it to. Exercise 1 Re-create the results above but this time work with sklearn . Use the LogisticRegression class. Follow the usual .fit , .score procedure. To match statsmodel 's coefficient values (roughly), you will need to adjust the input parameters: C solver One other parameter See the sklearn documentation Hint: statsmodels uses a Newton-Raphson method to optimize the beta values. In [84]: from sklearn.linear_model import LogisticRegression print ( \"target: \\n {} \" . format ( sm_fitted_logit . params )) #fitted_lr = LogisticRegression(C=___, solver=___, ___) target: const 296.788181 fixed acidity 0.422755 volatile acidity -3.630081 citric acid -0.821665 residual sugar 0.162801 chlorides -18.890258 free sulfur dioxide 0.002563 total sulfur dioxide -0.003620 density -312.945669 pH 1.511186 sulphates 2.557134 alcohol 0.494461 red 0.675074 dtype: float64 Answer : In [11]: # your code here In [12]: # %load solutions/sklearn_logistic.py Speaker note: When presenting solution, model reading the documentation from the webpage. How does one know where to look? Speaker note: Mention the wide variety of solvers and how (some) use different levels of derivatives to converge in fewer steps The Decision Boundary One powerful way to think about classification models is to consider where and how they draw the line between predicting \"class A\" and \"class B\". The code below lets you play with a 2d logistic regression. Points towards yellow will be predicted as 1s, points towards violet will be predicted as 0s. In [13]: from scipy.special import expit def plot_logistic_contour ( beta0 , betaX , betaY , betaXY = 0 , betaX2 = 0 , betaY2 = 0 ): delta =. 1 x_values = np . arange ( - 3.0 , 3.0 , delta ) y_values = np . arange ( - 3.0 , 3.0 , delta ) x_grid , y_grid = np . meshgrid ( x_values , y_values ) logistic_output = expit ( beta0 + betaX * x_grid + betaY * y_grid + betaXY * x_grid * y_grid + betaX2 * x_grid ** 2 + betaY2 * y_grid ** 2 ) contour_figure = plt . contour ( x_grid , y_grid , logistic_output ) plt . clabel ( contour_figure , inline = 1 , fontsize = 10 ); plt . xlim ( - 3 , 3 ) plt . ylim ( - 3 , 3 ) plt . show () #plot_logistic_contour(beta0=1, betaX=2, betaY=3, betaXY=0, betaY2=.1) In [14]: # Use this cell to experiment plot_logistic_contour ( beta0 = 1 , betaX = 2 , betaY = 3 ) Exercise 2 What happens to the decision boundary as the coefficient on X increases? What happens if you increase the Y coefficient to match? What does the constant term control? What impact do higher-order and interaction terms have on the boundary? What parameter settings should I show the class? Answers : your answer here In [15]: # %load solutions/boundaries.txt Part 3 (The Real Challenge): Multiclass Classification Before we move on, let's consider a more common use case of logistic regression: predicting not just a binary variable, but what level a categorical variable will take. Instead of breaking the quality variable into 'good' and 'other', let's discretize into 'good, 'medium', and 'bad'. In [36]: # copy the original data so that we're free to make changes wines_df_recode = wines_df . copy () # use the 'cut' function to reduce a variable down to particular bins. Here the lowest bin is 0-4, next is 5-7, # and the last is 7-10 wines_df_recode [ 'quality' ] = pd . cut ( wines_df_recode [ 'quality' ],[ 0 , 4 , 7 , 10 ], labels = [ 0 , 1 , 2 ]) # drop the un-needed columns x_data = wines_df_recode . drop ([ 'quality' , 'good' ], axis = 1 ) y_data = wines_df_recode [ 'quality' ] x_train , x_test , y_train , y_test = train_test_split ( x_data , y_data , test_size =. 2 , random_state = 8 , stratify = y_data ) print ( wines_df [ 'quality' ] . head ()) print ( wines_df_recode [ 'quality' ] . head ()) The cut function obviously stores a lot of extra information for us. It's a very useful tool for discretizing an existing variable. Exercise 3 Adapt your earlier logistic regression code to fit to the new training data. What is stored in .coef_ and .intercept_ ? How well does this model predict the test data? Put the model's performance in context. Think of a trivial model to compare to, and provide its accuracy score on the test set. Answers : 1. In [17]: # your code here In [18]: # %load solutions/multi_logistic.py your answer here In [19]: # %load solutions/multi_logistic.txt 2. In [20]: # your code here In [21]: # %load solutions/score1.py your answer here 3. In [22]: # make a dumb prediction that always guesses 1, the most common class # your code here In [23]: # %load solutions/trivial_model.py your solution here But, a trivial model that guesses the most likely class also does really well on the test set, too. In [24]: # %load solutions/3.3.txt Summary Logistic regression extends OLS to work naturally with a dependent variable that's only ever 0 and 1. In fact, Logistic regression is even more general and is used for predicting the probability of an example belonging to each of $N$ classes. The code for the two cases is identical and just like LinearRegression : .fit , .score , and all the rest Significant predictors does not imply that the model actually works well. Signifigance can be driven by data size alone. The data aren't enough to do what we want Warning : Logistic regression tries to hand back valid probabilities. As with all models, you can't trust the results until you validate them- if you're going to use raw probabilities instead of just predicted class, take the time to verify that if you pool all cases where the model says \"I'm 30% confident it's class A\" 30% of them actually are class A. Part 4: Dimensionality Reduction Our models are clearly struggling, but it's hard to tell why. Let's PCA to shrink the problem down to 2d (with as little loss as possible) and see if that gives us a clue about what makes this problem tough. In [25]: from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler # scale the datasets scale_transformer = StandardScaler ( copy = True ) . fit ( x_train ) x_train_scaled = scale_transformer . transform ( x_train ) x_test_scaled = scale_transformer . transform ( x_test ) # reduce dimensions pca_transformer = PCA ( 2 ) . fit ( x_train_scaled ) x_train_2d = pca_transformer . transform ( x_train_scaled ) x_test_2d = pca_transformer . transform ( x_test_scaled ) print ( x_train_2d . shape ) x_train_2d [ 0 : 5 ,:] Some comments: Both scaling and reducing dimension follow the same pattern: we fit the object to the training data, then use .transform to convert the training and test data. This ensures that, for instance, we scale the test data using the training mean and variance, not its own mean and variance We need to equalize the variance of each feature before applying PCA, otherwise certain dimensions will dominate the scaling: our PCA dimensions would just be the features with the largest spread. In [26]: ## plot each group # notice that we set up lists to track each group's plotting color and label colors = [ 'r' , 'c' , 'b' ] label_text = [ \"Bad Wines\" , \"Medium Wines\" , \"Good Wines\" ] # and we loop over the different groups for cur_quality in [ 0 , 1 , 2 ]: cur_df = x_train_2d [ y_train == cur_quality ] plt . scatter ( cur_df [:, 0 ], cur_df [:, 1 ], c = colors [ cur_quality ], label = label_text [ cur_quality ]) # all plots need labels plt . xlabel ( \"PCA Dimension 1\" ) plt . ylabel ( \"PCA Dimention 2\" ) plt . legend (); Well, that gives us some idea of why the problem is difficult: the good wines and bad wines are hiding right among the average wines. It does look like the wines separate into two groups, though, possibly one for reds and one for whites. Exercise 4 What critique can you make against the plot above? Why does this plot not prove that the different wines are hopelessly similar? The wine data we've used so far consist entirely of continuous predictors. Would PCA work with categorical data? Answer : your answer here In [27]: # %load solutions/4.txt Exercise 5 Edit the code above to plot the locations of red wines and white wines Answer : In [28]: # your code here Evaluating PCA - Variance Explained One of the criticisms we made of the PCA plot was that it's lost something from the original data. Let's actually investigate how much of the original data's structure the 2d PCA captures. We'll look at the explained_variance_ratio_ portion of the PCA fit. This lists, in order, the percentage of the x data's total variance that is captured by the nth PCA dimension. In [29]: var_explained = pca_transformer . explained_variance_ratio_ print ( \"Variance explained by each PCA component:\" , var_explained ) print ( \"Total Variance Explained:\" , np . sum ( var_explained )) The first PCA dimension captures 33% of the variance in the data, and the second PCA dimension adds another 20%. Together, we've got about half of the total variation in the training data covered with just these two dimensions. Exercise 6 Fit a PCA that finds the first 10 PCA components of our training data Use np.cumsum to print out the variance we'd be able to explain by using n PCA dimensions for n=1 through 10 Does the 10-dimension PCA agree with the 2d PCA on how much variance the first components explain? Do the 10d and 2d PCAs find the same first two dimensions? Why or why not? Make a plot of number of PCA dimensions against total variance explained. What PCA dimension looks good to you? Hint: np.cumsum stands for 'cumulative sum', so np.cumsum([1,3,2,-1,2]) is [1,4,6,5,7] Answer : In [30]: #your code here 3. your answer here In [31]: # %load solutions/6.3.txt 4. In [32]: #your code here In [33]: # %load solutions/6.4.py A PCA dimension of 3, 4, or 5 looks good to me. These values are roughly where we hit diminishing returns on variance explained. Plots like the one above are called 'Scree' or 'Elbow' plots. They are often used to heuristically select a good number of PCA dimensions. Summary PCA maps a high-dimensional space into a lower dimensional space. The PCA dimensions are ordered by how much of the original data's variance they capture There are other cool and useful properties of the PCA dimensions (orthogonal, etc.). See a textbook . PCA on a given dataset always gives the same dimensions in the same order. You can select the number of dimensions by fitting a big PCA and examining a plot of the cumulative variance explained. Part 5: Did we fail? None of the models worked, and we can't tell good wines from bad. Was it all a waste of time and money? Not really. All analyses are a roll of the dice. Some analyses fail, like this one did, becuase the data at hand just don't support the task we've set out. What can we do about it? Be honest about the methods and the null result. Lots of analyses fail. Collect a dataset you think has a better chance of success. Maybe we collected the wrong chemical signals... Keep trying new approaches. Just beware of overfitting the data you're validating on. Always have a test set locked away for when the final model is built. Change the question. Maybe something you noticed during analysis seems interesting or useful (classifying red versus white). But again, you the more you try, the more you might overfit, so have test data locked away. Just move on. If the odds of success start to seem small, maybe you need a new project. The Moral of the Lab Sometimes, the data just aren't enough to adequately predict outcomes. In this lab we saw that no amount of modeling finesse would let us use a wine's chemical properties to tell good wines and bad wines from mediocre ones. The chemical properties were very good at telling red wines from whites, however. PCA helped us visualize the data and confirm that the highly rated wines just aren't chemically distinct from the other wines. NOT ALL ANALYSES YIELD USEFUL RESULTS Sometimes (arguably most of the time), the data aren't suitable for a task or just don't have anything interesting to say. Part 6 (Sidebar): Weighting the training points Some models can accept weights on the training points to given them greater priority in the model's fitting process. This can be useful if, for instance, certain classes are rare but we want to be sure the model classifies them correctly (e.g. we're trying to classify cancers and one form is rare but very aggressive). In general, weighting training points is like moving along the ROC curve; we change some model parameters to alter the mistakes the model makes to be more in line with our tastes. Let's see this in action with a logistic regression: In [39]: # copy the original data so that we're free to make changes wines_df_recode = wines_df . copy () # use the 'cut' function to reduce a variable down to particular bins. Here the lowest bin is 0-4, next is 5-7, # and the last is 7-10 wines_df_recode [ 'quality' ] = pd . cut ( wines_df_recode [ 'quality' ],[ 0 , 4 , 7 , 10 ], labels = [ 0 , 1 , 2 ]) # drop the un-needed columns x_data = wines_df_recode . drop ([ 'quality' , 'good' ], axis = 1 ) y_data = wines_df_recode [ 'quality' ] x_train , x_test , y_train , y_test = train_test_split ( x_data , y_data , test_size =. 2 , random_state = 8 , stratify = y_data ) print ( wines_df [ 'quality' ] . head ()) print ( wines_df_recode [ 'quality' ] . head ()) In [49]: unweighted_lr = LogisticRegression ( C = 1000000 ) . fit ( x_train , y_train ) weight_dict = { 0 : 1000 , 1 : 1 , 2 : 100 } weighted_lr = LogisticRegression ( C = 1000000 , class_weight = weight_dict ) . fit ( x_train , y_train ) In [50]: from sklearn.metrics import confusion_matrix print ( \"Rows: True Lables (Bad, Medium, Good), \\n Colummns: Predicted Lables (Bad, Medium, Good)\" ) print () print ( \"unweighted:\" ) print ( confusion_matrix ( y_test , unweighted_lr . predict ( x_test ))) print ( \"weighted:\" ) print ( confusion_matrix ( y_test , weighted_lr . predict ( x_test ))) Without weighting, the model plays it safe and predicts that all of the test set wines are medium. With weighting, the model is told to care more about getting the bad and good wines right. The model does as we've asked and correctly IDs 3 good/bad test wines, at the price of 17 falsely bad wines and 16 falsely good wines. However, if identifying bad and good wines is, as implied, 100 times more important than identifying medium wines, we've made a really good trade. Exercise 7 What happens if you give a weight of 0 to the medium wines? What weighting gives results that accord with your personal sense of what the model should be doing? How many actually-medium bottles is a single good bottle worth? Answers : The model learns a classification rule that never predicts 'medium'. It's as it we dropped the medium wines from training. 100, 1, 100 looks the best to me. We get a 1-in-8 sucess rate on the wines flagged as good. However, I found these values by looking at the test set confusion matrix; it's not clear they'd maintain the 1-in-8 ratio on new data. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab6/notebook/"},{"title":"Advanced Section 1: First Lecture","text":"Slides PDF Notebooks Examples","tags":"lectures","url":"lectures/a-sec1/"},{"title":"Lecture 0: First Lecture","text":"Slides PDF Notebooks Examples","tags":"lectures","url":"lectures/lecture0/"},{"title":"Lecture 1","text":"Lecture 1","tags":"pages","url":"pages/lecture1/"},{"title":"Lab 3: KNN Regression, Simple Linear Regression","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS109A/STAT121A Introduction to Data Science Lab 3: KNN Regression, Simple Linear Regression Harvard University Fall 2018 Instructors: Pavlos Protopapas and Kevin Rader Material Preparation: David Sondak, Will Claybaugh, Eleni Kaxiras. Run the cell below to properly highlight the exercises In [ ]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Table of Contents Array creation and reshape Some plotting Simple linear regression $k$-nearest neighbors Learning Goals Overall description and goal for the lab. By the end of this lab, you should be able to: Understand array reshaping Review how to make plots Feel comfortable with simple linear regression Feel comfortable with $k$ nearest neighbors This lab corresponds to lecture 4 and maps on to homework 2 (and beyond). In [8]: # import the necessary libraries import warnings warnings . filterwarnings ( 'ignore' ) % matplotlib inline import numpy as np import scipy as sp import matplotlib as mpl import matplotlib.cm as cm import matplotlib.pyplot as plt import pandas as pd import time pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.notebook_repr_html' , True ) import seaborn as sns Simple Linear Regression Linear regression and its many extensions are a workhorse of the statistics and data science community, both in application and as a reference point for other models. Most of the major concepts in machine learning can be and often are discussed in terms of various linear regression models. Thus, this section will introduce you to building and fitting linear regression models and some of the process behind it, so that you can 1) fit models to data you encounter 2) experiment with different kinds of linear regression and observe their effects 3) see some of the technology that makes regression models work. Linear regression with a toy dataset We first examine a toy problem, focusing our efforts on fitting a linear model to a small dataset with three observations. Each observation consists of one predictor $x_i$ and one response $y_i$ for $i = 1, 2, 3$, \\begin{align*} (x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}. \\end{align*} To be very concrete, let's set the values of the predictors and responses. \\begin{equation*} (x , y) = \\{(1, 2), (2, 2), (3, 4)\\} \\end{equation*} There is no line of the form $\\beta_0 + \\beta_1 x = y$ that passes through all three observations, since the data are not collinear. Thus our aim is to find the line that best fits these observations in the least-squares sense , as discussed in lecture. Exercise (10 min) Make two numpy arrays out of this data, x_train and y_train Check the dimentions of these arrays Try to reshape them into a different shape Make points into a very simple scatterplot Make a better scatterplot In [28]: # your code here x_train = np . array ([ 1 , 2 , 3 ]) y_train = np . array ([ 2 , 3 , 6 ]) type ( x_train ) Out[28]: numpy.ndarray In [12]: x_train . shape Out[12]: (3,) In [14]: x_train = x_train . reshape ( 3 , 1 ) x_train . shape Out[14]: (3, 1) In [15]: xx = np . array ([[ 1 , 3 , 5 ],[ 6 , 2 , 1 ]]) xx . shape Out[15]: (2, 3) In [18]: xx = xx . reshape ( 3 , - 1 ) xx Out[18]: array([[1, 3], [5, 6], [2, 1]]) In [29]: # %load solutions/simple_scatterplot.py # Make a simple scatterplot plt . scatter ( x_train , y_train ) # check dimensions print ( x_train . shape , y_train . shape ) (3,) (3,) In [38]: # %load solutions/nice_scatterplot.py def nice_scatterplot ( x , y , title ): # font size f_size = 18 # make the figure fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 5 )) # Create figure object # set axes limits to make the scale nice ax . set_xlim ( np . min ( x ) - 1 , np . max ( x ) + 1 ) ax . set_ylim ( np . min ( y ) - 1 , np . max ( y ) + 1 ) # adjust size of tickmarks in axes ax . tick_params ( labelsize = f_size ) # remove tick labels ax . tick_params ( labelbottom = False , bottom = False ) # adjust size of axis label ax . set_xlabel ( r '$x$' , fontsize = f_size ) ax . set_ylabel ( r '$y$' , fontsize = f_size ) # set figure title label ax . set_title ( title , fontsize = f_size ) # you may set up grid with this ax . grid ( True , lw = 1.75 , ls = '--' , alpha = 0.15 ) # make actual plot (Notice the label argument!) #ax.scatter(x, y, label=r'$my points$') #ax.scatter(x, y, label='$my points$') ax . scatter ( x , y , label = r '$my\\,points$' ) ax . legend ( loc = 'best' , fontsize = f_size ); return ax nice_scatterplot ( x_train , y_train , 'hello nice plot' ) Out[38]: Formulae Linear regression is special among the models we study beuase it can be solved explicitly. While most other models (and even some advanced versions of linear regression) must be solved itteratively, linear regression has a formula where you can simply plug in the data. For the single predictor case it is: \\begin{align} \\beta_1 &= \\frac{\\sum_{i=1}&#94;n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sum_{i=1}&#94;n{(x_i-\\bar{x})&#94;2}}\\\\ \\beta_0 &= \\bar{y} - \\beta_1\\bar{x}\\ \\end{align} Where $\\bar{y}$ and $\\bar{x}$ are the mean of the y values and the mean of the x values, respectively. From the re-aranged second equation we can see that the best-fit line passes through $(\\bar{x},\\bar{y})$, the center of mass of the data From any of the first equations, we can see that the slope of the line has to do with whether or not an x value that is above/below the center of mass is typically paired with a y value that is likewise above/below, or typically paired with one that is opposite. Building a model from scratch In this part, we will solve the equations for simple linear regression and find the best fit solution to our toy problem. The snippets of code below implement the linear regression equations on the observed predictors and responses, which we'll call the training data set. Let's walk through the code. We have to reshape our arrrays to 2D. We will see later why. Exercise (5 min) make an array with shape (2,3) reshape it to a size that you want In [ ]: # your code here xx = np . array ([[ 1 , 2 , 3 ],[ 4 , 6 , 8 ]]) xxx = xx . reshape ( - 1 , 2 ) xxx . shape In [39]: # Reshape to be a proper 2D array x_train = x_train . reshape ( x_train . shape [ 0 ], 1 ) y_train = y_train . reshape ( y_train . shape [ 0 ], 1 ) print ( x_train . shape ) (3, 1) In [40]: # first, compute means y_bar = np . mean ( y_train ) x_bar = np . mean ( x_train ) # build the two terms numerator = np . sum ( ( x_train - x_bar ) * ( y_train - y_bar ) ) denominator = np . sum (( x_train - x_bar ) ** 2 ) print ( numerator . shape , denominator . shape ) #check shapes () () Why the empty brackets? (The numerator and denominator are scalars, as expected.) In [47]: #slope beta1 beta_1 = numerator / denominator #intercept beta0 beta_0 = y_bar - beta_1 * x_bar print ( \"The best-fit line is {0:3.2f} + {1:3.2f} * x\" . format ( beta_0 , beta_1 )) print ( f 'The best fit is {beta_0} ' ) The best-fit line is -0.33 + 2.00 * x The best fit is -0.3333333333333335 Exercise (5 min) Turn the code from the above cells into a function called simple_linear_regression_fit , that inputs the training data and returns beta0 and beta1 . To do this, copy and paste the code from the above cells below and adjust the code as needed, so that the training data becomes the input and the betas become the output. def simple_linear_regression_fit ( x_train : np . ndarray , y_train : np . ndarray ) -> np . ndarray : return Check your function by calling it with the training data from above and printing out the beta values. In [ ]: # Your code here In [50]: # %load solutions/simple_linear_regression_fit.py def simple_linear_regression_fit ( x_train : np . ndarray , y_train : np . ndarray ) -> np . ndarray : \"\"\" Inputs: x_train: a (num observations by 1) array holding the values of the predictor variable y_train: a (num observations by 1) array holding the values of the response variable Returns: beta_vals: a (num_features by 1) array holding the intercept and slope coeficients \"\"\" # Check input array sizes if len ( x_train . shape ) < 2 : print ( \"Reshaping features array.\" ) x_train = x_train . reshape ( x_train . shape [ 0 ], 1 ) if len ( y_train . shape ) < 2 : print ( \"Reshaping observations array.\" ) y_train = y_train . reshape ( y_train . shape [ 0 ], 1 ) # first, compute means y_bar = np . mean ( y_train ) x_bar = np . mean ( x_train ) # build the two terms numerator = np . sum ( ( x_train - x_bar ) * ( y_train - y_bar ) ) denominator = np . sum (( x_train - x_bar ) ** 2 ) #slope beta1 beta_1 = numerator / denominator #intercept beta0 beta_0 = y_bar - beta_1 * x_bar return np . array ([ beta_0 , beta_1 ]) Let's run this function and see the coefficients In [55]: x_train = np . array ([ 1 , 2 , 3 ]) y_train = np . array ([ 2 , 2 , 4 ]) betas = simple_linear_regression_fit ( x_train , y_train ) beta_0 = betas [ 0 ] beta_1 = betas [ 1 ] print ( \"The best-fit line is {0:8.6f} + {1:8.6f} * x\" . format ( beta_0 , beta_1 )) Reshaping features array. Reshaping observations array. The best-fit line is 0.666667 + 1.000000 * x Exercise (5 min) Do the values of beta0 and beta1 seem reasonable? Plot the training data using a scatter plot. Plot the best fit line with beta0 and beta1 together with the training data. In [ ]: # Your code here In [65]: # %load solutions/best_fit_scatterplot.py fig_scat , ax_scat = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) # Plot best-fit line x_train = np . array ([[ 1 , 2 , 3 ]]) . T best_fit = beta_0 + beta_1 * x_train ax_scat . scatter ( x_train , y_train , s = 300 , label = 'Training Data' ) ax_scat . plot ( x_train , best_fit , ls = '--' , label = 'Best Fit Line' ) ax_scat . set_xlabel ( r '$x_ {train} $' ) ax_scat . set_ylabel ( r '$y$' ); The values of beta0 and beta1 seem roughly reasonable. They capture the positive correlation. The line does appear to be trying to get as close as possible to all the points. Building a model with statsmodels and sklearn Now that we can concretely fit the training data from scratch, let's learn two python packages to do it all for us: statsmodels and scikit-learn (sklearn) . Our goal is to show how to implement simple linear regression with these packages. For an important sanity check, we compare the $\\beta$ values from statsmodels and sklearn to the $\\beta$ values that we found from above with our own implementation. For the purposes of this lab, statsmodels and sklearn do the same thing. More generally though, statsmodels tends to be easier for inference [finding the values of the slope and intercept and dicussing uncertainty in those values], whereas sklearn has machine-learning algorithms and is better for prediction [guessing y values for a given x value]. (Note that both packages make the same guesses, it's just a question of which activity they provide more support for. Note: statsmodels and sklearn are different packages! Unless we specify otherwise, you can use either one. Below is the code for statsmodels . Statsmodels does not by default include the column of ones in the $X$ matrix, so we include it manually with sm.add_constant . In [58]: import statsmodels.api as sm In [66]: # create the X matrix by appending a column of ones to x_train X = sm . add_constant ( x_train ) # this is the same matrix as in our scratch problem! print ( X ) # build the OLS model (ordinary least squares) from the training data toyregr_sm = sm . OLS ( y_train , X ) # do the fit and save regression info (parameters, etc) in results_sm results_sm = toyregr_sm . fit () # pull the beta parameters out from results_sm beta0_sm = results_sm . params [ 0 ] beta1_sm = results_sm . params [ 1 ] print ( \"The regression coefficients from the statsmodels package are: beta_0 = {0:8.6f} and beta_1 = {1:8.6f} \" . format ( beta0_sm , beta1_sm )) [[ 1. 1.] [ 1. 2.] [ 1. 3.]] The regression coefficients from the statsmodels package are: beta_0 = 0.666667 and beta_1 = 1.000000 Besides the beta parameters, results_sm contains a ton of other potentially useful information. In [60]: import warnings warnings . filterwarnings ( 'ignore' ) print ( results_sm . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.750 Model: OLS Adj. R-squared: 0.500 Method: Least Squares F-statistic: 3.000 Date: Fri, 21 Sep 2018 Prob (F-statistic): 0.333 Time: 11:31:12 Log-Likelihood: -2.0007 No. Observations: 3 AIC: 8.001 Df Residuals: 1 BIC: 6.199 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.6667 1.247 0.535 0.687 -15.181 16.514 x1 1.0000 0.577 1.732 0.333 -6.336 8.336 ============================================================================== Omnibus: nan Durbin-Watson: 3.000 Prob(Omnibus): nan Jarque-Bera (JB): 0.531 Skew: -0.707 Prob(JB): 0.767 Kurtosis: 1.500 Cond. No. 6.79 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Now let's turn our attention to the sklearn library. In [67]: from sklearn import linear_model In [68]: # build the least squares model toyregr = linear_model . LinearRegression () # save regression info (parameters, etc) in results_skl results = toyregr . fit ( x_train , y_train ) # pull the beta parameters out from results_skl beta0_skl = toyregr . intercept_ beta1_skl = toyregr . coef_ [ 0 ] print ( \"The regression coefficients from the sklearn package are: beta_0 = {0:8.6f} and beta_1 = {1:8.6f} \" . format ( beta0_skl , beta1_skl )) The regression coefficients from the sklearn package are: beta_0 = 0.666667 and beta_1 = 1.000000 We should feel pretty good about ourselves now, and we're ready to move on to a real problem! The shape of things in scikit-learn Before diving right in to a \"real\" problem, we really ought to discuss more of the details of sklearn . We do this now. Along the way, we'll import the real-world dataset. Scikit-learn is the main python machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as train_test_split . It can be used in python by the incantation import sklearn . In scikit-learn, an estimator is a Python object that implements the methods fit(X, y) and predict(T) Let's see the structure of scikit-learn needed to make these fits. .fit always takes two arguments: estimator . fit ( Xtrain , ytrain ) We will consider two estimators in this lab: LinearRegression and KNeighborsRegressor . Critically, Xtrain must be in the form of an array of arrays (or a 2x2 array) with the inner arrays each corresponding to one sample, and whose elements correspond to the feature values for that sample (visuals coming in a moment). ytrain on the other hand is a simple array of responses. These are continuous for regression problems. Practice with sklearn We begin by loading up the mtcars dataset and cleaning it up a little bit. In [73]: import pandas as pd #load mtcars dfcars = pd . read_csv ( \"data/mtcars.csv\" ) dfcars = dfcars . rename ( columns = { \"Unnamed: 0\" : \"car name\" }) dfcars . head () Out[73]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } car name mpg cyl disp hp drat wt qsec vs am gear carb 0 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 1 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 2 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 3 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 4 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Next, let's split the dataset into a training set and test set. In [74]: # split into training set and testing set from sklearn.model_selection import train_test_split #set random_state to get the same split every time traindf , testdf = train_test_split ( dfcars , test_size = 0.2 , random_state = 42 ) In [75]: # testing set is around 20% of the total data; training set is around 80% print ( \"Shape of full dataset is: {0} \" . format ( dfcars . shape )) print ( \"Shape of training dataset is: {0} \" . format ( traindf . shape )) print ( \"Shape of test dataset is: {0} \" . format ( testdf . shape )) Shape of full dataset is: (32, 12) Shape of training dataset is: (25, 12) Shape of test dataset is: (7, 12) Now we have training and test data. We still need to select a predictor and a response from this dataset. Keep in mind that we need to choose the predictor and response from both the training and test set. You will do this in the exercises below. However, we provide some starter code for you to get things going. In [76]: # Extract the response variable that we're interested in y_train = traindf . mpg Notice the shape of y_train . In [77]: np . shape ( y_train ) Out[77]: (25,) Another way to see the shape is to use the shape method. In [78]: y_train . shape Out[78]: (25,) This is not an \"array of arrays\". That's okay! Remember, sklearn requires an array of arrays only for the predictor array! You will have to pay close attention to this in the exercises later. For now, let's discuss two ways out of this debacle. All we'll do is get y_train to be an array of arrays. This doesn't hurt anything because sklearn doesn't care too much about the shape of y_train . First, let's reshape y_train to be an array of arrays using the reshape method. We want the first dimension of y_train to be size $25$ and the second dimension to be size $1$. In [79]: y_train_reshape = y_train . values . reshape ( y_train . shape [ 0 ], 1 ) In [80]: y_train_reshape . shape Out[80]: (25, 1) Notice that y_train.shape[0] gives the size of the first dimension. There's an even easier way to get the correct shape right from the beginning. In [ ]: y_train_reshape = traindf [[ 'mpg' ]] In [ ]: y_train_reshape . shape Finally, there is a nice shortcut to reshaping an array. numpy can infer a dimension based on the other dimensions specified. In [ ]: y_train_reshape = y_train . values . reshape ( - 1 , 1 ) y_train_reshape . shape In this case, we said the second dimension should be size $1$. Since the requirement of the reshape() method is that the requested dimensions be compatible, numpy decides the the first dimension must be size $25$. What would the .shape return if we did y_train.values.reshape(-1,5) ? Okay, enough of that. The whole reason we went through that whole process was to show you how to reshape your data into the correct format. IMPORTANT: Remember that your response variable ytrain can be a vector but your predictor variable xtrain must be an array! Simple linear regression with automobile data We will now use sklearn to predict automobile mileage per gallon (mpg) and evaluate these predictions. We already loaded the data and split them into a training set and a test set. We need to choose the variables that we think will be good predictors for the dependent variable mpg . Exercise (10 min) Pick one variable to use as a predictor for simple linear regression. Create a markdown cell below and discuss your reasons. Justify your choice with some visualizations. Is there a second variable you'd like to use? For example, we're not doing multiple linear regression here, but if we were, is there another variable you'd like to include if we were using two predictors? In [ ]: # Your code here In [82]: # %load solutions/cars_simple_EDA.py y_mpg = dfcars . mpg x_wt = dfcars . wt x_hp = dfcars . hp fig_wt , ax_wt = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax_wt . scatter ( x_wt , y_mpg ) ax_wt . set_xlabel ( r 'Car Weight' ) ax_wt . set_ylabel ( r 'Car MPG' ) fig_hp , ax_hp = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax_hp . scatter ( x_hp , y_mpg ) ax_hp . set_xlabel ( r 'Car HP' ) ax_hp . set_ylabel ( r 'Car MPG' ) Out[82]: Exercise Use sklearn to fit the training data using simple linear regression. Use the model to make mpg predictions on the test set. Plot the data and the prediction. Print out the mean squared error for the training set and the test set and compare. Hints: Use the following to perform the analysis: from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error In [83]: from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error dfcars = pd . read_csv ( \"data/mtcars.csv\" ) dfcars = dfcars . rename ( columns = { \"Unnamed: 0\" : \"name\" }) dfcars . head () Out[83]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name mpg cyl disp hp drat wt qsec vs am gear carb 0 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 1 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 2 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 3 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 4 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 In [84]: traindf , testdf = train_test_split ( dfcars , test_size = 0.2 , random_state = 42 ) y_train = np . array ( traindf . mpg ) X_train = np . array ( traindf . wt ) X_train = X_train . reshape ( X_train . shape [ 0 ], 1 ) In [85]: y_test = np . array ( testdf . mpg ) X_test = np . array ( testdf . wt ) X_test = X_test . reshape ( X_test . shape [ 0 ], 1 ) #create linear model regression = LinearRegression () #fit linear model regression . fit ( X_train , y_train ) predicted_y = regression . predict ( X_test ) r2 = regression . score ( X_test , y_test ) print ( r2 ) 0.68797618576 In [86]: print ( regression . score ( X_train , y_train )) print ( mean_squared_error ( predicted_y , y_test )) print ( mean_squared_error ( y_train , regression . predict ( X_train ))) print ( 'Coefficients: \\n ' , regression . coef_ [ 0 ], regression . intercept_ ) 0.770137990979 12.4759856599 7.77369776639 Coefficients: -5.33694140056 36.9373103135 In [87]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( y_test , predicted_y , 'o' ) grid = np . linspace ( np . min ( dfcars . mpg ), np . max ( dfcars . mpg ), 100 ) ax . plot ( grid , grid , color = \"black\" ) # 45 degree line ax . set_xlabel ( \"actual y\" ) ax . set_ylabel ( \"predicted y\" ) fig1 , ax1 = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax1 . plot ( dfcars . wt , dfcars . mpg , 'o' ) xgrid = np . linspace ( np . min ( dfcars . wt ), np . max ( dfcars . wt ), 100 ) ax1 . plot ( xgrid , regression . predict ( xgrid . reshape ( 100 , 1 ))) Out[87]: [ ] $k$-nearest neighbors Great, so we did a simple linear regression on the car data. Now that you're familiar with sklearn , you're ready to do a KNN regression. Let's use $5$ nearest neighbors. In [ ]: from sklearn.neighbors import KNeighborsRegressor knnreg = KNeighborsRegressor ( n_neighbors = 5 ) In [ ]: knnreg . fit ( X_train , y_train ) r2 = knnreg . score ( X_test , y_test ) r2 Exercise What is the $R&#94;{2}$ score on the training set? In [ ]: # Your code here In [ ]: # solution knnreg . score ( X_train , y_train ) Lets vary the number of neighbors and see what we get. In [ ]: regdict = {} # Do a bunch of KNN regressions for k in [ 1 , 2 , 4 , 6 , 8 , 10 , 15 ]: knnreg = KNeighborsRegressor ( n_neighbors = k ) knnreg . fit ( X_train , y_train ) regdict [ k ] = knnreg # Store the regressors in a dictionary In [ ]: # Now let's plot it all fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( dfcars . wt , dfcars . mpg , 'o' , label = \"data\" ) xgrid = np . linspace ( np . min ( dfcars . wt ), np . max ( dfcars . wt ), 100 ) for k in [ 1 , 2 , 6 , 10 , 15 ]: predictions = regdict [ k ] . predict ( xgrid . reshape ( 100 , 1 )) if k in [ 1 , 6 , 15 ]: ax . plot ( xgrid , predictions , label = \" {} -NN\" . format ( k )) ax . legend (); Notice how the $1$-NN goes through every point on the training set but utterly fails elsewhere. Lets look at the scores on the training set. In [ ]: ks = range ( 1 , 15 ) # Grid of k's scores_train = [] # R2 scores for k in ks : knnreg = KNeighborsRegressor ( n_neighbors = k ) # Create KNN model knnreg . fit ( X_train , y_train ) # Fit the model to training data score_train = knnreg . score ( X_train , y_train ) # Calculate R&#94;2 score scores_train . append ( score_train ) # Plot fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 8 )) ax . plot ( ks , scores_train , 'o-' ) ax . set_xlabel ( r '$k$' ) ax . set_ylabel ( r '$R&#94; {2} $' ) Why do we get a perfect $R&#94;2$ at k=1? Exercise (5 min) Make the same plot as above on the test set. What is the best $k$? In [ ]: # Your code here In [ ]: # %load solutions/knn_regression.py if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab3/notebook/"},{"title":"Lab 4: Multiple and Polynomial Regression","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109A Introduction to Data Science Lab 4: Multiple and Polynomial Regression Harvard University Fall 2018 Instructors: Pavlos Protopapas and Kevin Rader Lab Instructor: Rahul Dave Authors: Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas In [142]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Table of Contents Learning Goals Polynomial Regression, and Revisiting the Cab Data Multiple regression and exploring the Football data A nice trick for forward-backwards Learning Goals After this lab, you should be able to Implement arbitrary multiple regression models in both SK-learn and Statsmodels Interpret the coefficent estimates produced by each model, including transformed and dummy variables In [33]: import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm from statsmodels.api import OLS from sklearn import preprocessing from sklearn.preprocessing import PolynomialFeatures from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from pandas.plotting import scatter_matrix import seaborn as sns % matplotlib inline statsmodels is focused on the inference task: guess good values for the betas and discuss how certain you are in those answers. sklearn is focused on the prediction task: given [new] data, guess what the response value is. As a result, statsmodels has lots of tools to discuss confidence, but isn't great at dealing with test sets. Sklearn is great at test sets and validations, but can't really discuss uncertainty in the parameters or predictions. In short: sklearn is about putting a line through it and predicting new values using that line. If the line gives good predictions on the test set, who cares about anything else? statsmodels assumes more about how the data were generated, and (if the assumptions are correct) can tell you about uncertainty in the results Some terms R-squared : An interpretable summary of how well the model did. 1 is perfect, 0 is a trivial baseline model, negative is worse than the trivial model F-statistic : A value testing whether we're likely to see these results (or even stronger ones) if none of the predictors actually mattered. Prob (F-statistic) : The probability that we'd see these results (or even stronger ones) if none of the predictors actually mattered. If this probability is small then either A) some combination of predictors actually matters or B) something rather unlikely has happened coef : The estimate of each beta. This has several sub-components: std err : The amount we'd expect this value to wiggle if we re-did the data collection and re-ran our model. More data tends to make this wiggle smaller, but sometimes the collected data just isn't enough to pin down a particular value. t and P>|t| : similar to the F-statistic, these measure the probability of seeing coefficients this big (or even bigger) if the given variable didn't actually matter. Small probability doesn't necessarily mean the value matters [0.025 0.975] : Endpoints of the 95% confidence interval. This is a interval drawn in a clever way and which gives an idea of where the true beta value might plausibly live. (If you want to understand why \"there's a 95% chance the true beta is in the interval\" is wrong , start a chat with Will : ) Part 1: Polynomial Regression, and Revisiting the Cab Data In [34]: # read in the data, break into train and test cab_df = pd . read_csv ( \"data/dataset_1.txt\" ) train_data , test_data = train_test_split ( cab_df , test_size =. 2 , random_state = 42 ) cab_df . head () In [35]: cab_df . shape In [36]: # do some data cleaning X_train = train_data [ 'TimeMin' ] . values . reshape ( - 1 , 1 ) / 60 y_train = train_data [ 'PickupCount' ] . values X_test = test_data [ 'TimeMin' ] . values . reshape ( - 1 , 1 ) / 60 y_test = test_data [ 'PickupCount' ] . values def plot_cabs ( cur_model , poly_transformer = None ): # build the x values for the prediction line x_vals = np . arange ( 0 , 24 , . 1 ) . reshape ( - 1 , 1 ) # if needed, build the design matrix if poly_transformer : design_mat = poly_transformer . fit_transform ( x_vals ) else : design_mat = x_vals # make the prediction at each x value prediction = cur_model . predict ( design_mat ) # plot the prediction line, and the test data plt . plot ( x_vals , prediction , color = 'k' , label = \"Prediction\" ) plt . scatter ( X_test , y_test , label = \"Test Data\" ) # label your plots plt . ylabel ( \"Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () plt . show () In [37]: from sklearn.linear_model import LinearRegression fitted_cab_model0 = LinearRegression () . fit ( X_train , y_train ) plot_cabs ( fitted_cab_model0 ) In [39]: fitted_cab_model0 . score ( X_test , y_test ) We can see that there's still a lot of variation in cab pickups that's not being caught by a linear fit. And the linear fit is predicting massively more pickups at 11:59pm than at 12:00am. However, we can add columns to our design matrix for $TimeMin&#94;2$ and $TimeMin&#94;3$ and so on, allowing a wigglier polynomial that will better fit the data. We'll be using sklearn's PolynomialFeatures to take some of the tedium out of building the new design matrix. In fact, if all we want is a formula like $y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x&#94;2 + ...$ it will directly return the new design matrix. In [38]: transformer_3 = PolynomialFeatures ( 3 , include_bias = False ) new_features = transformer_3 . fit_transform ( X_train ) new_features A few notes on PolynomialFeatures : The interface is a bit strange. PolynomialFeatures is a 'transformer' in sklearn. We'll be using several transformers that learn a transformation on the training data and then apply that transformation on future data. On these (more typical) transformers it makes sense to have a .fit() and a separate .transform() . With PolynomialFeatures, the .fit() is pretty trivial, and we often fit and transform in one command, as seen above. You rarely want to include_bias (a column of all 1s), since sklearn will add it automatically and statsmodels can just add_constant right before you fit to the design matrix If you want polynomial features for a several different variables, you should call .fit_transform() separately on each column and append all the results to the design matrix (unless you also want interaction terms between the newly-created features). See np.concatenate for joining arrays. In [8]: fitted_cab_model3 = LinearRegression () . fit ( new_features , y_train ) plot_cabs ( fitted_cab_model3 , transformer_3 ) Exercise Questions : Calculate the polynomial model's $R&#94;2$ performance on the test set. Does the polynomial model improve on the purely linear model? Make a residual plot for the polynomial model. What does this plot tell us about the model? your answer here In [9]: # your code here In [40]: # your code here In [41]: # your code here Other features Polynomial features are not the only constucted features that help fit the data. Because these data have a 24 hour cycle, we may want to build features that follow such a cycle. For example, $sin(24\\frac{x}{2\\pi})$, $sin(12\\frac{x}{2\\pi})$, $sin(8\\frac{x}{2\\pi})$. Other feature transformations are appropriate to other types of data. For instance certain feature transformations have been developed for geographical data. Part 2: Multiple regression and exploring the Football data Let's move on to a truly interesting dataset. The data imported below were scraped by Shubham Maurya and record various facts about players in the English Premier League. Our goal will be to fit models that predict the players' market value (what the player could earn when hired by a new team), as estimated by transfermrkt.com. name : Name of the player club : Club of the player age : Age of the player position : The usual position on the pitch position_cat : 1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers market_value : As on transfermrkt.com on July 20th, 2017 page_views : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017 fpl_value : Value in Fantasy Premier League as on July 20th, 2017 fpl_sel : % of FPL players who have selected that player in their team fpl_points : FPL points accumulated over the previous season region : 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World nationality : Player's nationality new_foreign : Whether a new signing from a different league, for 2017/18 (till 20th July) age_cat : a categorical version of the Age feature club_id : a numerical version of the Club feature big_club : Whether one of the Top 6 clubs new_signing : Whether a new signing for 2017/18 (till 20th July) As always, we first import, verify, split, and explore the data. Part 2.1: Import and verification and grouping In [85]: league_df = pd . read_csv ( \"data/league_data.txt\" ) print ( league_df . dtypes ) league_df . head () In [86]: league_df . shape In [87]: league_df . describe () (Stratified) train/test split We want to make sure that the training and test data have appropriate representation of each region; it would be bad for the training data to entirely miss a region. This is especially important because some regions are rather rare. Exercise Questions : Use the train_test_split function to and its 'stratify' argument to split the data, keeping equal representation of each region (This will not work out of the box on this data. Deal with the resulting issue). Deal with the issue you encountered above. How did you deal with the error generated by train_test_split ? How did you justify your action? your answer here : In [88]: # your code here In [89]: train_data . shape , test_data . shape Now that we won't be peeking at the test set, let's explore and look for patterns! We'll introduce a number of useful pandas and numpy functions along the way. Groupby Pandas' .groupby() function is a wonderful tool for data analysis. It allows us to analyze each of several subgroups. Many times, .groupby() is combined with .agg() to get a summary statistic for each subgroup. For instance: What is the average market value, median page views, and maximum fpl for each player position? In [90]: train_data . groupby ( 'position' ) . agg ({ 'market_value' : np . mean , 'page_views' : np . median , 'fpl_points' : np . max }) In [91]: train_data . position . unique () In [92]: train_data . groupby ([ 'big_club' , 'position' ]) . agg ({ 'market_value' : np . mean , 'page_views' : np . mean , 'fpl_points' : np . mean }) Part 2.2: Linear regression on the football data This section of the lab focuses on fitting a model to the football data and interpreting the model results. The model we'll use is $$\\text{market_value} \\approx \\beta_0 + \\beta_1\\text{fpl_points} + \\beta_2\\text{age} + \\beta_3\\text{age}&#94;2 + \\beta_4log_2\\left(\\text{page_views}\\right) + \\beta_5\\text{new_signing} +\\beta_6\\text{big_club} + \\beta_7\\text{position_cat}$$ We're including a 2nd degree polynomial in age because we expect pay to increase as a player gains experience, but then decrease as they continue aging. We're taking the log of page views because they have such a large, skewed range and the transformed variable will have fewer outliers that could bias the line. We choose the base of the log to be 2 just to make interpretation cleaner. Exercise Questions : Build a design matrix function and fit this model to the training data. How good is the overall model? Interpret the regression model. What is the meaning of the coefficient for: age and age$&#94;2$ $log_2($page_views$)$ big_club What should a player do in order to improve their market value? How many page views should a player go get to increase their market value by 10? In [93]: # your code here In [94]: # your code here your answer here In [95]: agecoef = fitted_model_1 . params . age age2coef = fitted_model_1 . params . age_squared In [96]: x_vals = np . linspace ( - 100 , 100 , 1000 ) y_vals = agecoef * x_vals + age2coef * x_vals ** 2 plt . plot ( x_vals , y_vals ) plt . title ( \"Effect of Age\" ) plt . xlabel ( \"Age\" ) plt . ylabel ( \"Contribution to Predicted Market Value\" ) plt . show () Part 2.3: Turning Categorical Variables into multiple binary variables Of course, we have an error in how we've included player position. Even though the variable is numeric (1,2,3,4) and the model runs without issue, the value we're getting back is garbage. The interpretation, such as it is, is that there is an equal effect of moving from position category 1 to 2, from 2 to 3, and from 3 to 4, and that this effect is about -.61. In reality, we don't expect moving from one position category to another to be equivalent, nor for a move from category 1 to category 3 to be twice as important as a move from category 1 to category 2. We need to introduce better features to model this variable. We'll use pd.get_dummies to do the work for us. In [97]: train_design_recoded = pd . get_dummies ( train_design , columns = [ 'position_cat' ], drop_first = True ) test_design_recoded = pd . get_dummies ( test_design , columns = [ 'position_cat' ], drop_first = True ) train_design_recoded . head () We've removed the original position_cat column and created three new ones. Why only three new columns? Why does pandas give us the option to drop the first category? Exercise Questions : If we're fitting a model without a constant, should we have three dummy columns or four dummy columns? Fit a model and interpret the coefficient of position_cat_2 . In [109]: resu = OLS ( y_train , train_design_recoded ) . fit () resu . summary () In [110]: r2_score ( y_test , resu . predict ( test_design_recoded )) In [99]: train_design_recoded . shape , y_train . shape Answers : If our model does not have a constant, we must include all four dummy variable columns. If we drop one, we're not modeling any effect of being in that category, and effectively assuming the dropped category's effect is 0. Being in position 2 (instead of position 1) has an impact between -1.54 and +2.38 on a player's market value. Since we're using an intercept, the dropped category becomes the baseline and the effect of any dummy variable is the effect of being in that category instead of the baseline category. Part 3: A nice trick for forward-backwards XOR (operator &#94;) is a logical operation that only returns true when input differ. We can use it to implement forward-or-backwards selection when we want to keep track of whet predictors are \"left\" from a given list of predictors. The set analog is \"symmetric difference\". From the python docs: s.symmetric_difference(t) s &#94; t new set with elements in either s or t but not both In [1]: set () &#94; set ([ 1 , 2 , 3 ]) Out[1]: {1, 2, 3} In [2]: set ([ 1 ]) &#94; set ([ 1 , 2 , 3 ]) Out[2]: {2, 3} In [3]: set ([ 1 , 2 ]) &#94; set ([ 1 , 2 , 3 ]) Out[3]: {3} Exercise Outline a step-forwards algorithm which uses this idea your answer here if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab4/notebook/"},{"title":"Lab 5: Regularization and Cross-Validation","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109A Introduction to Data Science Lab 5: Regularization and Cross-Validation Harvard University Fall 2018 Instructors: Pavlos Protopapas and Kevin Rader Lab Instructor: Keivn Rader (today at least) Authors: Kevin Rader, Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas Table of Contents Learning Goals Review of regularized regression Bootstrapped Sampling Distributions and Confidence Intervals Ridge regression for Simple Regression Ridge regression with polynomial features on a grid Cross-validation --- Multiple Estimates Cross-validation --- Finding the best regularization parameter Learning Goals In this lab, you will work with some noisy data. You will use simple linear and ridge regressions to fit linear, high-order polynomial features to the dataset. You will attempt to figure out what degree polynomial fits the dataset the best and ultimately use cross validation to determine the best polynomial order. Finally, you will automate the cross validation process using sklearn in order to determine the best regularization paramter for the ridge regression analysis on your dataset. By the end of this lab, you should: Really understand regularized regression principles. Have a good grasp of working with ridge regression through the sklearn API Understand the effects of the regularization (a.k.a penalization or tuning) parameter on fits from ridge regression Understand the ideas behind cross-validation Why is it necessary? Why is it important? Basic implementation details. Be able to use sklearn objects to automate the cross validation process. This lab corresponds to lectures 5, 6, and 7 and maps to homework 4 (and beyond). In [ ]: % matplotlib inline import numpy as np import scipy as sp import matplotlib as mpl import matplotlib.cm as cm import matplotlib.pyplot as plt import pandas as pd pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.notebook_repr_html' , True ) import seaborn.apionly as sns sns . set_style ( \"whitegrid\" ) sns . set_context ( \"poster\" ) Part 1: Review of regularized regression We briefly review the idea of regularization as introduced in lecture. Recall that in the ordinary least squares problem we find the regression coefficients $\\boldsymbol{\\beta}\\in\\mathbb{R}&#94;{m}$ that minimize the loss function \\begin{align*} L(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}&#94;n \\|y_i - \\boldsymbol{\\beta}&#94;T \\mathbf{x}_i\\|&#94;2. \\end{align*} Recall that we have $n$ observations. Here $y_i$ is the response variable for observation $i$ and $\\mathbf{x}_i\\in\\mathbb{R}&#94;{m}$ is a vector from the predictor matrix corresponding to observation $i$. The general idea behind regularization is to penalize the loss function to account for possibly very large values of the coefficients $\\boldsymbol{\\beta}$. Instead of minimizing $L(\\boldsymbol{\\beta})$, we minimize the regularized loss function \\begin{align*} L_{\\text{reg}}(\\boldsymbol{\\beta}) = L(\\boldsymbol{\\beta}) + \\lambda R(\\boldsymbol{\\beta}) \\end{align*} where $R(\\boldsymbol{\\beta})$ is a penalty function and $\\lambda$ is a scalar that weighs the relative importance of this penalty. In this lab we will explore one regularized regression model: ridge regression. In ridge regression, the penalty function is the sum of the squares of the parameters, which is written as \\begin{align*} L_{\\text{ridge}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}&#94;n \\|y_i - \\boldsymbol{\\beta}&#94;T \\mathbf{x}_i\\|&#94;2 + \\lambda \\sum_{j=1}&#94;m \\beta_{j}&#94;{2}. \\end{align*} In lecture, you also learned about LASSO regression in which the penalty function is the sum of the absolute values of the parameters. This is written as, \\begin{align*} L_{\\text{LASSO}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}&#94;n \\|y_i - \\boldsymbol{\\beta}&#94;T \\mathbf{x}_i\\|&#94;2 + \\lambda \\sum_{j=1}&#94;m |\\beta_j|. \\end{align*} In this lab, we will show how these optimization problems can be solved with sklearn to determine the model parameters $\\boldsymbol{\\beta}$. We will also show how to choose $\\lambda$ appropriately via cross-validation. Dataset We will work with a synthetic dataset contained in data/noisypopulation.csv . The data were generated from a specific function $f\\left(x\\right)$ (the actual mathematical form will remain a mystery). Noise was added to the function to generate synthetic, noisy (aka, real) observations via $y = f\\left(x\\right) + \\epsilon$ where $\\epsilon$ was drawn from a random distribution. Even if you could make observations at every single value of $x$, the true function may still be obscured. Of course, the data you actually observe are a subset of all the possible observations in the population. In this lab, we will refer to observations at every single value of $x$ as the population and the subset of observations as in-sample y or simply the observations . The dataset contains three columns: f is the true function value x is the predictor y is the measured response. In [ ]: df = pd . read_csv ( \"data/noisypopulation.csv\" ) df . head () In this lab, we will try out some regression methods to fit the data and see how well our model matches the true function f . In [ ]: # Convert f, x, y to numpy array f = df . f . values x = df . x . values y = df . y . values df . shape Let's take a quick look at the dataset. We will plot the true function value and the population. In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( x , y , '.' , alpha = 0.8 , label = r 'Population' ) ax . plot ( x , f , lw = 4 , label = 'True Response' ) ax . legend ( loc = 'upper left' ) ax . set_xlabel ( r '$x$' ) ax . set_ylabel ( r '$y$' ) fig . tight_layout () It is often the case that you just can't make observations at every single value of $x$. We will simulate this situation by making a random choice of $60$ points from the full $200$ points. We do it by choosing the indices randomly and then using these indices as a way of getting the appropriate samples. In [ ]: #np.random.seed(12345) indexes = np . sort ( np . random . choice ( x . shape [ 0 ], size = 60 , replace = False )) # Note: using sort to make plotting easier later indexes Note: If you are not familiar with the numpy sort method or the numpy random.choice() method, then please take a moment to look them up in the numpy documentation. Moving on, let's get the $60$ random samples from our dataset. In [ ]: # Create a new dataframe from the random points sample_df = pd . DataFrame ( dict ( x = x [ indexes ], f = f [ indexes ], y = y [ indexes ])) # New dataframe sample_df . head () Let's take one more look at our data to see which points we've selected. In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( sample_df [ 'x' ], sample_df [ 'y' ], 's' , alpha = 0.4 , ms = 10 , label = \"in-sample y (observed)\" ) ax . plot ( x , y , '.' , label = r 'Population $y$' ) ax . plot ( x , f , lw = 4 , label = 'True Response' ) ax . legend ( loc = 'upper left' ) ax . set_xlabel ( r '$x$' ) ax . set_ylabel ( r '$y$' ) fig . tight_layout () Now we do our favorite thing and split the sample data into training and testing sets. Note that here we are actually getting indices instead of the actual training and test set. This is okay and is another way of generating train-test splits. In [ ]: from sklearn.model_selection import train_test_split datasize = sample_df . shape [ 0 ] #split dataset using the index, as we have x, f, and y that we want to split. itrain , itest = train_test_split ( np . arange ( 60 ), train_size = 0.8 ) xtrain = sample_df . x [ itrain ] . values ftrain = sample_df . f [ itrain ] . values ytrain = sample_df . y [ itrain ] . values xtest = sample_df . x [ itest ] . values ftest = sample_df . f [ itest ] . values ytest = sample_df . y [ itest ] . values Great! At this point we've explored our data a little bit, selected a sample of the dataset, and done a train-test split on the sample dataset. Let's move on to the data analysis. We'll begin with ridge regression. In particular we'll do ridge regression on a single predictor and compare it with simple linear regression. To start, let's fit the old classic, linear regression. In [ ]: from sklearn.linear_model import LinearRegression # fit the model to training data simp_reg = LinearRegression () . fit ( xtrain . reshape ( - 1 , 1 ), ytrain ) # save the beta coefficients beta0_sreg = simp_reg . intercept_ beta1_sreg = simp_reg . coef_ [ 0 ] print ( \"(beta0, beta1) = ( {0:8.6f} , {1:8.6f} )\" . format ( beta0_sreg , beta1_sreg )) Part 2: Bootstrapping But wait! Unlike statsmodels , we don't get confidence intervals for the betas. Fortunately, we can bootstrap to build the confidence intervals **Exercise 1** In the code below, two key steps of bootstrapping are missing. Fill in the code to draw sample indices with replacement and to fit the model to the bootstrap sample. You'll need numpy 's np.random.choice . Here's the function documentation in case you need it. Visualize the results, and use numpy 's np.percentile : function documentation . In [ ]: N = 1000 bootstrap_beta1s = np . zeros ( N ) for cur_bootstrap_rep in range ( N ): # use np.random.choice to select 48 indices ranging from 0 to 47, with replacement, # and store them in inds_to_sample (48 is the size of our training set) ########### # your code here ########### # take the sample x_train_resample = xtrain [ inds_to_sample ] y_train_resample = ytrain [ inds_to_sample ] # refit the model ########### # your code here ########### # extract the beta1 and append bootstrap_beta1s [ cur_bootstrap_rep ] = bootstrap_model . coef_ [ 0 ] In [ ]: ## calculate 5th and 95th percentiles & display the results ########### # your code here ########### From the above, we find that the bootstrap $90\\%$ confidence interval is well away from $0$. We can confidently say that $\\beta_{1}$ is not secretly $0$ and we're being fooled by randomness. Part 3: Ridge regression for Simple Regression To begin, we'll use sklearn to do simple linear regression on the sampled training data. We'll then do ridge regression with the same data, setting the penalty parameter $\\lambda$ to zero. What happens when we set $\\lambda = 0$ for Ridge's Penalty factor? We will store the regression coefficients in a dataframe for easy comparison. The cell below provides some code to set up the dataframe ahead of time. Notice that we don't know the actual values in the pandas series, so we just set them to NaN . We will overwrite these later. In [ ]: regression_coeffs = dict () # Store regression coefficients from each model in a dictionary regression_coeffs [ 'OLS' ] = [ np . nan ] * 2 # Initialize to 0 regression_coeffs [ r 'Ridge $\\lambda = 0$' ] = [ np . nan ] * 2 dfResults = pd . DataFrame ( regression_coeffs ) # Create dataframe dfResults . rename ({ 0 : r '$\\hat{\\beta}_ {0} $' , 1 : r '$\\hat{\\beta}_ {1} $' }, inplace = True ) # Rename rows dfResults We start with simple linear regression to get the ball rolling. In [ ]: simp_reg = LinearRegression () # build the the ordinary least squares model simp_reg . fit ( xtrain . reshape ( - 1 , 1 ), ytrain ) # fit the model to training data # save the beta coefficients beta0_sreg = simp_reg . intercept_ beta1_sreg = simp_reg . coef_ [ 0 ] dfResults [ 'OLS' ][:] = [ beta0_sreg , beta1_sreg ] dfResults In [ ]: #y_predict = lambda x : beta0_sreg + beta1_sreg*x # a user function to make predictions ypredict_ols = simp_reg . predict ( x . reshape ( - 1 , 1 )) We will use the above $\\boldsymbol\\beta$ coefficients as a benchmark for comparision to the ridge method. The same coefficients can be obtained with ridge regression, which we demonstrate now. For reference, here is the ridge regression documentation: sklearn.linear_model.Ridge . In [ ]: from sklearn.linear_model import Ridge The snippet of code below implements the ridge regression with $\\lambda = 0$. Note: The weight $\\lambda$ is referred to as alpha in the documentation. Remark: $\\lambda$ goes by many names including, but not limited to: regularization parameter, penalization parameter, penalty factor, tuning parameter, shrinking parameter, and weight. Regardless of these names, it is a hyperparameter. That is, you set it before you begin the training process. An algorithm can be very sensitive to its hyperparameters and we will discuss how a method for selecting the \"correct\" hyperparameter values later in this lab. In [ ]: ridge_reg = Ridge ( alpha = 0 ) # build the ridge regression model with specified lambda, i.e. alpha ridge_reg . fit ( xtrain . reshape ( - 1 , 1 ), ytrain ) # fit the model to training data # save the beta coefficients beta0_ridge = ridge_reg . intercept_ beta1_ridge = ridge_reg . coef_ [ 0 ] ypredict_ridge = ridge_reg . predict ( x . reshape ( - 1 , 1 )) # make predictions everywhere dfResults [ r 'Ridge $\\lambda = 0$' ][:] = [ beta0_ridge , beta1_ridge ] dfResults The beta coefficients for linear and ridge regressions coincide for $\\lambda = 0$, as expected. We plot the data and fits. In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( xtrain , ytrain , 's' , alpha = 0.3 , ms = 10 , label = \"in-sample y (observed)\" ) # plot in-sample training data ax . plot ( x , y , '.' , alpha = 0.4 , label = \"population y\" ) # plot population data ax . plot ( x , f , ls = '-' , alpha = 0.4 , lw = 4 , label = \"True function\" ) ax . plot ( x , y_predict ( x ), ls = '--' , lw = 4 , label = \"OLS\" ) # plot simple linear regression fit ax . plot ( x , ypredict_ridge , ls = '-.' , lw = 4 , label = \"Ridge\" ) # plot ridge regression fit ax . set_xlabel ( '$x$' ) ax . set_ylabel ( '$y$' ) ax . legend ( loc = 4 ); fig . tight_layout () Exercise 2 Explore the effect of $\\lambda$ on ridge regression. Make a plot with of the ridge regression predictions with $\\lambda = 0, 5, 10, 100$. Be sure to include a legend. What happens for very large $\\lambda$ (e.g. $\\lambda \\to \\infty$)? Your plot should look something like the following plot (doesn't have to be exact): In [ ]: # Your code here Part 3 Recap That was nice, but we were just doing simple linear regression. We really want to do more interesting regression problems like multilinear regression. We will do so in the next section. Part 4: Ridge regression with polynomial features on a grid Now we'll make a more complex model by adding polynomial features. Instead of building the linear model $y = \\beta_0 + \\beta_1 x$, we build a polynomial model $y = \\beta_0 + \\beta_1 x + \\beta_2 x&#94;2 + \\ldots \\beta_d x&#94;d$ for some $d$ to be determined. This regression will be linear though, since we'll be treating $x&#94;2, \\ldots, x&#94;d$ themselves as predictors in the linear model. The design matrix $\\mathbf{X}$ contains columns corresponding to $1, x, x&#94;2, \\ldots, x&#94;d$. To build it, we use sklearn . (The particular design matrix is also known as the Vandermonde matrix ). For example, if we have three observations \\begin{align*} \\left\\{\\left(x_{1}, y_{1}\\right), \\left(x_{2}, y_{2}\\right), \\left(x_{3}, y_{3}\\right)\\right\\} \\end{align*} and we want polynomial features up to and including degree $4$, the design matrix looks like \\begin{align*} X = \\begin{bmatrix} x_1&#94;0 & x_1&#94;1 & x_1&#94;2 & x_1&#94;3 & x_1&#94;4\\\\ x_2&#94;0 & x_2&#94;1 & x_2&#94;2 & x_2&#94;3 & x_2&#94;4\\\\ x_3&#94;0 & x_3&#94;1 & x_3&#94;2 & x_3&#94;3 & x_3&#94;4\\\\ \\end{bmatrix} = \\begin{bmatrix} 1& x_1&#94;1 & x_1&#94;2 & x_1&#94;3 & x_1&#94;4\\\\ 1 & x_2&#94;1 & x_2&#94;2 & x_2&#94;3 & x_2&#94;4\\\\ 1 & x_3&#94;1 & x_3&#94;2 & x_3&#94;3 & x_3&#94;4\\\\ \\end{bmatrix}. \\end{align*} Exercise 3 Make a toy vector called toy , where \\begin{align*} \\mathrm{toy} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 5 \\\\ \\end{bmatrix}. \\end{align*} Build the feature matrix up to (and including) degree $4$. Confirm that the entries in the matrix are what you'd expect based on the above discussion. Note: You may use sklearn to build the matrix using PolynomialFeatures() . In [ ]: from sklearn.preprocessing import PolynomialFeatures In [ ]: # your code here We now continue working with our data. Exercise 4 The code provided below is missing a few lines and it's missing many comments. Do the following: Comment every line of the code Normally, you won't do such excessive commenting. In this case, we want to make sure you understand every single line since you didn't actually write this code. Fill in the missing lines Create a ridge regression object at each $\\lambda$ value in the list Perform the ridge regression using the fit method from the newly created ridge regression object Make a prediction on the grid and store the results in ypredict_ridge . Note: We're not giving you an example figure here since we gave you most of the code. Warning! Make sure you understand the entire code! There are many nice things in there. In [ ]: d = 20 # You will create a grid of plots of this size (7 x 2) rows = 7 cols = 2 lambdas = [ 0. , 1e-6 , 1e-3 , 1e-2 , 1e-1 , 1 , 10 ] grid_to_predict = np . arange ( 0 , 1 , . 01 ) Xtrain = PolynomialFeatures ( d ) . fit_transform ( xtrain . reshape ( - 1 , 1 )) test_set = PolynomialFeatures ( d ) . fit_transform ( grid_to_predict . reshape ( - 1 , 1 )) fig , axs = plt . subplots ( rows , cols , sharex = 'col' , figsize = ( 12 , 24 )) # Set up plotting objects for i , lam in enumerate ( lambdas ): # your code here # Create regression object # Fit on regression object # Do a prediction on the test set ### Provided code axs [ i , 0 ] . plot ( xtrain , ytrain , 's' , alpha = 0.4 , ms = 10 , label = \"in-sample y\" ) axs [ i , 0 ] . plot ( grid_to_predict , ypredict_ridge , 'k-' , label = r \"$\\lambda = {0} $\" . format ( lam )) axs [ i , 0 ] . set_ylabel ( '$y$' ) axs [ i , 0 ] . set_ylim (( 0 , 1 )) axs [ i , 0 ] . set_xlim (( 0 , 1 )) axs [ i , 0 ] . legend ( loc = 'best' ) coef = ridge_reg . coef_ . ravel () axs [ i , 1 ] . semilogy ( np . abs ( coef ), ls = ' ' , marker = 'o' , label = r \"$\\lambda = {0} $\" . format ( lam )) axs [ i , 1 ] . set_ylim (( 1e-04 , 1e+15 )) axs [ i , 1 ] . set_xlim ( 1 , 20 ) axs [ i , 1 ] . yaxis . set_label_position ( \"right\" ) axs [ i , 1 ] . set_ylabel ( r '$\\left|\\beta_ {j} \\right|$' ) axs [ i , 1 ] . legend ( loc = 'best' ) axs [ - 1 , 0 ] . set_xlabel ( \"x\" ) axs [ - 1 , 1 ] . set_xlabel ( r \"$j$\" ); As you can see, as we increase $\\lambda$ from 0 to 1, we start out overfitting, then doing well, and then our fits develop a mind of their own irrespective of data, as the penalty term dominates. Exercise 5 What would you expect if you compared a performance metric between these models on a grid? What performance metric should you use? YOUR DISCUSSION HERE Part 4 Recap We did a ridge regression on our dataset where the features were the polynomial terms. We also assessed the impact of the regularization parameter on the solution. Part 5: Cross-validation --- Finding the best penalization parameter In order to determine the critical value of $\\lambda$ that gives us our best predictive model, which we'll refer to as $\\lambda&#94;*$, we will assess this via cross-validation. To do this we use the concept of a meta-estimator from scikit-learn . Model selection is supported by two distinct meta-estimators: GridSearchCV RandomizedSearchCV The input to these meta-estimators is an estimator, which has some hyperparameters (e.g. $\\lambda$) that need to be optimized, and a set of hyperparameter settings to search through. The concept of a meta-estimator allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example: est = Ridge () parameters = { \"alpha\" : [ 1e-8 , 1e-6 , 1e-5 , 5e-5 , 1e-4 , 5e-4 , 1e-3 , 1e-2 , 1e-1 , 1.0 ]} gridclassifier = GridSearchCV ( est , param_grid = parameters , cv = 4 , scoring = \"neg_mean_squared_error\" ) The GridSearchCV replaces the manual iteration over the folds using KFolds and the averaging we just did, doing it all for us. It takes a hyperparameter grid in the shape of a dictionary as input, and sets $\\lambda$ to the values you want to try, one by one. It then trains the model using cross-validation, and gets the error for each value of the hyperparameter $\\lambda$. Finally it compares the errors for the different $\\lambda$'s, and picks the best choice model. Here is a helper function that we will use to get the best Ridge regression. In [ ]: from sklearn.model_selection import GridSearchCV def cv_optimize_ridge ( x : np . ndarray , y : np . ndarray , list_of_lambdas : list , n_folds : int = 4 ): est = Ridge () parameters = { 'alpha' : list_of_lambdas } # the scoring parameter below is the default one in ridge, but you can use a different one # in the cross-validation phase if you want. gs = GridSearchCV ( est , param_grid = parameters , cv = n_folds , scoring = \"neg_mean_squared_error\" ) gs . fit ( x , y ) return gs Exercise 6 Use the function above to fit the model on the training set with $4$-fold cross validation. Save the fit as the variable fitmodel . In [ ]: lambs = [ 1e-7 , 1e-6 , 1e-5 , 5e-5 , 1e-4 , 5e-4 , 1e-3 , 1e-2 , 1e-1 , 1.0 , 10.0 ] # your code here In [ ]: print ( fitmodel . best_estimator_ , \" \\n \" ) print ( fitmodel . best_params_ , \" \\n \" ) print ( fitmodel . best_score_ , \" \\n \" ) We also output the mean cross-validation error at different $\\lambda$ (with a negative sign, as scikit-learn likes to maximize negative error which is equivalent to minimizing error). In [ ]: fitmodel . cv_results_ In [ ]: fit_lambdas = [ d [ 'alpha' ] for d in fitmodel . cv_results_ [ 'params' ]] fit_scores = fitmodel . cv_results_ [ 'mean_test_score' ] Now we make a log-log plot of -fit_scores versus fit_lambdas . In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( fit_lambdas , - fit_scores , ls = '-' , marker = 'o' ) ax . set_xscale ( 'log' ) ax . set_yscale ( 'log' ) ax . set_xlabel ( '$\\lambda$' ) ax . set_ylabel ( 'scores' ); SK-learn's cross_val_score : Easier Cross Validation GridSearchCV is an important tool when you are searching over many hyperparameters (and believe us, you will be), but when you only need to get CV scores for a particular model, some students find cross_val_score more intuitive. In [ ]: from sklearn.model_selection import cross_val_score lr_object = Ridge ( alpha = 0 ) cross_val_score ( lr_object , Xtrain , ytrain , cv = 5 ) We can loop over particular models and get scores for each (equivalent to GridSearchCV over the given parameter settings). In [ ]: for cur_alpha in [ 1e-8 , 1e-4 , 1e-2 , 1.0 , 10.0 ]: lr_object = Ridge ( alpha = cur_alpha ) scores = cross_val_score ( lr_object , Xtrain , ytrain , cv = 5 ) print ( \"lambda {0} \\t R&#94;2 scores: {1} \\t Mean R&#94;2: {2} \" . format ( cur_alpha , scores , np . mean ( scores ))) Built-in Cross Validation: RidgeCV and LassoCV Some sklearn models have built-in, automated cross validation to tune their hyper parameters. In [ ]: from sklearn.linear_model import RidgeCV ridgeCV_object = RidgeCV ( alphas = lambs , cv = 5 ) ridgeCV_object . fit ( Xtrain , ytrain ) print ( \"Best model searched: \\n alpha = {} \\n intercept = {} \\n betas = {} , \" . format ( ridgeCV_object . alpha_ , ridgeCV_object . intercept_ , ridgeCV_object . coef_ ) ) Important note: For any tool more automated than literally using k_fold, just setting cv=5 will NOT shuffle your data by default. To force shuffling, explicitly pass a KFold object (with shuffling turned on) to the cv argument You may prefer a strategy where you shuffle the rows of your data at the outset of analysis In [ ]: # declare and pass a KFold object to properly shuffle the training data, and/or set the random state splitter = KFold ( 5 , random_state = 42 , shuffle = True ) ridgeCV_object = RidgeCV ( alphas = ( 1e-8 , 1e-4 , 1e-2 , 1.0 , 10.0 ), cv = splitter ) ridgeCV_object . fit ( Xtrain , ytrain ) print ( \"Best model searched: \\n alpha = {} \\n intercept = {} \\n betas = {} , \" . format ( ridgeCV_object . alpha_ , ridgeCV_object . intercept_ , ridgeCV_object . coef_ ) ) Part 5b: Refitting on full training set At this point, we have determined the best penalization parameter for the ridge regression on our current dataset using cross validation. Let's refit the estimator on the training set and calculate and plot the test set error and the polynomial coefficients. Notice how many of these coefficients have been pushed to lower values or 0. Exercise 7 Assign to variable est the classifier obtained by fitting the entire training set using the best $\\lambda$ found above. Assign the predictions to the variable ypredict_ridge_best . In [ ]: # your code here In [ ]: # code provided from here on fig , axs = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) left = 0 right = 1 axs [ left ] . plot ( x , f , lw = 4 , label = 'True Response' ) axs [ left ] . plot ( xtrain , ytrain , 's' , alpha = 0.3 , ms = 10 , label = \"in-sample y (observed)\" ) axs [ left ] . plot ( x , y , '.' , alpha = 0.8 , label = \"population y\" ) axs [ left ] . plot ( grid_to_predict , ypredict_ridge_best , 'k--' , label = r \"$\\lambda = {{ {0:1.4f} }}$\" . format ( best_lambda )) axs [ left ] . set_ylabel ( '$y$' ) axs [ left ] . set_ylim (( 0 , 1 )) axs [ left ] . set_xlim (( 0 , 1 )) axs [ left ] . legend ( loc = 2 ) coef = est . coef_ . ravel () axs [ right ] . semilogy ( np . abs ( coef ), marker = 'o' , label = r \"$\\lambda = {0} $\" . format ( best_lambda )) axs [ right ] . set_ylim (( 1e-04 , 1.0e+11 )) axs [ right ] . set_xlim ( 1 , 20 ) axs [ right ] . yaxis . set_label_position ( \"right\" ) axs [ right ] . set_ylabel ( r '$\\left|\\beta_ {j} \\right|$' ) axs [ right ] . legend ( loc = 'best' ) axs [ left ] . set_xlabel ( \"x\" ) axs [ right ] . set_xlabel ( r '$j$' ); if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab5/notebook/"},{"title":"Lab 10: Neural Networks using keras","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109A Introduction to Data Science Lab 10: Neural Networks using keras Harvard University Fall 2018 Instructors: Pavlos Protopapas and Kevin Rader Lab Instructor: Eleni Kaxiras Authors: David Sondak, Eleni Kaxiras, and Pavlos Protopapas In [ ]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get \\ ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) In [ ]: import numpy as np import matplotlib.pyplot as plt plt . rcParams [ 'axes.labelsize' ] = 14 plt . rcParams [ 'xtick.labelsize' ] = 12 plt . rcParams [ 'ytick.labelsize' ] = 12 % matplotlib inline Anatomy of an Artificial Neural Network In a previous lab we created our own neural network by writing some simple python functions. We focused on a regression problem where we tried to learn a function. We practiced using the logistic activation function in a network with multiple nodes, but a single or two hidden layers. Some of the key observations were: Increasing the number of nodes allows us to represent more complicated functions The weights and biases have a very big impact on the solution Finding the \"correct\" weights and biases is really hard to do manually There must be a better method for determining the weights and biases automatically We also didn't assess the effects of different activation functions or different network depths. The 3 parts of an ANN Part 1: the input layer (dimentions are determined from our dataset) Part 2: the internal architecture or hidden layers (the number of layers, the activation functions, the learnable parameters and other hyperparameters) Part 3: the output layer (what we want from the network) A word about .npy files Numpy arrays are faster than plain python lists, as we know. Numpy also offers a file format called .npy, which, when it comes to reading the same data multiple times from disk storage, is a lot faster than reading from a csv file. You can save any list or array into this format. In [ ]: np . save ( '/tmp/123' , np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]])) hello = np . load ( '/tmp/123.npy' ) In [ ]: hello Keras Basics https://keras.io/ Machine learning computations can be quite demanding. TensorFlow is a framework for representing complicated ML algorithms and executing them in any platform, from a phone to a distributed system using GPUs. Developed by Google Brain, TensorFlow is used very broadly today. keras , is a high-level API used for fast prototyping, advanced research, and production. We will use tf.keras which is TensorFlow's implementation of the keras API. Models are assemblies of layers The core data structure of Keras is a model , a way to organize layers. A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, one can use the Keras Functional API, which allows to build arbitrary graphs of layers. https://keras.io/models/model/ Everything you need to know about the Sequential model is here: https://keras.io/models/sequential/ Keras Installation If you haven't already, install Keras using the instructions found at https://keras.io/#installation Choose the TensorFlow installation instructions (found at https://www.tensorflow.org/install/ ). Example: Approximating a Gaussian using keras Let's try to redo the problem from last week. Recall that we had a function $$ \\begin{aligned} f\\left(x\\right) = e&#94;{-x&#94;{2}} \\end{aligned} $$ and we wanted to use a neural network to approximate that function. This week, we will use keras to do the true optimization. First, we import the necessary keras modules. In [ ]: import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential from tensorflow.keras.regularizers import l2 print ( tf . VERSION ) print ( tf . keras . __version__ ) In [ ]: # Checking if our machine has GPUs. Mine does not.. with tf . Session () as sess : devices = sess . list_devices () print ( devices ) Before we get started, we need to create some data . We will generate data points from an underlying function (here the Guassian). Then we will use the sklearn train_test_split method to split the dataset into training and testing portions. Remember that we train a machine learning algorithm on the training set and then assess the algorithm's performance on the test set. In [ ]: from sklearn.model_selection import train_test_split n_samples = 1050 # set the number of samples to take for each dataset test_size = 0.3 # set the proportion of data to hold out for testing # define the function and add noise def f_gauss ( x ): return np . exp ( - x * x ) + np . random . normal ( loc = 0 , scale =. 1 , size = x . shape [ 0 ]) X = np . random . permutation ( np . linspace ( - 10 , 10 , n_samples )) # choose some points from the function Y = f_gauss ( X ) # create training and testing data from this set of points X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = test_size ) Let's plot the data In [ ]: import matplotlib.pyplot as plt plt . scatter ( X_train , Y_train ) Building a keras network Now we will create a neural network model with keras. We're going to use a single layer and just 2 neurons in that layer. We will start with the sigmoid activation function. We also choose a linear output layer since we are doing regression. The loss function is selected to be the mean squared error (MSE) . In addition to these choices we must also specify our initial weights as well as the optimization method that will be used to minimize the loss function. The keras interface has many choises as to those hyperparameters. Part 1: First we start by defining the number of nodes in a layer and the input dimensions. If we have more than one layer we might need to define a value for the number of nodes (H) for each layer. H = input_dim = Then we instantiate the model model = models.Sequential() Part 2: Then we add the hidden layers. Adding layers and stacking them is done using .add() model.add(layers.Dense(H, input_dim=input_dim, activation='sigmoid')) An alternative way model = Sequential([ Dense(200, input_shape=(X_train.shape[1],), activation='relu'), Dense(100, activation='relu'), Dense(50, activation='relu'), Dense(4, activation='linear') ]) Part 3: We end with the final layer (output) model.add(layers.Dense(1, activation='linear')) Our model is not ready yet. We need to configure its learning process with .compile(): model.compile(loss='mean_squared_error', optimizer='sgd') If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code) model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(lr=0.01)) Our model is now ready to use. We haven't trained it yet, but we'll do that now using the fit method. Notice that we also need to specify the batch size for the stochastic gradient decent algorithm as well as the number of epochs to run. model.fit(X_train, Y_train, batch_size=100, epochs=100)#, verbose=1) Exercise 1: Build a NN with one hidden layer with 2 neurons . Use the tanh activation function. Train the model using the X_train dataset from above (train the model in this case means run .compile and .fit ). In [ ]: # your code here In [ ]: # %load solutions/NN_1_layer_2_nodes.py Great! We've trained a model. Now it's time to explore the results. Notice the loss function. In [ ]: # Some constants for our plots FIG_SIZE = ( 10 , 5 ) FONT_SIZE = 10 LABEL_SIZE = 15 In [ ]: # use our model to predict in the range we want X_range = np . linspace ( - 10 , 10 , 1000 ) y_pred = model . predict ( X_range ) In [ ]: # Plot the results fig , ax = plt . subplots ( 1 , 1 , figsize = FIG_SIZE ) ax . scatter ( X_train , Y_train , label = 'Training data' ) ax . plot ( X_range , y_pred , lw = 4 , color = 'r' , label = f 'MLP with one hidden layer and {H} nodes' ) ax . set_xlabel ( r '$X$' , fontsize = FONT_SIZE ) ax . set_ylabel ( r '$Y$' , fontsize = FONT_SIZE ) ax . set_title ( f 'NN with {len(model_history.model.layers)-1} layers, {H} nodes in each layer' , fontsize = LABEL_SIZE ) ax . tick_params ( labelsize = LABEL_SIZE ) ax . legend ( loc = 0 , fontsize = FONT_SIZE ) plt . show () Exercise 2: Change the number of neurons in the layer. Try changing the activation function to reLU . Can you get better results? What worked the best? In [ ]: # your code here In [ ]: # %load solutions/NN_1_layer_16_nodes.py Is the loss smaller now? You may access the results in a model by its .history In [ ]: model2_history . history [ 'loss' ][ - 1 ] Again let's use the new model to predict: In [ ]: # use our model to predict in the range we want X_range = np . linspace ( - 10 , 10 , 1000 ) y_pred = model2 . predict ( X_range ) fig , ax = plt . subplots ( 1 , 1 , figsize = FIG_SIZE ) ax . scatter ( X_train , Y_train , label = 'Training data' , alpha = 0.3 ) ax . scatter ( X_test , Y_test , label = 'Testing data' , alpha = 0.3 ) ax . plot ( X_range , y_pred , lw = 4 , color = 'r' , label = f 'NN with one hidden layer and {H} nodes' ) ax . set_xlabel ( r '$X$' , fontsize = FONT_SIZE ) ax . set_ylabel ( r '$Y$' , fontsize = FONT_SIZE ) ax . set_title ( f 'NN with {len(model2_history.model.layers)-1} layers, {H} nodes in each layer' , fontsize = LABEL_SIZE ) ax . tick_params ( labelsize = LABEL_SIZE ) ax . legend ( loc = 0 , fontsize = FONT_SIZE ) plt . show () Exercise 3: Plot the loss function as a function of the epochs. Hint: You can access the loss function values with the command: model_history.history['loss'] In [ ]: # your code here In [ ]: # %load solutions/print_history.py How good is the model? We can compute the $R&#94;{2}$ score to get a sense of the model performance. In [ ]: # evaluate the training and testing performance of your model # note: you should extract and check both the loss function and your evaluation metric from sklearn.metrics import r2_score as r2 train_score = model . evaluate ( X_train , Y_train , verbose = 1 ) print ( 'Train loss:' , train_score ) print ( 'Train R2:' , r2 ( Y_train , model . predict ( X_train ))) test_score = model . evaluate ( X_test , Y_test , verbose = 1 ) print ( 'Test loss:' , test_score ) print ( 'Test R2:' , r2 ( Y_test , model . predict ( X_test ))) Exercise 4 Let's add more layers. Fix the width $H$ and fit a MLP network with multiple hidden layers, each with the same width. Start with logistic or hyperbolic-tan activation functions for the hidden nodes and linear activation for the output. Experiment with the number of layers and observe the effect of this on the quality of the fit. In [ ]: # your code here In [ ]: # %load solutions/NN_10_layers_100_nodes.py In [ ]: # configure the model model3 . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) In [ ]: # fit the model - INTENSIVE model3_history = model3 . fit ( X_train , Y_train , batch_size = 256 , epochs = 1500 , verbose = 1 , \\ shuffle = True , validation_split = 0.3 ) In [ ]: len ( model3_history . model . layers ) In [ ]: # use our model to predict in the range we want num_epochs = f '{len(model2_history.epoch)}' X_range = np . linspace ( - 10 , 10 , 500 ) y_pred = model3 . predict ( X_range ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . scatter ( X_train , Y_train , label = 'Training data' ) ax . plot ( X_range , y_pred , lw = 4 , color = 'r' , label = f 'NN ( {num_epochs} epochs)' ) ax . set_xlabel ( r '$X$' , fontsize = 20 ) ax . set_ylabel ( r '$Y$' , fontsize = 20 ) ax . set_title ( f 'NN with {len(model3_history.model.layers)} layers, {H} nodes in each layer' , fontsize = LABEL_SIZE ) ax . tick_params ( labelsize = 20 ) ax . legend ( loc = 0 ) plt . show () In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( model2_history . history [ 'loss' ]), 'r' ) ax . plot ( np . sqrt ( model2_history . history [ 'val_loss' ]), 'b' , label = 'Val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Loss' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) In [ ]: # evaluate the training and testing performance of your model # note: you should extract and check both the loss function and your evaluation metric score = model2 . evaluate ( X_train , Y_train , verbose = 1 ) print ( 'Train loss:' , score ) print ( 'Train R2:' , r2 ( Y_train , model2 . predict ( X_train ))) In [ ]: score = model2 . evaluate ( X_test , Y_test , verbose = 1 ) print ( 'Test loss:' , score ) print ( 'Test R2:' , r2 ( Y_test , model2 . predict ( X_test ))) We got a better score this time. Overfitting the model Exercise 5 Usually we want to avoid overfitting of the data to our model. But here we want to achive overfitting! So we can regularize! There are a few reasons why a model overfits. One is the lack of data. So we will try to overfit by reducing the data. Try that with model3 and see if it overfits. In [ ]: # Having very few points in our data from sklearn.model_selection import train_test_split In [ ]: n_samples = 50 # set the number of samples to take for each dataset test_size = 0.3 # set the proportion of data to hold out for testing # define the function and add noise def f_gauss ( x ): return np . exp ( - x * x ) + np . random . normal ( loc = 0 , scale =. 1 , size = x . shape [ 0 ]) X = np . random . permutation ( np . linspace ( - 10 , 10 , n_samples )) # choose some points from the function Y = f_gauss ( X ) # create training and testing data from this set of points X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = test_size ) In [ ]: import matplotlib.pyplot as plt plt . scatter ( X_train , Y_train ) In [ ]: # number of hidden nodes H = 100 # input dimension input_dim = 1 # create sequential multi-layer perceptron model4 = models . Sequential () # layer 0 model4 . add ( layers . Dense ( H , input_dim = input_dim , activation = 'tanh' )) # layer 1 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 2 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 3 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 4 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 5 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 6 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 7 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 8 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 9 model4 . add ( layers . Dense ( H , activation = 'tanh' )) # layer 10 - output model4 . add ( layers . Dense ( 1 , activation = 'linear' )) In [ ]: # configure the model model4 . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) In [ ]: # fit the model - INTENSIVE model4_history = model4 . fit ( X_train , Y_train , batch_size = 256 , epochs = 1500 , verbose = 1 , \\ shuffle = True , validation_split = 0.2 ) In [ ]: # use our model to predict in the range we want num_epochs = f '{len(model4_history.epoch)}' X_range = np . linspace ( - 10 , 10 , 500 ) y_pred = model4 . predict ( X_range ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . scatter ( X_train , Y_train , label = 'Training data' ) ax . plot ( X_range , y_pred , lw = 4 , color = 'r' , label = f 'NN ( {num_epochs} epochs)' ) ax . set_xlabel ( r '$X$' , fontsize = 20 ) ax . set_ylabel ( r '$Y$' , fontsize = 20 ) ax . set_title ( f 'NN with {len(model4_history.model.layers)} layers, {H} nodes in each layer' , fontsize = LABEL_SIZE ) ax . tick_params ( labelsize = 20 ) ax . legend ( loc = 0 ) plt . show () In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( model4_history . history [ 'loss' ]), 'r' ) ax . plot ( np . sqrt ( model4_history . history [ 'val_loss' ]), 'b' , label = 'Val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Loss' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) Regularization Let's try adding a regularizer in our model: kernel_regularizer=regularizers.l2(l2) . Also let's create a function that takes the number of layers and the l2 value as the input and creates the model. Usage: def create_dense([10, 20], l2=0.01) will create a model with two hidden layers of 10 and 20 nodes each, l2=0.01 regularization and num_classes output nodes. In [ ]: from keras import regularizers In [ ]: H = 100 # number of hidden nodes input_dim = 1 model5 = models . Sequential () # Input layer of the neural network with ReLU activation function and L2 regularization model5 . add ( layers . Dense ( H , input_dim = input_dim , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( 0.01 ))) # hidden layers model5 . add ( layers . Dense ( H , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( 0.01 ))) model5 . add ( layers . Dense ( H , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( 0.01 ))) model5 . add ( layers . Dense ( H , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( 0.01 ))) model5 . add ( layers . Dense ( H , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( 0.01 ))) model5 . add ( layers . Dense ( H , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( 0.01 ))) model5 . add ( layers . Dense ( H , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( 0.01 ))) # output layer model5 . add ( layers . Dense ( 1 , activation = 'linear' )) In [ ]: # configure the model model5 . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) In [ ]: # fit the model - INTENSIVE model5_history = model5 . fit ( X_train , Y_train , batch_size = 256 , epochs = 1500 , verbose = 1 , \\ shuffle = True , validation_split = 0.2 ) In [ ]: # use our model to predict in the range we want num_epochs = f '{len(model5_history.epoch)}' X_range = np . linspace ( - 10 , 10 , 500 ) y_pred = model5 . predict ( X_range ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . scatter ( X_train , Y_train , label = 'Training data' ) ax . plot ( X_range , y_pred , lw = 4 , color = 'r' , label = f 'NN ( {num_epochs} epochs)' ) ax . set_xlabel ( r '$X$' , fontsize = 20 ) ax . set_ylabel ( r '$Y$' , fontsize = 20 ) ax . set_title ( f 'NN with {len(model5_history.model.layers)} layers, {H} nodes in each layer' , fontsize = LABEL_SIZE ) ax . tick_params ( labelsize = 20 ) ax . legend ( loc = 0 ) plt . show () That seems very good. Let's see the $R&#94;2$ In [ ]: from sklearn.metrics import r2_score as r2 In [ ]: score = model5 . evaluate ( X_test , Y_test , verbose = 1 ) print ( 'Test loss:' , score ) print ( 'Test R2:' , r2 ( Y_test , model5 . predict ( X_test ))) In [ ]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( model5_history . history [ 'loss' ]), 'r' ) ax . plot ( np . sqrt ( model5_history . history [ 'val_loss' ]), 'b' , label = 'Val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Loss' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab10/notebook/"},{"title":"Lab 8: Discriminant Analysis","text":"Lab 8","tags":"labs","url":"labs/lab8/"},{"title":"Lab 9: Random Forest and Boosting","text":"Lab 9","tags":"labs","url":"labs/lab9/"},{"title":"Lab 9: Random Forest and Boosting","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109A Introduction to Data Science Lab 9: Regression Trees, Bagged Trees, Random Forests and Boosting Harvard University Fall 2018 Instructors: Pavlos Protopapas and Kevin Rader Lab Instructor: Kevin Rader (today at least) Authors: Kevin Rader, Rahul Dave We will look here into the practicalities of fitting regression trees, random forests, and boosted trees. These involve out-of-bound estmates and cross-validation, and how you might want to deal with hyperparameters in these models. Along the way we will play a little bit with different loss functions, so that you start thinking about what goes in general into cooking up a machine learning model. In [15]: % matplotlib inline import numpy as np import scipy as sp import matplotlib as mpl import matplotlib.cm as cm import matplotlib.pyplot as plt import pandas as pd pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.notebook_repr_html' , True ) import seaborn as sns sns . set_style ( \"whitegrid\" ) sns . set_context ( \"poster\" ) Dataset First, the data. We will be attempting to predict the presidential election results (at the county level) from 2016, measured as 'votergap' = (trump - clinton) in percentage points, based mostly on demographic features of those counties. Let's quick take a peak at the data: In [16]: elect_df = pd . read_csv ( \"county_level_election.csv\" ) elect_df . head () Out[16]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state fipscode county population hispanic minority female unemployed income nodegree bachelor inactivity obesity density cancer votergap trump clinton 0 Colorado 8117 Summit County 27239 15.173 4.918 45.996 2.5 68352 5.4 48.1 8.1 13.1 46.0 46.2 -27.632 31.530 59.162 1 Colorado 8037 Eagle County 53653 30.040 5.169 47.231 3.1 76661 10.1 47.3 9.4 11.8 31.0 47.1 -19.897 36.058 55.955 2 Idaho 16067 Minidoka County 19226 34.070 5.611 49.318 3.7 46332 24.1 11.8 18.3 34.2 80.0 61.8 54.148 71.135 16.987 3 Colorado 8113 San Miguel County 7558 10.154 4.747 46.808 3.7 59603 4.7 54.4 12.4 16.7 5.7 62.6 -44.769 23.892 68.662 4 Utah 49051 Wasatch County 21600 13.244 4.125 48.812 3.4 65207 9.5 34.4 13.9 23.0 257.8 68.3 25.357 50.471 25.114 In [4]: from sklearn.model_selection import train_test_split In [5]: # split 80/20 train-test X = elect_df [[ 'population' , 'hispanic' , 'minority' , 'female' , 'unemployed' , 'income' , 'nodegree' , 'bachelor' , 'inactivity' , 'obesity' , 'density' , 'cancer' ]] response = elect_df [ 'votergap' ] Xtrain , Xtest , ytrain , ytest = train_test_split ( X , response , test_size = 0.2 ) In [6]: plt . hist ( ytrain ) Xtrain . hist ( column = [ 'minority' , 'population' , 'hispanic' , 'female' ]); How would you describe these variables? In [7]: print ( elect_df . shape ) print ( Xtrain . shape ) print ( Xtest . shape ) (3066, 18) (2452, 12) (614, 12) Regression Trees We will start by using a simple Decision Tree Regressor to predict votergap. Thats not the aim of this lab, so we'll run a few of these models without any cross-validation or 'regularization' just to illustrate what is going on. This is what you ought to keep in mind about decision trees. from the docs: max_depth : int or None, optional (default=None) The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_split : int, float, optional (default=2) The deeper the tree, the more prone you are to overfitting. The smaller min_samples_split , the more the overfitting. One may use min_samples_leaf instead. More samples per leaf, the higher the bias. In [8]: from sklearn.tree import DecisionTreeRegressor #x = np.arange(0, 2*np.pi, 0.1) #y = np.sin(x) + 0.1*np.random.normal(size=x.shape[0]) x = Xtrain [ 'minority' ] . values o = np . argsort ( x ) x = x [ o ] y = ytrain . values y = y [ o ] In [9]: plt . plot ( x , y , '.' ); In [10]: plt . plot ( np . log ( x ), y , '.' ); **Exercise 0** Which of the two versions of 'minority' would be a better choice to use as a predictor for inference? For prediction? In [11]: plt . plot ( np . log ( x ), y , '.' ) xx = np . log ( x ) . reshape ( - 1 , 1 ) for i in [ 1 , 2 ]: dtree = DecisionTreeRegressor ( max_depth = i ) dtree . fit ( xx , y ) plt . plot ( np . log ( x ), dtree . predict ( xx ), label = str ( i ), alpha = 1 - i / 10 , lw = 4 ) plt . legend (); In [12]: plt . plot ( np . log ( x ), y , '.' ) xx = np . log ( x ) . reshape ( - 1 , 1 ) for i in [ 500 , 200 , 100 , 20 ]: dtree = DecisionTreeRegressor ( min_samples_split = i ) dtree . fit ( xx , y ) plt . plot ( np . log ( x ), dtree . predict ( xx ), label = str ( i ), alpha = 0.8 , lw = 4 ) plt . legend (); In [13]: plt . plot ( np . log ( x ), y , '.' ) xx = np . log ( x ) . reshape ( - 1 , 1 ) for i in [ 500 , 200 , 100 , 20 ]: dtree = DecisionTreeRegressor ( max_depth = 6 , min_samples_split = i ) dtree . fit ( xx , y ) plt . plot ( np . log ( x ), dtree . predict ( xx ), label = str ( i ), alpha = 0.8 , lw = 4 ) plt . legend (); In [14]: #let's also include logminority as a predictor going forward xtemp = np . log ( Xtrain [ 'minority' ] . values ) Xtrain = Xtrain . assign ( logminority = xtemp ) Xtest = Xtest . assign ( logminority = np . log ( Xtest [ 'minority' ] . values )) Xtrain . head () Out[14]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population hispanic minority female unemployed income nodegree bachelor inactivity obesity density cancer logminority 521 37907 1.867 5.811 49.366 4.6 44077 12.7 14.5 32.4 32.3 435.8 175.9 1.759753 2933 15884 1.669 8.167 48.987 7.1 41810 9.3 16.8 23.8 28.7 18.8 324.1 2.100102 1462 1547297 13.974 54.930 52.731 6.9 39037 18.6 24.5 25.7 29.8 83.4 226.3 4.006060 307 62574 4.702 7.572 46.402 4.4 48681 11.7 14.2 20.6 38.0 505.1 154.3 2.024457 972 1789900 28.182 25.267 51.074 4.2 58127 15.3 29.7 24.0 29.1 8.8 204.1 3.229499 **Exercise 1** Perform 5-fold cross-validation to determine what the best max_depth would be for a single regression tree using the entire 'Xtrain' feature set. Visualize the results with mean +/- 2 sd's across the validation sets. In [ ]: from sklearn.model_selection import cross_val_score # your code here depths = list ( range ( 1 , 21 )) train_scores = [] cvmeans = [] cvstds = [] cv_scores = [] for depth in depths : dtree = DecisionTreeRegressor ( max_depth = depth ) # Perform 5-fold cross validation and store results cv_scores = cross_val_score ( dtree , Xtrain , ytrain , cv = 5 ) cvmeans = append () cvmeans = np . array ( cvmeans ) cvstds = np . array ( cvstds ) In [ ]: # your code here # plot means and shade the 2 SD interval #plt.fill_between(depths, cvmeans - 2*cvstds, cvmeans + 2*cvstds, alpha=0.3); Ok with this discussion in mind, lets improve this model by Bagging. Bootstrap-Aggregating (called Bagging) Whats the basic idea? A Single Decision tree is likely to overfit. So lets introduce replication through Bootstrap sampling. Bagging uses bootstrap resampling to create different training datasets. This way each training will give us a different tree. Added bonus: the left off points can be used to as a natural \"validation\" set, so no need to Since we have many trees that we will average over for prediction , we can choose a large max_depth and we are ok as we will rely on the law of large numbers to shrink this large variance, low bias approach for each individual tree. In [ ]: from sklearn.utils import resample ntrees = 500 estimators = [] R2s = [] yhats_test = np . zeros (( Xtest . shape [ 0 ], ntrees )) plt . plot ( np . log ( x ), y , '.' ) for i in range ( ntrees ): simpletree = DecisionTreeRegressor ( max_depth = 3 ) boot_xx , boot_y = resample ( Xtrain [[ 'logminority' ]], ytrain ) estimators = np . append ( estimators , simpletree . fit ( boot_xx , boot_y )) R2s = np . append ( R2s , simpletree . score ( Xtest [[ 'logminority' ]], ytest )) yhats_test [:, i ] = simpletree . predict ( Xtest [[ 'logminority' ]]) plt . plot ( np . log ( x ), simpletree . predict ( np . log ( x ) . reshape ( - 1 , 1 )), 'red' , alpha = 0.05 ) In [ ]: yhats_test . shape **Exercise 2** Edit the code below (which is just copied from above) to refit many bagged trees on the entire xtrain feature set (without the plot...lots of predictors now so difficult to plot). Summarize how each of the separate trees performed (both numerically and visually) using $R&#94;2$ as the metric. How do they perform on average? Combine the trees into one prediction and evaluate it using $R&#94;2$. Briefly discuss the results. How will the results above change if 'max_depth=4' is increased? What if it is decreased? In [ ]: from sklearn.metrics import r2_score ntrees = 500 estimators = [] R2s = [] yhats_test = np . zeros (( Xtest . shape [ 0 ], ntrees )) for i in range ( ntrees ): dtree = DecisionTreeRegressor ( max_depth = 3 ) boot_xx , boot_y = resample ( Xtrain [[ 'logminority' ]], ytrain ) estimators = np . append ( estimators , dtree . fit ( boot_xx , boot_y )) R2s = np . append ( R2s , dtree . score ( Xtest [[ 'logminority' ]], ytest )) yhats_test [:, i ] = dtree . predict ( Xtest [[ 'logminority' ]]) # your code here Your answer here Random Forests What's the basic idea? Bagging alone is not enough randomization, because even after bootstrapping, we are mainly training on the same data points using the same variablesn, and will retain much of the overfitting. So we will build each tree by splitting on \"random\" subset of predictors at each split (hence, each is a 'random tree'). This can't be done in with just one predcitor, but with more predictors we can choose what predictors to split on randomly and how many to do this on. Then we combine many 'random trees' together by averaging their predictions, and this gets us a forest of random trees: a random forest . Below we create a hyper-param Grid. We are preparing to use the bootstrap points not used in training for validation. max_features : int, float, string or None, optional (default=\"auto\") - The number of features to consider when looking for the best split. max_features : Default splits on all the features and is probably prone to overfitting. You'll want to validate on this. You can \"validate\" on the trees n_estimators as well but many a times you will just look for the plateau in the trees as seen below. From decision trees you get the max_depth , min_samples_split , and min_samples_leaf as well but you might as well leave those at defaults to get a maximally expanded tree. In [ ]: from sklearn.ensemble import RandomForestRegressor In [ ]: # code from # Adventures in scikit-learn's Random Forest by Gregory Saunders from itertools import product from collections import OrderedDict param_dict = OrderedDict ( n_estimators = [ 400 , 600 , 800 ], max_features = [ 0.2 , 0.4 , 0.6 , 0.8 ] ) param_dict . values () Using the OOB score. We have been putting \"validate\" in quotes. This is because the bootstrap gives us left-over points! So we'll now engage in our very own version of a grid-search, done over the out-of-bag scores that sklearn gives us for free In [ ]: from itertools import product In [ ]: #make sure ytrain is the correct data type...in case you have warnings #print(yytrain.shape,ytrain.shape,Xtrain.shape) #ytrain = np.ravel(ytrain) #Let's Cross-val. on the two 'hyperparameters' we based our grid on earlier results = {} estimators = {} for ntrees , maxf in product ( * param_dict . values ()): params = ( ntrees , maxf ) est = RandomForestRegressor ( oob_score = True , n_estimators = ntrees , max_features = maxf , max_depth = 50 , n_jobs =- 1 ) est . fit ( Xtrain , ytrain ) results [ params ] = est . oob_score_ estimators [ params ] = est outparams = max ( results , key = results . get ) outparams In [ ]: rf1 = estimators [ outparams ] In [ ]: results In [ ]: rf1 . score ( Xtest , ytest ) Finally you can find the feature importance of each predictor in this random forest model. Whenever a feature is used in a tree in the forest, the algorithm will log the decrease in the splitting criterion (such as gini). This is accumulated over all trees and reported in est.feature_importances_ In [ ]: pd . Series ( rf1 . feature_importances_ , index = list ( Xtrain )) . sort_values () . plot ( kind = \"barh\" ) Since our response isn't very symmetric, we may want to suppress outliers by using the mean_absolute_error instead. In [ ]: from sklearn.metrics import mean_absolute_error mean_absolute_error ( ytest , rf1 . predict ( Xtest )) **Bonus Exercise (we will likely skip this)** Tune the 'RandomForestRegressor' above to minimize 'mean_absolute_error' instead of the default __ Note: sklearn supports this ( criterion='mae' ) since v0.18, but does not have completely arbitrary loss functions for Random Forests. In [ ]: # your code here Note: instead of using oob scoring, we could do cross-validation, and a cv of 3 will roughly be comparable (same approximate train-vs.-validation set sizes). But this will take much more time as you are doing the fit 3 times plus the bootstraps. So at least three times as long! In [ ]: param_dict2 = OrderedDict ( n_estimators = [ 600 ], max_features = [ 0.2 , 0.4 , 0.6 , 0.8 ] ) In [ ]: from sklearn.model_selection import GridSearchCV est2 = RandomForestRegressor ( oob_score = False ) gs = GridSearchCV ( est2 , param_grid = param_dict2 , cv = 3 , n_jobs =- 1 ) gs . fit ( Xtrain , ytrain ) In [ ]: rf2 = gs . best_estimator_ rf2 In [ ]: gs . best_score_ **Exercise 3: thinking questions** What are the 3 hyperparameters for a random forest (one of the hyperparameters come in many flavors )? Which hyperparameters need to be tuned? How would you go about tuning these hyperparemeters? Your answer here Seeing error as a function of the proportion of predictors at each split Let's look to see how max_features affects performance on the training set. In [ ]: # from http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html feats = param_dict [ 'max_features' ] # error_rate = OrderedDict (( label , []) for label in feats ) # Range of `n_estimators` values to explore. min_estimators = 200 step_estimators = 100 num_steps = 3 max_estimators = min_estimators + step_estimators * num_steps for label in feats : for i in range ( min_estimators , max_estimators + 1 , step_estimators ): clf = RandomForestRegressor ( oob_score = True , max_features = label ) clf . set_params ( n_estimators = i ) clf . fit ( Xtrain , ytrain ) # Record the OOB error for each `n_estimators=i` setting. oob_error = 1 - clf . oob_score_ error_rate [ label ] . append (( i , oob_error )) In [ ]: print ( feats ) In [ ]: # Generate the \"OOB error rate\" vs. \"n_estimators\" plot. for label , clf_err in error_rate . items (): xs , ys = zip ( * clf_err ) plt . plot ( xs , ys , label = label ) plt . xlim ( min_estimators , max_estimators ) plt . xlabel ( \"n_estimators\" ) plt . ylabel ( \"OOB error rate\" ) plt . legend ( loc = \"upper right\" ) plt . show () Boosting Regression Trees Adaboost Classification, which you will be doing in your homework, is a special case of a gradient-boosted algorithm. Gradient Bossting is very state of the art, and has major connections to logistic regression, gradient descent in a functional space, and search in information space. See Schapire and Freund's MIT Press book for details (Google is a wonderful thing). But briefly, let us cover the idea here. The idea is that we will use a bunch of weak 'learners' (aka, models) which are fit sequentially. The first one fits the signal, the second one the first model's residual, the third the second model's residual, and so on. At each stage we upweight the places that our previous model did badly on. First let us illustrate. In [ ]: from sklearn.ensemble import AdaBoostRegressor estab = AdaBoostRegressor ( base_estimator = DecisionTreeRegressor ( max_depth = 1 ), n_estimators = 200 , learning_rate = 1.0 ) estab . fit ( xx , y ) staged_predict_generator = estab . staged_predict ( xx ) In [ ]: # code from http://nbviewer.jupyter.org/github/pprett/pydata-gbrt-tutorial/blob/master/gbrt-tutorial.ipynb import time from IPython import display plt . plot ( xx , y , '.' ); counter = 0 for stagepred in staged_predict_generator : counter = counter + 1 if counter in [ 1 , 2 , 4 , 8 , 10 , 50 , 100 , 200 ]: plt . plot ( xx , stagepred , alpha = 0.7 , label = str ( counter ), lw = 4 ) plt . legend (); display . display ( plt . gcf ()) display . clear_output ( wait = True ) time . sleep ( 1 ) In [ ]: print ( counter , i ) Ok, so this demonstration helps us understand some things about boosting. n_estimators is the number of trees, and thus the stage in the fitting. It also controls the complexity for us. The more trees we have the more we fit to the tiny details. staged_predict gives us the prediction at each step once again max_depth from the underlying decision tree tells us the depth of the tree. But here it tells us the amount of features interactions we have, not just the scale of our fit. But clearly it increases the variance again. Ideas from decision trees remain. For example, increase min_samples_leaf or decrease max_depth to reduce variance and increase the bias. **Exercise 4** What do you expect to happen if you increase max_depth to 5? Edit the code above to explore the result. What do you expect to happen if you put max_depth back to 1 and decrease the learning_rate to 0.1? Edit the code above to explore the result. Do a little work to find some sort of 'best' values of max_depth and learning_rate . Does this result make sense? In [ ]: # your code here Note: what's the relationship between residuals and the gradient? Pavlos showed in class that for the squared loss, taking the gradient in the \"data point functional space\", ie a N-d space for N data points with each variable being $f(x_i)$ just gives us the residuals. It turns out that the gradient descent is a more general idea, and one can use this for different losses. And the upweighting of poorly fit points in AdaBoost is simply a weighing by gradient. If the gradient (or residual) is high it means you are far away from optimum in this functional space, and if you are at 0, you have a flat gradient! The ideas from the general theory of gradient descent tell us this: we can slow the learning by shrinking the predictions of each tree by some small number, which is called the learning_rate (learning_rate). This \"shrinkage\" helps us not overshoot, but for a finite number of iterations also simultaneously ensures we dont overfit by being in the neighboorhood of the minimum rather than just at it! But we might need to increase the iterations some to get into the minimum area. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab9/notebook/"},{"title":"Lecture 2: Notebook Grammar of Data","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS109A Introduction to Data Science Lecture2, Notebook: Pandas, and the Grammar of Data (created with Rahul Dave for cs109) Harvard University Summer 2018 Instructors: Pavlos Protopapas and Kevin Rader In [3]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[3]: h1 { padding-top: 25px; padding-bottom: 25px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } h2 { padding-top: 10px; padding-bottom: 10px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } div.exercise { background-color: #ffcccc; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; } div.theme { background-color: #DDDDDD; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 18pt; } p.q1 { padding-top: 5px; padding-bottom: 5px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } header { padding-top: 35px; padding-bottom: 35px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } We'd like a data structure that: can represent the columns in the data by their name has a structure that can easily store variables of different types that stores column names, and that we can reference by column name as well as by indexed position and it would be nice if this data structure came with built-in functions that we can use to manipulate it. Pandas is a package/library that does all of this! The library is built on top of numpy. There are two basic pandas objects, series and dataframes , which can be thought of as enhanced versions of 1D and 2D numpy arrays, respectively. Indeed Pandas attempts to keep all the efficiencies that numpy gives us. For reference, here is a useful pandas cheat sheet and the pandas documentation . The basic EDA workflow Below is a basic checklist for the early stages of exploratory data analysis in Python. While not universally applicable, the rubric covers patterns which recur in several data analysis contexts, so useful to keep it in mind when encountering a new dataset. The basic workflow (enunciated in this form by Chris Beaumont) is as follows: Build a Dataframe from the data (ideally, put all data in this object) Clean the Dataframe. It should have the following properties: Each row describes a single object Each column describes a property of that object Columns are numeric whenever appropriate Columns contain atomic properties that cannot be further decomposed Explore global properties . Use histograms, scatter plots, and aggregation functions to summarize the data. Explore group properties . Use groupby, queries, and small multiples to compare subsets of the data. This process transforms the data into a format which is easier to work with, gives you a basic overview of the data's properties, and likely generates several questions for you to follow-up on in subsequent analysis. In [4]: # The %... is an iPython thing, and is not part of the Python language. # In this case we're just telling the plotting library to draw things on # the notebook, instead of on a separate window. % matplotlib inline # See all the \"as ...\" contructs? They're just aliasing the package names. # That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot(). import numpy as np import scipy as sp import matplotlib as mpl import matplotlib.cm as cm import matplotlib.pyplot as plt import pandas as pd import time pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.notebook_repr_html' , True ) import seaborn as sns sns . set_style ( \"whitegrid\" ) sns . set_context ( \"poster\" ) Building a dataframe The easiest way to build a dataframe is simply to read in a CSV file. We WILL see an example of this here, and we shall see more examples in labs. We'll also see how we may combine multiple data sources into a larger dataframe. This example is adapted from: https://github.com/tthibo/SQL-Tutorial The first 3 lines of the file ( !head -3 data/candidates.txt on mac/unix) look like this. id|first_name|last_name|middle_name|party 33|Joseph|Biden||D 36|Samuel|Brownback||R In [8]: dfcand = pd . read_csv ( \"../data/candidates.txt\" , sep = '|' ) dfcand . head ( 10 ) Out[8]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id first_name last_name middle_name party 0 33 Joseph Biden NaN D 1 36 Samuel Brownback NaN R 2 34 Hillary Clinton R. D 3 39 Christopher Dodd J. D 4 26 John Edwards NaN D 5 22 Rudolph Giuliani NaN R 6 24 Mike Gravel NaN D 7 16 Mike Huckabee NaN R 8 30 Duncan Hunter NaN R 9 31 Dennis Kucinich NaN D A pandas dataframe is a set of columns pasted together into a spreadsheet, as shown in the schematic below, which is taken from the cheatsheet above. The columns in pandas are called series objects. In [7]: #All the columns in this dataframe: dfcand . columns Out[7]: Index(['id', 'first_name', 'last_name', 'middle_name', 'party'], dtype='object') And the types of these columns: In [37]: dfcand . dtypes Out[37]: id int64 first_name object last_name object middle_name object party object dtype: object Access to a particular column can be obtained by treating the column name as an \"attribute\" of the dataframe: In [12]: type ( dfcand [ 'first_name' ]) Out[12]: pandas.core.series.Series But Pandas supports a dictionary like access to columns. This is very useful when column names have spaces: Python variables cannot have spaces in them. In [14]: dfcand [[ 'middle_name' , 'first_name' ]] Out[14]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } middle_name first_name 0 NaN Joseph 1 NaN Samuel 2 R. Hillary 3 J. Christopher 4 NaN John 5 NaN Rudolph 6 NaN Mike 7 NaN Mike 8 NaN Duncan 9 NaN Dennis 10 NaN John 11 NaN Barack 12 NaN Ron 13 NaN Bill 14 NaN Mitt 15 NaN Tom 16 D. Fred We can also get sub-dataframes by choosing a set of series. We pass a list of the columns we want as \"dictionary keys\" to the dataframe. In [40]: columns_i_want = [ 'first_name' , 'last_name' ] dfcand [[ 'first_name' , 'last_name' ]] #dfcand[columns_i_want] Out[40]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name last_name 0 Joseph Biden 1 Samuel Brownback 2 Hillary Clinton 3 Christopher Dodd 4 John Edwards 5 Rudolph Giuliani 6 Mike Gravel 7 Mike Huckabee 8 Duncan Hunter 9 Dennis Kucinich 10 John McCain 11 Barack Obama 12 Ron Paul 13 Bill Richardson 14 Mitt Romney 15 Tom Tancredo 16 Fred Thompson Categoricals Even though party is a string, it takes on only a finite set of values, 'D', and 'R'. We can model this: In [15]: dfcand . party . unique () Out[15]: array(['D', 'R'], dtype=object) In [16]: dfcand . dtypes dfcand [ 'party' ] = dfcand [ 'party' ] . astype ( \"category\" ) In [17]: dfcand . dtypes Out[17]: id int64 first_name object last_name object middle_name object party category dtype: object In [44]: dfcand . head () Out[44]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } id first_name last_name middle_name party 0 33 Joseph Biden NaN D 1 36 Samuel Brownback NaN R 2 34 Hillary Clinton R. D 3 39 Christopher Dodd J. D 4 26 John Edwards NaN D In [20]: type ( dfcand . party [ 0 ]) Out[20]: str In [46]: dfcand . party . cat . categories Out[46]: Index(['D', 'R'], dtype='object') In [47]: dfcand . party . cat . ordered Out[47]: False Keep in mind that this is a relatively new feature of Pandas. You dont need to do this, but might find it useful to keep your types straight. Using categoricals in machine learning algorithms is more complex and usually involves a process called One Hot Encoding (more on this in the coming labs) Another piece of data This is a file of people who have contributed money to candidates: ( !head -3 data/contributors_with_candidate_id.txt ) id|last_name|first_name|middle_name|street_1|street_2|city|state|zip|amount|date|candidate_id |Agee|Steven||549 Laurel Branch Road||Floyd|VA|24091|500.00|2007-06-30|16 |Ahrens|Don||4034 Rennellwood Way||Pleasanton|CA|94566|250.00|2007-05-16|16 In [21]: dfcwci = pd . read_csv ( \"../data/contributors_with_candidate_id.txt\" , sep = \"|\" ) dfcwci . head () Out[21]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 NaN Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 1 NaN Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 2 NaN Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 3 NaN Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 4 NaN Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 Cleaning Data Most of the techniques you will learn about in Pandas are all about getting data in a form that can be used for further analysis. Cleaning usually means: dealing with missing values, transforming types appropriately, and taking care of data integrity. But we'll lump everything required to transform data to a form appropriate for analysis cleaning , even if what we are doing is for example, combining multiple data sets, or producing processed data from raw data. Lets start with some regular cleaning! In [49]: dfcwci . head () Out[49]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } id last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 NaN Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 1 NaN Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 2 NaN Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 3 NaN Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 4 NaN Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 In [22]: del dfcwci [ 'id' ] dfcwci . head () Out[22]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 1 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 2 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 3 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 We can see the size of our data In [51]: dfcwci . shape , dfcand . shape Out[51]: ((175, 11), (17, 5)) We will do some more cleaning soon but let us see the EDA process as sliced from another angle: data transformation, used both for cleaning and for seeing single column and multiple column properties in the data. Data transformation: Single Table Verbs Let us now focus on core data manipulation commands. These are universal across systems, and by identifying them, we can quickly ask how to do these when we encounter a new system. See https://gist.github.com/TomAugspurger/6e052140eaa5fdb6e8c0/ which has a comparison of dplyr and pandas. I stole and modified this table from there: dplyr has a small set of nicely defined verbs, which Hadley Wickham has used to identify core data manipulation commands. Here are listed the closest SQL and Pandas verbs, so we can see the universality of these manipulations. VERB dplyr pandas SQL QUERY/SELECTION filter() (and slice()) query() (and loc[], iloc[]) SELECT WHERE SORT arrange() sort() ORDER BY SELECT-COLUMNS/PROJECTION select() (and rename()) [](__getitem__) (and rename()) SELECT COLUMN SELECT-DISTINCT distinct() unique(),drop_duplicates() SELECT DISTINCT COLUMN ASSIGN mutate() (and transmute()) assign ALTER/UPDATE AGGREGATE summarise() describe(), mean(), max() None, AVG(),MAX() SAMPLE sample_n() and sample_frac() sample() implementation dep, use RAND() GROUP-AGG group_by/summarize groupby/agg, count, mean GROUP BY DELETE ? drop/masking DELETE/WHERE We'll tackle these one by one in Pandas, since these are data manipulations you will do all the time. QUERY In [52]: dfcwci . amount < 400 Out[52]: 0 False 1 True 2 True 3 True 4 True 5 False 6 False 7 True 8 False 9 False 10 False 11 False 12 True 13 False 14 True 15 False 16 False 17 True 18 True 19 False 20 False 21 False 22 True 23 False 24 False 25 True 26 True 27 True 28 False 29 True ... 145 True 146 True 147 True 148 True 149 True 150 False 151 True 152 True 153 True 154 True 155 False 156 False 157 True 158 True 159 False 160 True 161 True 162 True 163 False 164 False 165 True 166 True 167 True 168 False 169 False 170 True 171 False 172 True 173 False 174 False Name: amount, Length: 175, dtype: bool This gives us Trues and Falses. Such a series is called a mask . A mask is the basis of filtering. We can do: In [53]: dfcwci [ dfcwci . amount < 400 ] . head () Out[53]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 1 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 2 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 3 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 7 Aldridge Brittni NaN 808 Capitol Square Place, SW NaN Washington DC 20024 250.0 2007-06-06 16 Notice that the dataframe has been filtered down to only include those contributions with amount < 400. The rows with False in the mask have been eliminated, and those with True in the mask have been kept. In [54]: np . sum ( dfcwci . amount < 400 ), np . mean ( dfcwci . amount < 400 ) Out[54]: (132, 0.75428571428571434) Why did that work? The booleans are coerced to integers as below: In [55]: 1 * True , 1 * False Out[55]: (1, 0) Or directly, in Pandas, which works since the comparison is a pandas Series. In [56]: ( dfcwci . amount < 400 ) . mean () Out[56]: 0.75428571428571434 In [57]: dfcwci . describe () Out[57]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } zip amount candidate_id count 1.750000e+02 175.000000 175.000000 mean 3.780014e+08 3.418114 28.000000 std 3.628278e+08 1028.418999 7.823484 min 2.474000e+03 -2592.000000 16.000000 25% 9.336700e+04 -175.000000 20.000000 50% 3.233313e+08 100.000000 32.000000 75% 7.816946e+08 300.000000 35.000000 max 9.951532e+08 4600.000000 37.000000 You can combine queries. Note that we use Python's & operator instead of and . This is because we are \"Boolean AND\"ing masks to get a series of True's And Falses. In [58]: dfcwci [( dfcwci . state == 'VA' ) & ( dfcwci . amount < 400 )] Out[58]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 27 Buckheit Bruce NaN 8904 KAREN DR NaN FAIRFAX VA 220312731 100.00 2007-09-19 20 77 Ranganath Anoop NaN 2507 Willard Drive NaN Charlottesville VA 22903 -100.00 2008-04-21 32 88 Perreault Louise NaN 503 Brockridge Hunt Drive NaN Hampton VA 23666 -34.08 2008-04-21 32 145 ABDELLA THOMAS M. 4231 MONUMENT WALL WAY #340 NaN FAIRFAX VA 220308440 50.00 2007-09-30 35 Here is another way to write the query: In [29]: dfcwci [( dfcwci . amount < 0 )] Out[29]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 29 Buckel Linda NaN PO Box 683130 NaN Park City UT 840683130 -2300.00 2007-08-14 20 40 Buchanan John NaN 2025 NW 29th Rd NaN Boca Raton FL 334316303 -500.00 2007-09-24 20 43 Buchanan John NaN 2025 NW 29th Rd NaN Boca Raton FL 334316303 -700.00 2007-08-28 20 50 BYNUM HERBERT NaN 332 SUNNYSIDE ROAD NaN TAMPA FL 336177249 -500.00 2008-03-10 22 51 BYINGTON MARGARET E. 2633 MIDDLEBORO LANE N.E. NaN GRAND RAPIDS MI 495061254 -2300.00 2008-03-03 22 52 BYERS BOB A. 13170 TELFAIR AVENUE NaN SYLMAR CA 913423573 -2300.00 2008-03-07 22 53 BYERS AUDREY NaN 2658 LADBROOK WAY NaN THOUSAND OAKS CA 913615073 -200.00 2008-03-07 22 54 BUSH KRYSTIE NaN P.O. BOX 61046 NaN DENVER CO 802061046 -2300.00 2008-03-06 22 55 BUSH ERIC NaN P.O. BOX 61046 NaN DENVER CO 802061046 -2300.00 2008-03-06 22 56 BURTON SUSAN NaN 9338 DEER CREEK DRIVE NaN TAMPA FL 336472286 -2300.00 2008-03-05 22 57 BURTON STEVEN G. 9938 DEER CREEK DRIVE NaN TAMPA FL 33647 -2300.00 2008-03-05 22 58 BURTON GLENN M. 4404 CHARLESTON COURT NaN TAMPA FL 336092620 -2300.00 2008-03-05 22 59 BURKHARDT CRAIG S. 910 15TH STREET N.W. NaN WASHINGTON DC 200052503 -500.00 2008-03-07 22 60 BURKHARDT CRAIG S. 910 15TH STREET N.W. NaN WASHINGTON DC 200052503 -1000.00 2008-03-07 22 61 BURKHARDT BARBARA NaN 910 15TH STREET N.W. NaN WASHINGTON DC 200052503 -500.00 2008-03-07 22 62 BURKE SUZANNE M. 3401 EVANSTON NaN SEATTLE WA 981038677 -700.00 2008-03-05 22 63 BURKE GAIL NaN 165 E. 32ND STREET APARTMENT 9E NEW YORK NY 100166014 -2000.00 2008-03-05 22 64 BURKE DONALD J. 12 LOMPOC NaN RANCHO SANTA MARGA CA 926881817 -2300.00 2008-03-11 22 65 BURGERT RONALD L. 5723 PLUMTREE DRIVE NaN DALLAS TX 752524926 -1000.00 2008-03-05 22 66 BULL BARTLE B. 439 E. 51ST STREET NaN NEW YORK NY 100226473 -800.00 2008-03-10 22 67 BULL BARTLE B. 439 E. 51ST STREET NaN NEW YORK NY 100226473 -1000.00 2008-03-10 22 68 BUKOWSKI DANIEL J. 702 S. WRIGHT STREET NaN NAPERVILLE IL 605406736 -100.00 2008-03-10 22 69 BUISSON MARGARET A. P.O. BOX 197029 NaN LOUISVILLE KY 402597029 -200.00 2008-03-11 22 70 BUCKLEY WALTER W. 1635 COUNTRY ROAD NaN BETHLEHEM PA 180155718 -100.00 2008-03-05 22 71 BUCKLEY MARJORIE B. 1635 COUNTRY ROAD NaN BETHLEHEM PA 180155718 -100.00 2008-03-05 22 72 BRUNO JOHN NaN 10136 WINDERMERE CHASE BLVD. NaN GOTHA FL 347344707 -2300.00 2008-03-06 22 73 BRUNO IRENE NaN 10136 WINDERMERE CHASE BLVD. NaN GOTHA FL 347344707 -2300.00 2008-03-06 22 74 BROWN TIMOTHY J. 26826 MARLOWE COURT NaN STEVENSON RANCH CA 913811020 -2300.00 2008-03-06 22 75 Schuff Bryan NaN 1700 W Sweden Rd NaN Brockport NY 14420 -25.00 2008-08-22 32 76 Hobbs James NaN 229 Cherry Lane NaN White House TN 37188 -25.00 2008-08-19 32 77 Ranganath Anoop NaN 2507 Willard Drive NaN Charlottesville VA 22903 -100.00 2008-04-21 32 78 Nystrom Michael A 93A Fairmont Street NaN Arlington MA 2474 -503.00 2008-04-21 32 79 Muse Nina Jo 2915 Toro Canyon Rd NaN Austin TX 78746 -50.00 2008-04-21 32 80 Waddell James L. 1823 Spel Lane SW NaN Rochester MN 55902 -28.00 2008-04-21 32 81 Brucks William C. PO Box 391 NaN Corona del Mar CA 92625 -150.00 2008-04-21 32 82 Kuehn David NaN 14502 West 93rd Street NaN Lenexa KS 66215 -330.00 2008-04-21 32 83 Verster Jeanette M. 7220 SW 61st St NaN Miami FL 331431807 -1000.00 2008-04-21 32 84 Uihlein Richard NaN 1396 N Waukegan Rd NaN Lake Forest IL 600451147 -2300.00 2008-04-21 32 85 Eskenberry Robert P 10960 Gray Cir NaN Westminster CO 80020 -223.00 2008-04-21 32 86 Froehling Alan L. 302 Broadway St NaN Mount Vernon IL 628645116 -844.80 2008-04-21 32 87 Duryea Marcia A. 123 Bayview Ave NaN Amityville NY 11701 -299.50 2008-04-21 32 88 Perreault Louise NaN 503 Brockridge Hunt Drive NaN Hampton VA 23666 -34.08 2008-04-21 32 89 Rozenfeld Timur NaN 57 Herbert Road NaN Robbinsville NJ 8691 -777.95 2008-04-21 32 90 Kazor Christopher M 707 Spindletree ave NaN Naperville IL 60565 -2592.00 2008-04-21 32 91 Lehner Thomas S. 2701 Star Lane NaN Wadsworth OH 44281 -200.00 2008-04-21 32 92 Plummer Joseph NaN 587 Blake Hill Rd NaN New Hampton NH 32564424 -24.60 2008-04-21 32 93 Raught Philip M 4714 Plum Way NaN Pittsburgh PA 15201 -1046.00 2008-04-21 32 94 Ferrara Judith D 1508 Waterford Road NaN Yardley PA 19067 -1100.00 2008-04-21 32 95 Johnson Cathleen E. 1003 Justin Ln Apt 2016 NaN Austin TX 787572648 -14.76 2008-04-21 32 96 Sanford Bradley NaN 940 Post St #43 NaN San Francisco CA 94109 -24.53 2008-04-21 32 97 Gaarder Bruce NaN PO Box 4085 NaN Mountain Home AFB ID 83648 -261.00 2008-04-21 32 98 Choe Hyeokchan NaN 207 Bridle Way NaN Fort Lee NJ 70246302 -39.50 2008-04-21 32 99 Jacobs Richard G. 14337 Tawya Rd NaN Apple Valley CA 923075545 -1000.00 2008-04-21 32 110 Reid Elizabeth NaN 73 W Patent Rd OPHIR FARM NORTH Bedford Hills NY 105072222 -350.00 2008-08-28 34 111 Reich Thomas NaN 499 Park Ave NaN New York NY 100221240 -2300.00 2008-08-28 34 125 BOURNE TRAVIS NaN LAGE KAART 77 NaN BRASSCHATT NaN 2930 -500.00 2008-11-20 35 126 SECRIST BRIAN L. 3 MULE DEER TRAIL NaN LITTLETON CO 801275722 -1000.00 2008-04-07 35 127 TOLLESTRUP TRAVIS W. 16331 WINECREEK RD. NaN SAN DIEGO CA 92127 -1000.00 2008-05-15 35 In [30]: dfcwci [( dfcwci [ 'last_name' ] == 'BYERS' )] Out[30]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 52 BYERS BOB A. 13170 TELFAIR AVENUE NaN SYLMAR CA 913423573 -2300.0 2008-03-07 22 53 BYERS AUDREY NaN 2658 LADBROOK WAY NaN THOUSAND OAKS CA 913615073 -200.0 2008-03-07 22 For cleaning, we might want to use this querying ability In [60]: dfcwci [ dfcwci . state . isnull ()] Out[60]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 125 BOURNE TRAVIS NaN LAGE KAART 77 NaN BRASSCHATT NaN 2930 -500.0 2008-11-20 35 Or the opposite, which is probably more useful in making the selection: In [61]: dfcwci [ dfcwci . state . notnull ()] . head () Out[61]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 1 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 2 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 3 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 For categoricals you can use isin . You can use Boolean not on the mask to implement not in. In [62]: dfcwci [ dfcwci . state . isin ([ 'VA' , 'WA' ])] . head ( 10 ) Out[62]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.00 2007-06-30 16 27 Buckheit Bruce NaN 8904 KAREN DR NaN FAIRFAX VA 220312731 100.00 2007-09-19 20 62 BURKE SUZANNE M. 3401 EVANSTON NaN SEATTLE WA 981038677 -700.00 2008-03-05 22 77 Ranganath Anoop NaN 2507 Willard Drive NaN Charlottesville VA 22903 -100.00 2008-04-21 32 88 Perreault Louise NaN 503 Brockridge Hunt Drive NaN Hampton VA 23666 -34.08 2008-04-21 32 100 Aaronson Rebecca NaN 2000 Village Green Dr Apt 12 NaN Mill Creek WA 980125787 100.00 2008-02-08 34 106 Aaronson Rebecca NaN 2000 Village Green Dr Apt 12 NaN Mill Creek WA 980125787 100.00 2008-02-14 34 145 ABDELLA THOMAS M. 4231 MONUMENT WALL WAY #340 NaN FAIRFAX VA 220308440 50.00 2007-09-30 35 And you can chain queries thus. In [63]: dfcwci . query ( \"10 <= amount <= 50\" ) . head ( 10 ) Out[63]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 2 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 18 Ardle William NaN 412 Dakota Avenue NaN Springfield OH 45504 50.0 2007-06-28 16 25 Buckler Steve NaN 24351 Armada Dr NaN Dana Point CA 926291306 50.0 2007-07-30 20 26 Buckler Steve NaN 24351 Armada Dr NaN Dana Point CA 926291306 25.0 2007-08-16 20 34 Buck Barbara NaN 1780 NE 138th St NaN North Miami FL 331811316 50.0 2007-09-13 20 35 Buck Barbara NaN 1780 NE 138th St NaN North Miami FL 331811316 50.0 2007-07-19 20 38 Buchanek Elizabeth NaN 7917 Kentbury Dr NaN Bethesda MD 208144615 50.0 2007-09-30 20 49 Harrison Ryan NaN 2247 3rd St NaN La Verne CA 917504918 25.0 2007-07-26 20 101 Aarons Elaine NaN 481 Buck Island Rd Apt 17A APT 17A West Yarmouth MA 26733300 25.0 2008-02-26 34 104 Aaron Shirley NaN 101 Cherry Ave NaN Havana FL 323331311 50.0 2008-02-29 34 SORT In [64]: dfcwci . sort_values ( by = \"amount\" ) . head ( 10 ) Out[64]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 90 Kazor Christopher M 707 Spindletree ave NaN Naperville IL 60565 -2592.0 2008-04-21 32 72 BRUNO JOHN NaN 10136 WINDERMERE CHASE BLVD. NaN GOTHA FL 347344707 -2300.0 2008-03-06 22 64 BURKE DONALD J. 12 LOMPOC NaN RANCHO SANTA MARGA CA 926881817 -2300.0 2008-03-11 22 73 BRUNO IRENE NaN 10136 WINDERMERE CHASE BLVD. NaN GOTHA FL 347344707 -2300.0 2008-03-06 22 74 BROWN TIMOTHY J. 26826 MARLOWE COURT NaN STEVENSON RANCH CA 913811020 -2300.0 2008-03-06 22 58 BURTON GLENN M. 4404 CHARLESTON COURT NaN TAMPA FL 336092620 -2300.0 2008-03-05 22 57 BURTON STEVEN G. 9938 DEER CREEK DRIVE NaN TAMPA FL 33647 -2300.0 2008-03-05 22 84 Uihlein Richard NaN 1396 N Waukegan Rd NaN Lake Forest IL 600451147 -2300.0 2008-04-21 32 56 BURTON SUSAN NaN 9338 DEER CREEK DRIVE NaN TAMPA FL 336472286 -2300.0 2008-03-05 22 55 BUSH ERIC NaN P.O. BOX 61046 NaN DENVER CO 802061046 -2300.0 2008-03-06 22 In [65]: dfcwci [ dfcwci . amount < 0 ] Out[65]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 29 Buckel Linda NaN PO Box 683130 NaN Park City UT 840683130 -2300.00 2007-08-14 20 40 Buchanan John NaN 2025 NW 29th Rd NaN Boca Raton FL 334316303 -500.00 2007-09-24 20 43 Buchanan John NaN 2025 NW 29th Rd NaN Boca Raton FL 334316303 -700.00 2007-08-28 20 50 BYNUM HERBERT NaN 332 SUNNYSIDE ROAD NaN TAMPA FL 336177249 -500.00 2008-03-10 22 51 BYINGTON MARGARET E. 2633 MIDDLEBORO LANE N.E. NaN GRAND RAPIDS MI 495061254 -2300.00 2008-03-03 22 52 BYERS BOB A. 13170 TELFAIR AVENUE NaN SYLMAR CA 913423573 -2300.00 2008-03-07 22 53 BYERS AUDREY NaN 2658 LADBROOK WAY NaN THOUSAND OAKS CA 913615073 -200.00 2008-03-07 22 54 BUSH KRYSTIE NaN P.O. BOX 61046 NaN DENVER CO 802061046 -2300.00 2008-03-06 22 55 BUSH ERIC NaN P.O. BOX 61046 NaN DENVER CO 802061046 -2300.00 2008-03-06 22 56 BURTON SUSAN NaN 9338 DEER CREEK DRIVE NaN TAMPA FL 336472286 -2300.00 2008-03-05 22 57 BURTON STEVEN G. 9938 DEER CREEK DRIVE NaN TAMPA FL 33647 -2300.00 2008-03-05 22 58 BURTON GLENN M. 4404 CHARLESTON COURT NaN TAMPA FL 336092620 -2300.00 2008-03-05 22 59 BURKHARDT CRAIG S. 910 15TH STREET N.W. NaN WASHINGTON DC 200052503 -500.00 2008-03-07 22 60 BURKHARDT CRAIG S. 910 15TH STREET N.W. NaN WASHINGTON DC 200052503 -1000.00 2008-03-07 22 61 BURKHARDT BARBARA NaN 910 15TH STREET N.W. NaN WASHINGTON DC 200052503 -500.00 2008-03-07 22 62 BURKE SUZANNE M. 3401 EVANSTON NaN SEATTLE WA 981038677 -700.00 2008-03-05 22 63 BURKE GAIL NaN 165 E. 32ND STREET APARTMENT 9E NEW YORK NY 100166014 -2000.00 2008-03-05 22 64 BURKE DONALD J. 12 LOMPOC NaN RANCHO SANTA MARGA CA 926881817 -2300.00 2008-03-11 22 65 BURGERT RONALD L. 5723 PLUMTREE DRIVE NaN DALLAS TX 752524926 -1000.00 2008-03-05 22 66 BULL BARTLE B. 439 E. 51ST STREET NaN NEW YORK NY 100226473 -800.00 2008-03-10 22 67 BULL BARTLE B. 439 E. 51ST STREET NaN NEW YORK NY 100226473 -1000.00 2008-03-10 22 68 BUKOWSKI DANIEL J. 702 S. WRIGHT STREET NaN NAPERVILLE IL 605406736 -100.00 2008-03-10 22 69 BUISSON MARGARET A. P.O. BOX 197029 NaN LOUISVILLE KY 402597029 -200.00 2008-03-11 22 70 BUCKLEY WALTER W. 1635 COUNTRY ROAD NaN BETHLEHEM PA 180155718 -100.00 2008-03-05 22 71 BUCKLEY MARJORIE B. 1635 COUNTRY ROAD NaN BETHLEHEM PA 180155718 -100.00 2008-03-05 22 72 BRUNO JOHN NaN 10136 WINDERMERE CHASE BLVD. NaN GOTHA FL 347344707 -2300.00 2008-03-06 22 73 BRUNO IRENE NaN 10136 WINDERMERE CHASE BLVD. NaN GOTHA FL 347344707 -2300.00 2008-03-06 22 74 BROWN TIMOTHY J. 26826 MARLOWE COURT NaN STEVENSON RANCH CA 913811020 -2300.00 2008-03-06 22 75 Schuff Bryan NaN 1700 W Sweden Rd NaN Brockport NY 14420 -25.00 2008-08-22 32 76 Hobbs James NaN 229 Cherry Lane NaN White House TN 37188 -25.00 2008-08-19 32 77 Ranganath Anoop NaN 2507 Willard Drive NaN Charlottesville VA 22903 -100.00 2008-04-21 32 78 Nystrom Michael A 93A Fairmont Street NaN Arlington MA 2474 -503.00 2008-04-21 32 79 Muse Nina Jo 2915 Toro Canyon Rd NaN Austin TX 78746 -50.00 2008-04-21 32 80 Waddell James L. 1823 Spel Lane SW NaN Rochester MN 55902 -28.00 2008-04-21 32 81 Brucks William C. PO Box 391 NaN Corona del Mar CA 92625 -150.00 2008-04-21 32 82 Kuehn David NaN 14502 West 93rd Street NaN Lenexa KS 66215 -330.00 2008-04-21 32 83 Verster Jeanette M. 7220 SW 61st St NaN Miami FL 331431807 -1000.00 2008-04-21 32 84 Uihlein Richard NaN 1396 N Waukegan Rd NaN Lake Forest IL 600451147 -2300.00 2008-04-21 32 85 Eskenberry Robert P 10960 Gray Cir NaN Westminster CO 80020 -223.00 2008-04-21 32 86 Froehling Alan L. 302 Broadway St NaN Mount Vernon IL 628645116 -844.80 2008-04-21 32 87 Duryea Marcia A. 123 Bayview Ave NaN Amityville NY 11701 -299.50 2008-04-21 32 88 Perreault Louise NaN 503 Brockridge Hunt Drive NaN Hampton VA 23666 -34.08 2008-04-21 32 89 Rozenfeld Timur NaN 57 Herbert Road NaN Robbinsville NJ 8691 -777.95 2008-04-21 32 90 Kazor Christopher M 707 Spindletree ave NaN Naperville IL 60565 -2592.00 2008-04-21 32 91 Lehner Thomas S. 2701 Star Lane NaN Wadsworth OH 44281 -200.00 2008-04-21 32 92 Plummer Joseph NaN 587 Blake Hill Rd NaN New Hampton NH 32564424 -24.60 2008-04-21 32 93 Raught Philip M 4714 Plum Way NaN Pittsburgh PA 15201 -1046.00 2008-04-21 32 94 Ferrara Judith D 1508 Waterford Road NaN Yardley PA 19067 -1100.00 2008-04-21 32 95 Johnson Cathleen E. 1003 Justin Ln Apt 2016 NaN Austin TX 787572648 -14.76 2008-04-21 32 96 Sanford Bradley NaN 940 Post St #43 NaN San Francisco CA 94109 -24.53 2008-04-21 32 97 Gaarder Bruce NaN PO Box 4085 NaN Mountain Home AFB ID 83648 -261.00 2008-04-21 32 98 Choe Hyeokchan NaN 207 Bridle Way NaN Fort Lee NJ 70246302 -39.50 2008-04-21 32 99 Jacobs Richard G. 14337 Tawya Rd NaN Apple Valley CA 923075545 -1000.00 2008-04-21 32 110 Reid Elizabeth NaN 73 W Patent Rd OPHIR FARM NORTH Bedford Hills NY 105072222 -350.00 2008-08-28 34 111 Reich Thomas NaN 499 Park Ave NaN New York NY 100221240 -2300.00 2008-08-28 34 125 BOURNE TRAVIS NaN LAGE KAART 77 NaN BRASSCHATT NaN 2930 -500.00 2008-11-20 35 126 SECRIST BRIAN L. 3 MULE DEER TRAIL NaN LITTLETON CO 801275722 -1000.00 2008-04-07 35 127 TOLLESTRUP TRAVIS W. 16331 WINECREEK RD. NaN SAN DIEGO CA 92127 -1000.00 2008-05-15 35 In [66]: dfcwci . sort_values ( by = \"amount\" , ascending = False ) . head ( 10 ) Out[66]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 30 Buckel Linda NaN PO Box 683130 NaN Park City UT 840683130 4600.0 2007-08-14 20 159 ABATE MARIA ELENA 1291 NIGHTINGALE AVENUE NaN MIAMI SPRINGS FL 331663832 2600.0 2008-01-25 37 15 Anthony John NaN 211 Long Island Drive NaN Hot Springs AR 71913 2300.0 2007-06-12 16 33 Buck Blaine M 45 Eaton Ave NaN Camden ME 48431752 2300.0 2007-09-30 20 28 Buckel Linda NaN PO Box 683130 NaN Park City UT 840683130 2300.0 2007-08-14 20 21 Baker David NaN 2550 Adamsbrooke Drive NaN Conway AR 72034 2300.0 2007-04-11 16 13 Altes R.D. NaN 8600 Moody Road NaN Fort Smith AR 72903 2300.0 2007-06-21 16 135 ABRAMOWITZ NIRA NaN 411 HARBOR ROAD NaN SOUTHPORT CT 68901376 2300.0 2007-09-14 35 5 Akin Mike NaN 181 Baywood Lane NaN Monticello AR 71655 1500.0 2007-05-18 16 174 ABRAHAM SALEM A. P.O. BOX 7 NaN CANADIAN TX 790140007 1300.0 2008-01-30 37 SELECT-COLUMNS In [67]: dfcwci [[ 'first_name' , 'amount' ]] . head ( 10 ) Out[67]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name amount 0 Steven 500.0 1 Don 250.0 2 Don 50.0 3 Don 100.0 4 Charles 100.0 5 Mike 1500.0 6 Rebecca 500.0 7 Brittni 250.0 8 John D. 1000.0 9 John D. 1300.0 SELECT-DISTINCT Selecting a distinct set is useful for cleaning. Here, we might wish to focus on contributors rather than contributions and see how many distinct contributors we have. Of-course we might be wrong, some people have identical names. In [68]: dfcwci [[ 'last_name' , 'first_name' ]] . count () Out[68]: last_name 175 first_name 175 dtype: int64 In [69]: dfcwci [[ 'last_name' , 'first_name' ]] . drop_duplicates () . count () Out[69]: last_name 126 first_name 126 dtype: int64 In [70]: dfcwci [[ 'last_name' , 'first_name' ]] . drop_duplicates () . head ( 10 ) Out[70]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name 0 Agee Steven 1 Ahrens Don 4 Akin Charles 5 Akin Mike 6 Akin Rebecca 7 Aldridge Brittni 8 Allen John D. 10 Allison John W. 11 Allison Rebecca 13 Altes R.D. ASSIGN Assignment to a new column is easy. In [71]: dfcwci [ 'name' ] = dfcwci [ 'last_name' ] + \", \" + dfcwci [ 'first_name' ] dfcwci . head ( 10 ) Out[71]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id name 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 Agee, Steven 1 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 Ahrens, Don 2 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 Ahrens, Don 3 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 Ahrens, Don 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 Akin, Charles 5 Akin Mike NaN 181 Baywood Lane NaN Monticello AR 71655 1500.0 2007-05-18 16 Akin, Mike 6 Akin Rebecca NaN 181 Baywood Lane NaN Monticello AR 71655 500.0 2007-05-18 16 Akin, Rebecca 7 Aldridge Brittni NaN 808 Capitol Square Place, SW NaN Washington DC 20024 250.0 2007-06-06 16 Aldridge, Brittni 8 Allen John D. NaN 1052 Cannon Mill Drive NaN North Augusta SC 29860 1000.0 2007-06-11 16 Allen, John D. 9 Allen John D. NaN 1052 Cannon Mill Drive NaN North Augusta SC 29860 1300.0 2007-06-29 16 Allen, John D. In [72]: dfcwci . assign ( ucname = dfcwci . last_name + \":\" + dfcwci . first_name ) . head ( 5 ) Out[72]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id name ucname 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 Agee, Steven Agee:Steven 1 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 Ahrens, Don Ahrens:Don 2 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 Ahrens, Don Ahrens:Don 3 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 Ahrens, Don Ahrens:Don 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 Akin, Charles Akin:Charles In [73]: dfcwci . head () Out[73]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id name 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 Agee, Steven 1 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 Ahrens, Don 2 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 Ahrens, Don 3 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 Ahrens, Don 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 Akin, Charles Will the above command actually change dfcwci ? No, it produces a fresh dataframe. What if we wanted to change an existing assignment? In [43]: dfcwci [ dfcwci . state == 'VA' ] Out[43]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id name 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.00 2007-06-30 16 Agee, Steven 27 Buckheit Bruce NaN 8904 KAREN DR NaN FAIRFAX VA 220312731 100.00 2007-09-19 20 Buckheit, Bruce 77 Ranganath Anoop NaN 2507 Willard Drive NaN Charlottesville VA 22903 -100.00 2008-04-21 32 Ranganath, Anoop 88 Perreault Louise NaN 503 Brockridge Hunt Drive NaN Hampton VA 23666 -34.08 2008-04-21 32 Perreault, Louise 145 ABDELLA THOMAS M. 4231 MONUMENT WALL WAY #340 NaN FAIRFAX VA 220308440 50.00 2007-09-30 35 ABDELLA, THOMAS In [44]: dfcwci . loc [ dfcwci . state == 'VA' , 'name' ] Out[44]: 0 Agee, Steven 27 Buckheit, Bruce 77 Ranganath, Anoop 88 Perreault, Louise 145 ABDELLA, THOMAS Name: name, dtype: object In [45]: dfcwci . loc [ dfcwci . state == 'VA' , 'name' ] = \"junk\" In [46]: dfcwci . query ( \"state=='VA'\" ) Out[46]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id name 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.00 2007-06-30 16 junk 27 Buckheit Bruce NaN 8904 KAREN DR NaN FAIRFAX VA 220312731 100.00 2007-09-19 20 junk 77 Ranganath Anoop NaN 2507 Willard Drive NaN Charlottesville VA 22903 -100.00 2008-04-21 32 junk 88 Perreault Louise NaN 503 Brockridge Hunt Drive NaN Hampton VA 23666 -34.08 2008-04-21 32 junk 145 ABDELLA THOMAS M. 4231 MONUMENT WALL WAY #340 NaN FAIRFAX VA 220308440 50.00 2007-09-30 35 junk Drop-Column Real simple: In [47]: del dfcwci [ 'name' ] AGGREGATE In [48]: dfcwci . describe () Out[48]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } zip amount candidate_id count 1.750000e+02 175.000000 175.000000 mean 3.780014e+08 3.418114 28.000000 std 3.628278e+08 1028.418999 7.823484 min 2.474000e+03 -2592.000000 16.000000 25% 9.336700e+04 -175.000000 20.000000 50% 3.233313e+08 100.000000 32.000000 75% 7.816946e+08 300.000000 35.000000 max 9.951532e+08 4600.000000 37.000000 In [49]: dfcwci . amount . max () Out[49]: 4600.0 In [50]: dfcwci [ dfcwci . amount == dfcwci . amount . max ()] Out[50]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 30 Buckel Linda NaN PO Box 683130 NaN Park City UT 840683130 4600.0 2007-08-14 20 In [51]: dfcwci [ dfcwci . amount > dfcwci . amount . max () - 2300 ] Out[51]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 30 Buckel Linda NaN PO Box 683130 NaN Park City UT 840683130 4600.0 2007-08-14 20 159 ABATE MARIA ELENA 1291 NIGHTINGALE AVENUE NaN MIAMI SPRINGS FL 331663832 2600.0 2008-01-25 37 Grouping using Pandas and split-apply-combine In [52]: grouped_by_state = dfcwci . groupby ( \"state\" ) In [53]: grouped_by_state . amount Out[53]: How do we get access to these? Standard pandas functions distribute over the groupby , going one by one over the sub-dataframes or sub-series. This is an example of a paradigm called split-apply-combine. GROUP-AGG The fourth part of the EDA rubric is to look at properties of the sub-dataframes you get when you make groups. (We'll talk about the graphical aspects of this later). For instance, you might group contributions by state: In [54]: dfcwci . groupby ( \"state\" ) . describe () Out[54]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } amount candidate_id zip count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max state AK 3.0 403.333333 100.166528 300.00 355.0000 410.000 455.0000 500.0 3.0 37.000000 0.000000 37.0 37.00 37.0 37.00 37.0 3.0 9.951532e+08 0.000000e+00 995153207.0 9.951532e+08 995153207.0 9.951532e+08 995153207.0 AR 12.0 1183.333333 775.574079 100.00 875.0000 1000.000 1700.0000 2300.0 12.0 16.000000 0.000000 16.0 16.00 16.0 16.00 16.0 12.0 7.206583e+04 4.250251e+02 71603.0 7.165500e+04 72033.5 7.222700e+04 72903.0 AZ 1.0 120.000000 NaN 120.00 120.0000 120.000 120.0000 120.0 1.0 37.000000 NaN 37.0 37.00 37.0 37.00 37.0 1.0 8.600111e+08 NaN 860011121.0 8.600111e+08 860011121.0 8.600111e+08 860011121.0 CA 23.0 -217.988261 942.102438 -2300.00 -175.0000 50.000 225.0000 1000.0 23.0 26.086957 8.151481 16.0 20.00 22.0 34.50 37.0 23.0 6.407113e+08 4.333269e+08 92127.0 9.456600e+04 913811020.0 9.260504e+08 941151435.0 CO 4.0 -1455.750000 1025.166125 -2300.00 -2300.0000 -1650.000 -805.7500 -223.0 4.0 27.750000 6.751543 22.0 22.00 27.0 32.75 35.0 4.0 6.013695e+08 4.008598e+08 80020.0 6.009768e+08 801668384.0 8.020610e+08 802061046.0 CT 1.0 2300.000000 NaN 2300.00 2300.0000 2300.000 2300.0000 2300.0 1.0 35.000000 NaN 35.0 35.00 35.0 35.00 35.0 1.0 6.890138e+07 NaN 68901376.0 6.890138e+07 68901376.0 6.890138e+07 68901376.0 DC 5.0 -309.982000 529.644175 -1000.00 -500.0000 -500.000 200.0900 250.0 5.0 20.400000 2.607681 16.0 20.00 22.0 22.00 22.0 5.0 1.600684e+08 8.946976e+07 20024.0 2.000525e+08 200052503.0 2.000525e+08 200164320.0 FL 30.0 -135.000000 1177.383862 -2300.00 -500.0000 100.000 500.0000 2600.0 30.0 26.766667 7.151963 20.0 20.00 22.0 34.00 37.0 30.0 2.990209e+08 1.015776e+08 33647.0 3.233313e+08 333063809.5 3.343163e+08 347344707.0 IA 1.0 250.000000 NaN 250.00 250.0000 250.000 250.0000 250.0 1.0 16.000000 NaN 16.0 16.00 16.0 16.00 16.0 1.0 5.026600e+04 NaN 50266.0 5.026600e+04 50266.0 5.026600e+04 50266.0 ID 1.0 -261.000000 NaN -261.00 -261.0000 -261.000 -261.0000 -261.0 1.0 32.000000 NaN 32.0 32.00 32.0 32.00 32.0 1.0 8.364800e+04 NaN 83648.0 8.364800e+04 83648.0 8.364800e+04 83648.0 IL 6.0 -931.133333 1230.657981 -2592.00 -1936.2000 -472.400 12.5000 200.0 6.0 29.166667 6.645801 20.0 24.50 32.0 32.00 37.0 6.0 5.070114e+08 2.485720e+08 60565.0 6.005716e+08 603169827.0 6.062808e+08 628645116.0 KS 1.0 -330.000000 NaN -330.00 -330.0000 -330.000 -330.0000 -330.0 1.0 32.000000 NaN 32.0 32.00 32.0 32.00 32.0 1.0 6.621500e+04 NaN 66215.0 6.621500e+04 66215.0 6.621500e+04 66215.0 KY 1.0 -200.000000 NaN -200.00 -200.0000 -200.000 -200.0000 -200.0 1.0 22.000000 NaN 22.0 22.00 22.0 22.00 22.0 1.0 4.025970e+08 NaN 402597029.0 4.025970e+08 402597029.0 4.025970e+08 402597029.0 LA 2.0 650.000000 212.132034 500.00 575.0000 650.000 725.0000 800.0 2.0 37.000000 0.000000 37.0 37.00 37.0 37.00 37.0 2.0 7.030217e+08 4.242415e+06 700021823.0 7.015217e+08 703021663.5 7.045216e+08 706021504.0 MA 6.0 -13.833333 248.197838 -503.00 25.0000 47.500 92.5000 200.0 6.0 34.666667 1.966384 32.0 34.00 34.0 36.25 37.0 6.0 2.050444e+07 1.058572e+07 2474.0 1.975712e+07 25727725.0 2.673330e+07 26733300.0 MD 2.0 150.000000 141.421356 50.00 100.0000 150.000 200.0000 250.0 2.0 27.500000 10.606602 20.0 23.75 27.5 31.25 35.0 2.0 2.081438e+08 1.140563e+03 208143002.0 2.081434e+08 208143808.5 2.081442e+08 208144615.0 ME 4.0 630.000000 1113.522938 50.00 65.0000 85.000 650.0000 2300.0 4.0 30.500000 7.000000 20.0 30.50 34.0 34.00 34.0 4.0 4.141179e+07 4.679973e+06 39071806.0 3.907181e+07 39071806.0 4.141179e+07 48431752.0 MI 5.0 -253.000000 1156.630883 -2300.00 35.0000 200.000 300.0000 500.0 5.0 32.800000 6.220932 22.0 34.00 34.0 37.00 37.0 5.0 4.853947e+08 5.515168e+06 481885097.0 4.818851e+08 483862274.0 4.842798e+08 495061254.0 MN 3.0 107.333333 139.145008 -28.00 36.0000 100.000 175.0000 250.0 3.0 33.333333 1.154701 32.0 33.00 34.0 34.00 34.0 3.0 3.674463e+08 3.181694e+08 55902.0 2.755987e+08 551141508.0 5.511415e+08 551141508.0 MO 1.0 100.000000 NaN 100.00 100.0000 100.000 100.0000 100.0 1.0 20.000000 NaN 20.0 20.00 20.0 20.00 20.0 1.0 6.411100e+04 NaN 64111.0 6.411100e+04 64111.0 6.411100e+04 64111.0 NC 1.0 500.000000 NaN 500.00 500.0000 500.000 500.0000 500.0 1.0 16.000000 NaN 16.0 16.00 16.0 16.00 16.0 1.0 2.750200e+04 NaN 27502.0 2.750200e+04 27502.0 2.750200e+04 27502.0 NH 1.0 -24.600000 NaN -24.60 -24.6000 -24.600 -24.6000 -24.6 1.0 32.000000 NaN 32.0 32.00 32.0 32.00 32.0 1.0 3.256442e+07 NaN 32564424.0 3.256442e+07 32564424.0 3.256442e+07 32564424.0 NJ 2.0 -408.725000 522.163003 -777.95 -593.3375 -408.725 -224.1125 -39.5 2.0 32.000000 0.000000 32.0 32.00 32.0 32.00 32.0 2.0 3.512750e+07 4.966549e+07 8691.0 1.756809e+07 35127496.5 5.268690e+07 70246302.0 NV 4.0 181.250000 213.478141 50.00 68.7500 87.500 200.0000 500.0 4.0 36.000000 1.154701 35.0 35.00 36.0 37.00 37.0 4.0 8.939724e+08 1.883416e+06 891175812.0 8.936894e+08 894810312.5 8.950933e+08 895093326.0 NY 8.0 -809.312500 925.274590 -2300.00 -1250.0000 -575.000 -230.8750 300.0 8.0 29.125000 5.986592 22.0 22.00 32.0 34.00 35.0 8.0 7.576625e+07 4.678602e+07 11701.0 7.512812e+07 100206344.0 1.002265e+08 105072222.0 OH 4.0 112.500000 289.755644 -200.00 -12.5000 75.000 200.0000 500.0 4.0 20.000000 8.000000 16.0 16.00 16.0 20.00 32.0 4.0 4.401775e+04 1.126738e+03 43143.0 4.314300e+04 43712.0 4.458675e+04 45504.0 OK 3.0 266.666667 208.166600 100.00 150.0000 200.000 350.0000 500.0 3.0 34.000000 0.000000 34.0 34.00 34.0 34.00 34.0 3.0 7.341663e+08 5.157607e+06 731188602.0 7.311886e+08 731188602.0 7.356552e+08 740121840.0 PA 5.0 -429.200000 600.635663 -1100.00 -1046.0000 -100.000 -100.0000 200.0 5.0 29.000000 6.708204 22.0 22.00 32.0 32.00 37.0 5.0 1.080998e+08 9.866553e+07 15201.0 1.906700e+04 180153316.0 1.801557e+08 180155718.0 RI 2.0 100.000000 0.000000 100.00 100.0000 100.000 100.0000 100.0 2.0 35.000000 0.000000 35.0 35.00 35.0 35.00 35.0 2.0 2.903295e+07 0.000000e+00 29032946.0 2.903295e+07 29032946.0 2.903295e+07 29032946.0 SC 3.0 800.000000 624.499800 100.00 550.0000 1000.000 1150.0000 1300.0 3.0 23.000000 12.124356 16.0 16.00 16.0 26.50 37.0 3.0 9.873826e+07 1.709680e+08 29860.0 2.986000e+04 29860.0 1.480925e+08 296155069.0 TN 1.0 -25.000000 NaN -25.00 -25.0000 -25.000 -25.0000 -25.0 1.0 32.000000 NaN 32.0 32.00 32.0 32.00 32.0 1.0 3.718800e+04 NaN 37188.0 3.718800e+04 37188.0 3.718800e+04 37188.0 TX 9.0 220.582222 664.483748 -1000.00 -14.7600 50.000 500.0000 1300.0 9.0 33.555556 4.746344 22.0 32.00 35.0 37.00 37.0 9.0 6.912725e+08 2.594552e+08 78746.0 7.735463e+08 775816547.0 7.875726e+08 790140007.0 UT 11.0 459.090909 1716.183000 -2300.00 25.0000 75.000 87.5000 4600.0 11.0 30.909091 7.006490 20.0 27.50 35.0 35.00 35.0 11.0 8.410139e+08 2.208553e+05 840683130.0 8.408522e+08 841176911.0 8.411769e+08 841176911.0 VA 5.0 103.184000 234.748140 -100.00 -34.0800 50.000 100.0000 500.0 5.0 27.000000 8.426150 16.0 20.00 32.0 32.00 35.0 5.0 8.813837e+07 1.206562e+08 22903.0 2.366600e+04 24091.0 2.203084e+08 220312731.0 WA 3.0 -166.666667 461.880215 -700.00 -300.0000 100.000 100.0000 100.0 3.0 30.000000 6.928203 22.0 28.00 34.0 34.00 34.0 3.0 9.804301e+08 5.270573e+05 980125787.0 9.801258e+08 980125787.0 9.805822e+08 981038677.0 In [55]: dfcwci . groupby ( \"state\" ) . sum () Out[55]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } zip amount candidate_id state AK 2985459621 1210.00 111 AR 864790 14200.00 192 AZ 860011121 120.00 37 CA 14736360720 -5013.73 600 CO 2405477834 -5823.00 111 CT 68901376 2300.00 35 DC 800341853 -1549.91 102 FL 8970626520 -4050.00 803 IA 50266 250.00 16 ID 83648 -261.00 32 IL 3042068689 -5586.80 175 KS 66215 -330.00 32 KY 402597029 -200.00 22 LA 1406043327 1300.00 74 MA 123026638 -83.00 208 MD 416287617 300.00 55 ME 165647170 2520.00 122 MI 2426973485 -1265.00 164 MN 1102338918 322.00 100 MO 64111 100.00 20 NC 27502 500.00 16 NH 32564424 -24.60 32 NJ 70254993 -817.45 64 NV 3575889763 725.00 144 NY 606129991 -6474.50 233 OH 176071 450.00 80 OK 2202499044 800.00 102 PA 540499020 -2146.00 145 RI 58065892 200.00 70 SC 296214789 2400.00 69 TN 37188 -25.00 32 TX 6221452245 1985.24 302 UT 9251153394 5050.00 340 VA 440691831 515.92 135 WA 2941290251 -500.00 90 In [56]: mydf = dfcwci . groupby ( \"state\" )[ 'amount' ] . sum () mydf . sort_values () Out[56]: state NY -6474.50 CO -5823.00 IL -5586.80 CA -5013.73 FL -4050.00 PA -2146.00 DC -1549.91 MI -1265.00 NJ -817.45 WA -500.00 KS -330.00 ID -261.00 KY -200.00 MA -83.00 TN -25.00 NH -24.60 MO 100.00 AZ 120.00 RI 200.00 IA 250.00 MD 300.00 MN 322.00 OH 450.00 NC 500.00 VA 515.92 NV 725.00 OK 800.00 AK 1210.00 LA 1300.00 TX 1985.24 CT 2300.00 SC 2400.00 ME 2520.00 UT 5050.00 AR 14200.00 Name: amount, dtype: float64 In [57]: dfcwci . groupby ( \"state\" )[ 'amount' ] . apply ( lambda x : np . std ( x )) Out[57]: state AK 81.785628 AR 742.555647 AZ 0.000000 CA 921.394361 CO 887.819907 CT 0.000000 DC 473.728151 FL 1157.594489 IA 0.000000 ID 0.000000 IL 1123.431895 KS 0.000000 KY 0.000000 LA 150.000000 MA 226.572591 MD 100.000000 ME 964.339152 MI 1034.522112 MN 113.611424 MO 0.000000 NC 0.000000 NH 0.000000 NJ 369.225000 NV 184.877493 NY 865.515126 OH 250.935749 OK 169.967317 PA 537.224869 RI 0.000000 SC 509.901951 TN 0.000000 TX 626.481285 UT 1636.316287 VA 209.965120 WA 377.123617 Name: amount, dtype: float64 The dictionary-like structure is more obvious in this method of iteration, but it does not do the combining part... In [58]: for k , v in dfcwci . groupby ( 'state' ): print ( \"State\" , k , \"mean amount\" , v . amount . mean (), \"std\" , v . amount . std ()) State AK mean amount 403.3333333333333 std 100.16652800877813 State AR mean amount 1183.3333333333333 std 775.574078676661 State AZ mean amount 120.0 std nan State CA mean amount -217.9882608695652 std 942.1024379191479 State CO mean amount -1455.75 std 1025.16612474922 State CT mean amount 2300.0 std nan State DC mean amount -309.98199999999997 std 529.6441745360747 State FL mean amount -135.0 std 1177.3838620520878 State IA mean amount 250.0 std nan State ID mean amount -261.0 std nan State IL mean amount -931.1333333333333 std 1230.6579811900083 State KS mean amount -330.0 std nan State KY mean amount -200.0 std nan State LA mean amount 650.0 std 212.13203435596427 State MA mean amount -13.833333333333334 std 248.19783775582468 State MD mean amount 150.0 std 141.4213562373095 State ME mean amount 630.0 std 1113.5229379466475 State MI mean amount -253.0 std 1156.630883212099 State MN mean amount 107.33333333333333 std 139.14500829470433 State MO mean amount 100.0 std nan State NC mean amount 500.0 std nan State NH mean amount -24.6 std nan State NJ mean amount -408.725 std 522.163002567206 State NV mean amount 181.25 std 213.47814095749163 State NY mean amount -809.3125 std 925.2745900349181 State OH mean amount 112.5 std 289.75564417856185 State OK mean amount 266.6666666666667 std 208.16659994661327 State PA mean amount -429.2 std 600.6356632768321 State RI mean amount 100.0 std 0.0 State SC mean amount 800.0 std 624.4997998398399 State TN mean amount -25.0 std nan State TX mean amount 220.5822222222222 std 664.483747614977 State UT mean amount 459.09090909090907 std 1716.1830004137353 State VA mean amount 103.18400000000001 std 234.74814009912836 State WA mean amount -166.66666666666666 std 461.88021535170066 DELETE In [59]: dfcwci . head () Out[59]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 1 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 2 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 3 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 In-place drops In [60]: df2 = dfcwci . copy () df2 . set_index ( 'last_name' , inplace = True ) df2 . head () Out[60]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name middle_name street_1 street_2 city state zip amount date candidate_id last_name Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 250.0 2007-05-16 16 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 50.0 2007-06-18 16 Ahrens Don NaN 4034 Rennellwood Way NaN Pleasanton CA 94566 100.0 2007-06-21 16 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 In [61]: df2 . drop ([ 'Ahrens' ], inplace = True ) df2 . head () Out[61]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name middle_name street_1 street_2 city state zip amount date candidate_id last_name Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 Akin Mike NaN 181 Baywood Lane NaN Monticello AR 71655 1500.0 2007-05-18 16 Akin Rebecca NaN 181 Baywood Lane NaN Monticello AR 71655 500.0 2007-05-18 16 Aldridge Brittni NaN 808 Capitol Square Place, SW NaN Washington DC 20024 250.0 2007-06-06 16 In [62]: df2 . reset_index ( inplace = True ) df2 . head () Out[62]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 1 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 2 Akin Mike NaN 181 Baywood Lane NaN Monticello AR 71655 1500.0 2007-05-18 16 3 Akin Rebecca NaN 181 Baywood Lane NaN Monticello AR 71655 500.0 2007-05-18 16 4 Aldridge Brittni NaN 808 Capitol Square Place, SW NaN Washington DC 20024 250.0 2007-06-06 16 The recommended way to do it is to create a new dataframe. This might be impractical if things are very large. In [63]: dfcwci = dfcwci [ dfcwci . last_name != 'Ahrens' ] dfcwci . head ( 10 ) Out[63]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 5 Akin Mike NaN 181 Baywood Lane NaN Monticello AR 71655 1500.0 2007-05-18 16 6 Akin Rebecca NaN 181 Baywood Lane NaN Monticello AR 71655 500.0 2007-05-18 16 7 Aldridge Brittni NaN 808 Capitol Square Place, SW NaN Washington DC 20024 250.0 2007-06-06 16 8 Allen John D. NaN 1052 Cannon Mill Drive NaN North Augusta SC 29860 1000.0 2007-06-11 16 9 Allen John D. NaN 1052 Cannon Mill Drive NaN North Augusta SC 29860 1300.0 2007-06-29 16 10 Allison John W. NaN P.O. Box 1089 NaN Conway AR 72033 1000.0 2007-05-18 16 11 Allison Rebecca NaN 3206 Summit Court NaN Little Rock AR 72227 1000.0 2007-04-25 16 12 Allison Rebecca NaN 3206 Summit Court NaN Little Rock AR 72227 200.0 2007-06-12 16 LIMIT In [64]: dfcwci [ 0 : 3 ] # also see loc and iloc from the lab Out[64]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 0 Agee Steven NaN 549 Laurel Branch Road NaN Floyd VA 24091 500.0 2007-06-30 16 4 Akin Charles NaN 10187 Sugar Creek Road NaN Bentonville AR 72712 100.0 2007-06-16 16 5 Akin Mike NaN 181 Baywood Lane NaN Monticello AR 71655 1500.0 2007-05-18 16 Relationships: JOINs are Cartesian Products. Finally, there are many occasions you will want to combine dataframes. We might want to see who contributed to Obama: Simple subselect In [65]: dfcand . head () Out[65]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } id first_name last_name middle_name party 0 33 Joseph Biden NaN D 1 36 Samuel Brownback NaN R 2 34 Hillary Clinton R. D 3 39 Christopher Dodd J. D 4 26 John Edwards NaN D In [66]: obamaid = dfcand . query ( \"last_name=='Obama'\" )[ 'id' ] . values [ 0 ] obamaid Out[66]: 20 In [67]: myq = dfcand [ 'id' ] . values myq Out[67]: array([33, 36, 34, 39, 26, 22, 24, 16, 30, 31, 37, 20, 32, 29, 35, 38, 41]) In [68]: obamacontrib = dfcwci . query ( \"candidate_id== %i \" % obamaid ) obamacontrib . head () Out[68]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name first_name middle_name street_1 street_2 city state zip amount date candidate_id 25 Buckler Steve NaN 24351 Armada Dr NaN Dana Point CA 926291306 50.0 2007-07-30 20 26 Buckler Steve NaN 24351 Armada Dr NaN Dana Point CA 926291306 25.0 2007-08-16 20 27 Buckheit Bruce NaN 8904 KAREN DR NaN FAIRFAX VA 220312731 100.0 2007-09-19 20 28 Buckel Linda NaN PO Box 683130 NaN Park City UT 840683130 2300.0 2007-08-14 20 29 Buckel Linda NaN PO Box 683130 NaN Park City UT 840683130 -2300.0 2007-08-14 20 Explicit INNER JOIN This is the one you will want 90% of the time. It will only match keys that are present in both dataframes. (from http://pandas.pydata.org/pandas-docs/stable/merging.html ) In [69]: cols_wanted = [ 'last_name_x' , 'first_name_x' , 'candidate_id' , 'id' , 'last_name_y' ] dfcwci . merge ( dfcand , left_on = \"candidate_id\" , right_on = \"id\" )[ cols_wanted ] Out[69]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name_x first_name_x candidate_id id last_name_y 0 Agee Steven 16 16 Huckabee 1 Akin Charles 16 16 Huckabee 2 Akin Mike 16 16 Huckabee 3 Akin Rebecca 16 16 Huckabee 4 Aldridge Brittni 16 16 Huckabee 5 Allen John D. 16 16 Huckabee 6 Allen John D. 16 16 Huckabee 7 Allison John W. 16 16 Huckabee 8 Allison Rebecca 16 16 Huckabee 9 Allison Rebecca 16 16 Huckabee 10 Altes R.D. 16 16 Huckabee 11 Andres Dale 16 16 Huckabee 12 Anthony John 16 16 Huckabee 13 Arbogast Robert 16 16 Huckabee 14 Arbogast Robert 16 16 Huckabee 15 Ardle William 16 16 Huckabee 16 Atiq Omar 16 16 Huckabee 17 Atiq Omar 16 16 Huckabee 18 Baker David 16 16 Huckabee 19 Bancroft David 16 16 Huckabee 20 Banks Charles 16 16 Huckabee 21 Barbee John 16 16 Huckabee 22 Buckler Steve 20 20 Obama 23 Buckler Steve 20 20 Obama 24 Buckheit Bruce 20 20 Obama 25 Buckel Linda 20 20 Obama 26 Buckel Linda 20 20 Obama 27 Buckel Linda 20 20 Obama 28 Buck Thomas 20 20 Obama 29 Buck Jay 20 20 Obama ... ... ... ... ... ... 142 ABDELLA THOMAS 35 35 Romney 143 ABBOTT WELDON 35 35 Romney 144 ABBOTT WELDON 35 35 Romney 145 ABBOTT GERALD 35 35 Romney 146 ABBOTT GERALD 35 35 Romney 147 ABEDIN ZAINUL 37 37 McCain 148 ABBOTT SYBIL 37 37 McCain 149 ABBOTT SYBIL 37 37 McCain 150 ABBOTT RONALD 37 37 McCain 151 ABBOTT RONALD 37 37 McCain 152 ABBOTT ROBERT 37 37 McCain 153 ABBOTT MIKE 37 37 McCain 154 ABBOT DAVID 37 37 McCain 155 ABBO PAULINE 37 37 McCain 156 ABATE MARIA 37 37 McCain 157 ABAIR PETER 37 37 McCain 158 ABACHERLI SHIRLEY 37 37 McCain 159 AARONS CHARLES 37 37 McCain 160 AARONS CHARLES 37 37 McCain 161 AARONS CHARLES 37 37 McCain 162 ABEL JOHN 37 37 McCain 163 ABEL MARLING 37 37 McCain 164 ABEL RUDOLPH 37 37 McCain 165 ABELE RODNEY 37 37 McCain 166 ABERCROMBIE DENIS 37 37 McCain 167 ABESHAUS MERRILL 37 37 McCain 168 ABRAHAM GEORGE 37 37 McCain 169 ABRAHAMSON PETER 37 37 McCain 170 ABRAHAM SALEM 37 37 McCain 171 ABRAHAM SALEM 37 37 McCain 172 rows × 5 columns If the names of the columns you wanted to merge on were identical, you could simply say on=id , for example, rather than a left_on and a right_on . Outer JOIN left outer (contributors on candidates) In [70]: dfcwci . merge ( dfcand , left_on = \"candidate_id\" , right_on = \"id\" , how = \"left\" )[ cols_wanted ] Out[70]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name_x first_name_x candidate_id id last_name_y 0 Agee Steven 16 16 Huckabee 1 Akin Charles 16 16 Huckabee 2 Akin Mike 16 16 Huckabee 3 Akin Rebecca 16 16 Huckabee 4 Aldridge Brittni 16 16 Huckabee 5 Allen John D. 16 16 Huckabee 6 Allen John D. 16 16 Huckabee 7 Allison John W. 16 16 Huckabee 8 Allison Rebecca 16 16 Huckabee 9 Allison Rebecca 16 16 Huckabee 10 Altes R.D. 16 16 Huckabee 11 Andres Dale 16 16 Huckabee 12 Anthony John 16 16 Huckabee 13 Arbogast Robert 16 16 Huckabee 14 Arbogast Robert 16 16 Huckabee 15 Ardle William 16 16 Huckabee 16 Atiq Omar 16 16 Huckabee 17 Atiq Omar 16 16 Huckabee 18 Baker David 16 16 Huckabee 19 Bancroft David 16 16 Huckabee 20 Banks Charles 16 16 Huckabee 21 Barbee John 16 16 Huckabee 22 Buckler Steve 20 20 Obama 23 Buckler Steve 20 20 Obama 24 Buckheit Bruce 20 20 Obama 25 Buckel Linda 20 20 Obama 26 Buckel Linda 20 20 Obama 27 Buckel Linda 20 20 Obama 28 Buck Thomas 20 20 Obama 29 Buck Jay 20 20 Obama ... ... ... ... ... ... 142 ABDELLA THOMAS 35 35 Romney 143 ABBOTT WELDON 35 35 Romney 144 ABBOTT WELDON 35 35 Romney 145 ABBOTT GERALD 35 35 Romney 146 ABBOTT GERALD 35 35 Romney 147 ABEDIN ZAINUL 37 37 McCain 148 ABBOTT SYBIL 37 37 McCain 149 ABBOTT SYBIL 37 37 McCain 150 ABBOTT RONALD 37 37 McCain 151 ABBOTT RONALD 37 37 McCain 152 ABBOTT ROBERT 37 37 McCain 153 ABBOTT MIKE 37 37 McCain 154 ABBOT DAVID 37 37 McCain 155 ABBO PAULINE 37 37 McCain 156 ABATE MARIA 37 37 McCain 157 ABAIR PETER 37 37 McCain 158 ABACHERLI SHIRLEY 37 37 McCain 159 AARONS CHARLES 37 37 McCain 160 AARONS CHARLES 37 37 McCain 161 AARONS CHARLES 37 37 McCain 162 ABEL JOHN 37 37 McCain 163 ABEL MARLING 37 37 McCain 164 ABEL RUDOLPH 37 37 McCain 165 ABELE RODNEY 37 37 McCain 166 ABERCROMBIE DENIS 37 37 McCain 167 ABESHAUS MERRILL 37 37 McCain 168 ABRAHAM GEORGE 37 37 McCain 169 ABRAHAMSON PETER 37 37 McCain 170 ABRAHAM SALEM 37 37 McCain 171 ABRAHAM SALEM 37 37 McCain 172 rows × 5 columns right outer (contributors on candidates) = left outer (candidates on contributors) In [71]: dfcwci . merge ( dfcand , left_on = \"candidate_id\" , right_on = \"id\" , how = \"right\" )[ cols_wanted ] Out[71]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name_x first_name_x candidate_id id last_name_y 0 Agee Steven 16.0 16 Huckabee 1 Akin Charles 16.0 16 Huckabee 2 Akin Mike 16.0 16 Huckabee 3 Akin Rebecca 16.0 16 Huckabee 4 Aldridge Brittni 16.0 16 Huckabee 5 Allen John D. 16.0 16 Huckabee 6 Allen John D. 16.0 16 Huckabee 7 Allison John W. 16.0 16 Huckabee 8 Allison Rebecca 16.0 16 Huckabee 9 Allison Rebecca 16.0 16 Huckabee 10 Altes R.D. 16.0 16 Huckabee 11 Andres Dale 16.0 16 Huckabee 12 Anthony John 16.0 16 Huckabee 13 Arbogast Robert 16.0 16 Huckabee 14 Arbogast Robert 16.0 16 Huckabee 15 Ardle William 16.0 16 Huckabee 16 Atiq Omar 16.0 16 Huckabee 17 Atiq Omar 16.0 16 Huckabee 18 Baker David 16.0 16 Huckabee 19 Bancroft David 16.0 16 Huckabee 20 Banks Charles 16.0 16 Huckabee 21 Barbee John 16.0 16 Huckabee 22 Buckler Steve 20.0 20 Obama 23 Buckler Steve 20.0 20 Obama 24 Buckheit Bruce 20.0 20 Obama 25 Buckel Linda 20.0 20 Obama 26 Buckel Linda 20.0 20 Obama 27 Buckel Linda 20.0 20 Obama 28 Buck Thomas 20.0 20 Obama 29 Buck Jay 20.0 20 Obama ... ... ... ... ... ... 152 ABBOTT ROBERT 37.0 37 McCain 153 ABBOTT MIKE 37.0 37 McCain 154 ABBOT DAVID 37.0 37 McCain 155 ABBO PAULINE 37.0 37 McCain 156 ABATE MARIA 37.0 37 McCain 157 ABAIR PETER 37.0 37 McCain 158 ABACHERLI SHIRLEY 37.0 37 McCain 159 AARONS CHARLES 37.0 37 McCain 160 AARONS CHARLES 37.0 37 McCain 161 AARONS CHARLES 37.0 37 McCain 162 ABEL JOHN 37.0 37 McCain 163 ABEL MARLING 37.0 37 McCain 164 ABEL RUDOLPH 37.0 37 McCain 165 ABELE RODNEY 37.0 37 McCain 166 ABERCROMBIE DENIS 37.0 37 McCain 167 ABESHAUS MERRILL 37.0 37 McCain 168 ABRAHAM GEORGE 37.0 37 McCain 169 ABRAHAMSON PETER 37.0 37 McCain 170 ABRAHAM SALEM 37.0 37 McCain 171 ABRAHAM SALEM 37.0 37 McCain 172 NaN NaN NaN 33 Biden 173 NaN NaN NaN 36 Brownback 174 NaN NaN NaN 39 Dodd 175 NaN NaN NaN 26 Edwards 176 NaN NaN NaN 24 Gravel 177 NaN NaN NaN 30 Hunter 178 NaN NaN NaN 31 Kucinich 179 NaN NaN NaN 29 Richardson 180 NaN NaN NaN 38 Tancredo 181 NaN NaN NaN 41 Thompson 182 rows × 5 columns full outer In [72]: dfcwci . merge ( dfcand , left_on = \"candidate_id\" , right_on = \"id\" , how = \"outer\" )[ cols_wanted ] Out[72]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } last_name_x first_name_x candidate_id id last_name_y 0 Agee Steven 16.0 16 Huckabee 1 Akin Charles 16.0 16 Huckabee 2 Akin Mike 16.0 16 Huckabee 3 Akin Rebecca 16.0 16 Huckabee 4 Aldridge Brittni 16.0 16 Huckabee 5 Allen John D. 16.0 16 Huckabee 6 Allen John D. 16.0 16 Huckabee 7 Allison John W. 16.0 16 Huckabee 8 Allison Rebecca 16.0 16 Huckabee 9 Allison Rebecca 16.0 16 Huckabee 10 Altes R.D. 16.0 16 Huckabee 11 Andres Dale 16.0 16 Huckabee 12 Anthony John 16.0 16 Huckabee 13 Arbogast Robert 16.0 16 Huckabee 14 Arbogast Robert 16.0 16 Huckabee 15 Ardle William 16.0 16 Huckabee 16 Atiq Omar 16.0 16 Huckabee 17 Atiq Omar 16.0 16 Huckabee 18 Baker David 16.0 16 Huckabee 19 Bancroft David 16.0 16 Huckabee 20 Banks Charles 16.0 16 Huckabee 21 Barbee John 16.0 16 Huckabee 22 Buckler Steve 20.0 20 Obama 23 Buckler Steve 20.0 20 Obama 24 Buckheit Bruce 20.0 20 Obama 25 Buckel Linda 20.0 20 Obama 26 Buckel Linda 20.0 20 Obama 27 Buckel Linda 20.0 20 Obama 28 Buck Thomas 20.0 20 Obama 29 Buck Jay 20.0 20 Obama ... ... ... ... ... ... 152 ABBOTT ROBERT 37.0 37 McCain 153 ABBOTT MIKE 37.0 37 McCain 154 ABBOT DAVID 37.0 37 McCain 155 ABBO PAULINE 37.0 37 McCain 156 ABATE MARIA 37.0 37 McCain 157 ABAIR PETER 37.0 37 McCain 158 ABACHERLI SHIRLEY 37.0 37 McCain 159 AARONS CHARLES 37.0 37 McCain 160 AARONS CHARLES 37.0 37 McCain 161 AARONS CHARLES 37.0 37 McCain 162 ABEL JOHN 37.0 37 McCain 163 ABEL MARLING 37.0 37 McCain 164 ABEL RUDOLPH 37.0 37 McCain 165 ABELE RODNEY 37.0 37 McCain 166 ABERCROMBIE DENIS 37.0 37 McCain 167 ABESHAUS MERRILL 37.0 37 McCain 168 ABRAHAM GEORGE 37.0 37 McCain 169 ABRAHAMSON PETER 37.0 37 McCain 170 ABRAHAM SALEM 37.0 37 McCain 171 ABRAHAM SALEM 37.0 37 McCain 172 NaN NaN NaN 33 Biden 173 NaN NaN NaN 36 Brownback 174 NaN NaN NaN 39 Dodd 175 NaN NaN NaN 26 Edwards 176 NaN NaN NaN 24 Gravel 177 NaN NaN NaN 30 Hunter 178 NaN NaN NaN 31 Kucinich 179 NaN NaN NaN 29 Richardson 180 NaN NaN NaN 38 Tancredo 181 NaN NaN NaN 41 Thompson 182 rows × 5 columns When to use which? See this: http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/ Useful Links http://sebastianraschka.com/Articles/sqlite3_database.html and http://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html#unique_indexes https://github.com/tthibo/SQL-Tutorial http://chrisalbon.com And especially for R users: https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html https://gist.github.com/TomAugspurger/6e052140eaa5fdb6e8c0/ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lecture2/notebook/"},{"title":"Lab 3: KNN Regression, Simple Linear Regression","text":"Lab 3","tags":"labs","url":"labs/lab3/"},{"title":"Lab 4: Multiple and Polynomial Regression","text":"Lab 4","tags":"labs","url":"labs/lab4/"},{"title":"Lab 5: Regularization and Cross-Validation","text":"Lab 5","tags":"labs","url":"labs/lab5/"},{"title":"Lab 6:","text":"Lab 6","tags":"labs","url":"labs/lab6/"},{"title":"Lab 1: Intro to Python","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109A Introduction to Data Science Lab 1: Introduction to Python and its Numerical Stack Harvard University Fall 2018 Instructors: Pavlos Protopapas and Kevin Rader Lab Instructor: Rahul Dave Authors: Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas In [1]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) print ( 'hello' ) hello Programming Expectations All assignments for this class will use Python and the browser-based iPython notebook format you are currently viewing. Python experience is not a prerequisite for this course, as long as you are comfortable learning on your own as needed. Note though that the programming at the level of CS 50 is a prerequisite for this course. If you have concerns about the prerequisite, please come speak with any of the instructors. We will refer to the Python 3 documentation in this lab and throughout the course. There are also many introductory tutorials to help build programming skills, which we are listed in the last section of this lab. Table of Contents Learning Goals Getting Started Lists Simple Functions Numpy Scipy.stats and plotting distributions Conclusions Additional Stuff Dictionaries Reading CSVs using pandas Learning Goals This introductory lab is a condensed introduction to Python numerical programming. By the end of this lab, you will feel more comfortable: Writing short Python code using functions, loops, lists, numpy arrays, and dictionaries. Manipulating Python lists and numpy arrays and understanding the difference between them. Using probability distributions from scipy.stats Making very simple plots using matplotlib Reading and writing CSV files using pandas Learning and reading Python documentation. Lab 1 relates to material in lecture 0,1,2,3 and homework 0. Part 1: Getting Started Importing modules All notebooks should begin with code that imports modules , collections of built-in, commonly-used Python functions. Below we import the Numpy module, a fast numerical programming library for scientific computing. Future labs will require additional modules, which we'll import with the same import MODULE_NAME as MODULE_NICKNAME syntax. In [2]: import numpy as np #imports a fast numerical programming library Now that Numpy has been imported, we can access some useful functions. For example, we can use mean to calculate the mean of a set of numbers. In [3]: np . mean ([ 1.2 , 2 , 3.3 ]) to calculate the mean of 1.2, 2, and 3.3. The code above is not particularly efficient, and efficiency will be important for you when dealing with large data sets. Later and in lab 2 we will see more efficient options. Calculations and variables In [4]: # // is integer division 1 / 2 , 1 // 2 , 1.0 / 2.0 , 3 * 3.2 The last line in a cell is returned as the output value, as above. For cells with multiple lines of results, we can display results using print , as can be seen below. In [5]: print ( 1 + 3.0 , \" \\n \" , 9 , 7 ) 5 / 3 We can store integer or floating point values as variables. The other basic Python data types -- booleans, strings, lists -- can also be stored as variables. (more on types here: http://www.diveintopython3.net/native-datatypes.html ) In [6]: a = 1 b = 2.0 Here is the storing of a list (more about what a list is later): In [7]: a = [ 1 , 2 , 3 ] Think of a variable as a label for a value, not a box in which you put the value (image taken from Fluent Python by Luciano Ramalho) In [8]: b = a b This DOES NOT create a new copy of a . It merely puts a new label on the memory at a, as can be seen by the following code: In [9]: print ( \"a\" , a ) print ( \"b\" , b ) a [ 1 ] = 7 print ( \"a after change\" , a ) print ( \"b after change\" , b ) Tuples Multiple items on one line in the interface are returned as a tuple , an immutable sequence of Python objects. In [10]: a = 1 b = 2.0 a + a , a - b , b * b , 10 * a We can obtain the type of a variable, and use boolean comparisons to test these types. In [11]: type ( a ) == float In [12]: type ( a ) == int For reference, below are common arithmetic and comparison operations. EXERCISE**: Create a tuple called `tup` with the following seven objects: . The first element is an integer of your choice The second element is a float of your choice The third element is the sum of the first two elements The fourth element is the difference of the first two elements The fifth element is first element divided by the second element Display the output of tup . What is the type of the variable tup ? What happens if you try and chage an item in the tuple? In [13]: # your code here Part 2: Lists Much of Python is based on the notion of a list. In Python, a list is a sequence of items separated by commas, all within square brackets. The items can be integers, floating points, or another type. Unlike in C arrays, items in a Python list can be different types, so Python lists are more versatile than traditional arrays in C or in other languages. Let's start out by creating a few lists. In [14]: empty_list = [] float_list = [ 1. , 3. , 5. , 4. , 2. ] int_list = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] mixed_list = [ 1 , 2. , 3 , 4. , 5 ] print ( empty_list ) print ( int_list ) print ( mixed_list , float_list ) Lists in Python are zero-indexed, as in C. The first entry of the list has index 0, the second has index 1, and so on. In [15]: print ( int_list [ 0 ]) print ( float_list [ 1 ]) What happens if we try to use an index that doesn't exist for that list? Python will complain! In [16]: print ( float_list [ 10 ]) You can find the length of a list using the builtin function len : In [17]: print ( float_list ) len ( float_list ) Indexing on lists And since Python is zero-indexed, the last element of float_list is In [18]: float_list [ len ( float_list ) - 1 ] It is more idiomatic in python to use -1 for the last element, -2 for the second last, and so on In [19]: float_list [ - 1 ] We can use the : operator to access a subset of the list. This is called slicing. In [20]: print ( float_list [ 1 : 5 ]) print ( float_list [ 0 : 2 ]) Below is a summary of list slicing operations: You can slice \"backwards\" as well: In [21]: float_list [: - 2 ] # up to second last In [22]: float_list [: 4 ] # up to but not including 5th element You can also slice with a stride: In [23]: float_list [: 4 : 2 ] # above but skipping every second element We can iterate through a list using a loop. Here's a for loop. In [24]: for ele in float_list : print ( ele ) Or, if we like, we can iterate through a list using the indices using a for loop with in range . This is not idiomatic and is not recommended, but accomplishes the same thing as above. In [25]: for i in range ( len ( float_list )): print ( float_list [ i ]) What if you wanted the index as well? Use the built-in python method enumerate , which can be used to create a list of tuples with each tuple of the form (index, value) . In [26]: for i , ele in enumerate ( float_list ): print ( i , ele ) In [27]: # or make a list from it using the list constructor list ( enumerate ( float_list )) Appending and deleting We can also append items to the end of the list using the + operator or with append . In [28]: float_list + [ . 333 ] In [29]: float_list . append ( . 444 ) In [30]: print ( float_list ) len ( float_list ) Go and run the cell with float_list.append a second time. Then run the next line. What happens? To remove an item from the list, use del. In [31]: del ( float_list [ 2 ]) print ( float_list ) List Comprehensions Lists can be constructed in a compact way using a list comprehension . Here's a simple example. In [32]: squaredlist = [ i * i for i in int_list ] squaredlist And here's a more complicated one, requiring a conditional. In [33]: comp_list1 = [ 2 * i for i in squaredlist if i % 2 == 0 ] print ( comp_list1 ) This is entirely equivalent to creating comp_list1 using a loop with a conditional, as below: In [34]: comp_list2 = [] for i in squaredlist : if i % 2 == 0 : comp_list2 . append ( 2 * i ) print ( comp_list2 ) The list comprehension syntax [expression for item in list if conditional] is equivalent to the syntax for item in list: if conditional: expression Exercise: Build a list that contains every prime number between 1 and 100, in two different ways: Using for loops and conditional if statements. (Stretch Goal) Using a list comprehension. You should be able to do this in one line of code, and it may be helpful to look up the function all in the documentation. In [35]: # your code here In [36]: # your code here Part 4: Simple Functions A function object is a reusable block of code that does a specific task. Functions are all over Python, either on their own or on other objects. To invoke a function func , you call it as func(arguments) . We've seen built-in Python functions and methods. For example, len and print are built-in Python functions. And at the beginning, you called np.mean to calculate the mean of three numbers, where mean is a function in the numpy module and numpy was abbreviated as np . This syntax allows us to have multiple \"mean\" functions in different modules; calling this one as np.mean guarantees that we will pick up numpy's mean function, as opposed to a mean function from a different module. Methods A function that belongs to an object is called a method . By \"object\" here we mean an \"instance\" of a list, or integer, or floating point variable. An example of this is append on an existing list. In other words, a method is a function on an instance of a type of object (also called class , in this case, list type). In [37]: float_list = [ 1.0 , 2.09 , 4.0 , 2.0 , 0.444 ] print ( float_list ) float_list . append ( 56.7 ) float_list User-defined functions We'll now learn to write our own user-defined functions. Below is the syntax for defining a basic function with one input argument and one output. You can also define functions with no input or output arguments, or multiple input or output arguments. def name_of_function(arg): ... return(output) We can write functions with one input and one output argument. Here are two such functions. In [38]: def square ( x ): x_sqr = x * x return ( x_sqr ) def cube ( x ): x_cub = x * x * x return ( x_cub ) square ( 5 ), cube ( 5 ) What if you want to return two variables at a time? The usual way is to return a tuple: In [39]: def square_and_cube ( x ): x_cub = x * x * x x_sqr = x * x return ( x_sqr , x_cub ) square_and_cube ( 5 ) Lambda functions Often we quickly define mathematical functions with a one-line function called a lambda function. Lambda functions are great because they enable us to write functions without having to name them, ie, they're anonymous . No return statement is needed. In [40]: # create an anonymous function and assign it to the variable square square = lambda x : x * x print ( square ( 3 )) hypotenuse = lambda x , y : x * x + y * y ## Same as # def hypotenuse(x, y): # return(x*x + y*y) hypotenuse ( 3 , 4 ) Refactoring using functions In an exercise from Lab 0, you wrote code that generated a list of the prime numbers between 1 and 100. For the excercise below, it may help to revisit that code. Write a function called `isprime` that takes in a positive integer $N$, and determines whether or not it is prime. Return `True` if it's prime and return `False` if it isn't. Then, using a list comprehension and `isprime`, create a list `myprimes` that contains all the prime numbers less than 100. In [41]: # your code here Part 5: Introduction to Numpy Scientific Python code uses a fast array structure, called the numpy array. Those who have worked in Matlab will find this very natural. For reference, the numpy documention can be found here . Let's make a numpy array. In [42]: my_array = np . array ([ 1 , 2 , 3 , 4 ]) my_array In [43]: # works as in lists len ( my_array ) The shape array of an array is very useful (we'll see more of it later when we talk about 2D and higher dimensional arrays). In [44]: my_array . shape Numpy arrays are typed . This means that by default, all the elements will be assumed to be of one type In [45]: my_array . dtype Numpy arrays are listy (i.e. they act like lists)! Below we compute length, slice, and iterate. In [46]: print ( len ( my_array )) print ( my_array [ 2 : 4 ]) for ele in my_array : print ( ele ) In general you should manipulate numpy arrays by using numpy module functions ( np.mean , for example). This is for efficiency purposes, and a discussion about this will happen in Lab2. You can calculate the mean of the array elements either by calling the method .mean on a numpy array or by applying the function np.mean with the numpy array as an argument. In [47]: print ( my_array . mean ()) print ( np . mean ( my_array )) The way we constructed the numpy array above seems redundant. After all we already had a regular python list. Indeed, it is the other ways we have to construct numpy arrays that make them super useful. There are many such numpy array constructors . Here are some commonly used constructors. Look them up in the documentation. In [48]: np . ones ( 10 ) # generates 10 floating point ones Numpy gains a lot of its efficiency from being typed. That is, all elements in the array have the same type, such as integer or floating point. The default type, as can be seen above, is a float of size appropriate for the machine (64 bit on a 64 bit machine). In [49]: np . dtype ( float ) . itemsize # in bytes In [50]: np . ones ( 10 , dtype = 'int' ) # generates 10 integer ones In [51]: np . zeros ( 10 ) Often you will want random numbers. Use the random constructor! In [52]: np . random . random ( 10 ) # uniform on [0,1] You can generate random numbers from a normal distribution with mean 0 and variance 1: In [53]: normal_array = np . random . randn ( 1000 ) print ( \"The sample mean and standard devation are %f and %f , respectively.\" % ( np . mean ( normal_array ), np . std ( normal_array ))) You can sample with and without replacement from an array. Lets first construct a grid: In [54]: grid = np . arange ( 0. , 1.01 , 0.1 ) grid Without replacement In [55]: np . random . choice ( grid , 5 , replace = False ) In [56]: np . random . choice ( grid , 20 , replace = False ) With replacement: In [57]: np . random . choice ( grid , 12 , replace = True ) Numpy supports vector operations What does this mean? It means that instead of adding two arrays, element by element, you can just say: add the two arrays. Note that this behavior is very different from python lists. In [58]: first = np . ones ( 5 ) second = np . ones ( 5 ) first + second In [59]: first_list = [ 1. , 1. , 1. , 1. , 1. ] second_list = [ 1. , 1. , 1. , 1. , 1. ] first_list + second_list #not what u want On some computer chips this addition actually happens in parallel, so speedups can be high. But even on regular chips, the advantage of greater readability is important. Numpy supports a concept known as broadcasting , which dictates how arrays of different sizes are combined together. There are too many rules to list here, but importantly, multiplying an array by a number multiplies each element by the number. Adding a number adds the number to each element. In [60]: first + 1 In [61]: first * 5 This means that if you wanted the distribution $N(5, 7)$ you could do: In [62]: normal_5_7 = 5 + 7 * normal_array np . mean ( normal_5_7 ), np . std ( normal_5_7 ) Multiplying two arrays multiplies them element-by-element In [63]: ( first + 1 ) * ( first * 5 ) You might have wanted to compute the dot product instead: In [64]: np . dot (( first + 1 ) , ( first * 5 )) You can also use the @ operator for this purpose In [65]: ( first + 1 ) @ ( first * 5 ) Part 6: Probabilitiy Distributions from scipy.stats Since we'll be using many distributions, we'll want to access the pdf/pmf functions of these distributions and obtain samples from them. We already saw how to obtain samples from the continuous uniform and normal distributions. But we might want to obtain their pdfs as well. scipy.stats allows us to obtain the pdf function as well as samples. The programming interface is identical for all the distributions. To plot samples from and the pdfs of these distributions, we'll first import matplotlib , python's plotting library. The %matplotlib inline incantation ensures that plots are rendered inline in the browser. In [66]: % matplotlib inline import matplotlib.pyplot as plt Lets get the normal distribution namespace from scipy.stats . Docs here . In [67]: from scipy.stats import norm Lets create 1000 points between -10 and 10 In [68]: x = np . linspace ( - 10 , 10 , 1000 ) x [: 10 ], x [ - 10 :] Lets get the pdf of a normal distribution with a mean of 1 and standard deviation 3 and plot it using the grid points computed before... In [69]: pdf_x = norm . pdf ( x , 1 , 3 ) plt . plot ( x , pdf_x ); And you can get random variables using the rvs function. In [70]: norm . rvs ( size = 30 , loc = 1 , scale = 3 ) We can use a more instance based way of getting both the pdf and samples. The documentation calls this instance a \"frozen\" distribution: In [71]: frozen_norm = norm ( loc = 1 , scale = 3 ) type ( frozen_norm ) In [72]: plt . plot ( x , frozen_norm . pdf ( x )) In [73]: frozen_norm . rvs ( 10 ) We can now plot a histogram of the samples using matplotlib: (see docs on plt.hist by typing ?plt.hist in a cell by itself) In [74]: plt . hist ( frozen_norm . rvs ( 1000 ), bins = 30 ); By default the histogram gives us counts. We can re-normalize these counts to get an approximation to the probability distribution from the samples. In [75]: plt . hist ( frozen_norm . rvs ( 1000 ), bins = 30 , normed = True ); Part 7: Conclusions For more practice exercises (with solutions) and discussion, see this page . Some of these exercises are particularly relevant. Check them out! Don't forget to look up Jake's book . Finally, we would like to suggest using Chris Albon's web site as a reference. Lots of useful information there. Additional Stuff Part 1: Dictionaries A dictionary is another storage container. Like a list, a dictionary is a sequence of items. Unlike a list, a dictionary is unordered and its items are accessed with keys and not integer positions. Dictionaries are the closest container we have to a database. Let's make a dictionary with a few Harvard courses and their corresponding enrollment numbers. In [76]: enroll2017_dict = { 'CS50' : 692 , 'CS109A / Stat 121A / AC 209A' : 352 , 'Econ1011a' : 95 , 'AM21a' : 153 , 'Stat110' : 485 } enroll2017_dict One can obtain the value corrsponding to a key thus: In [77]: enroll2017_dict [ 'CS50' ] Or thus, which allows for the key to not be in the dictionary In [78]: enroll2017_dict . get ( 'CS01' , 5 ), enroll2017_dict . get ( 'CS01' ) In [79]: enroll2017_dict . get ( 'CS50' ) All sorts of iterations are supported: In [80]: enroll2017_dict . values () In [81]: enroll2017_dict . items () We can iterate over the tuples obtained above: (to read more about how the print formatting works , look at https://docs.python.org/3/library/stdtypes.html#old-string-formatting and https://docs.python.org/3/tutorial/inputoutput.html ) In [82]: for key , value in enroll2017_dict . items (): print ( \" %s : %d \" % ( key , value )) Simply iterating over a dictionary gives us the keys. This is useful when we want to do something with each item: In [83]: second_dict = {} for key in enroll2017_dict : second_dict [ key ] = enroll2017_dict [ key ] second_dict The above is an actual copy to another part of memory, unlike, second_dict = enroll2017_dict which would have made both variables label the same memory location. In this example, the keys were strings corresponding to course names. Keys don't have to be strings though. Like lists, you can construct dictionaries using a dictionary comprehension , which is similar to a list comprehension. Notice the brackets {} and the use of zip , which is another iterator that combines two lists together. In [84]: my_dict = { k : v for ( k , v ) in zip ( int_list , float_list )} my_dict You can also create dictionaries using the constructor function dict . In [85]: dict ( a = 1 , b = 2 ) Part 2: Introduction to Pandas Often data is stored in comma separated values (CSV) files. For the remainder of this lab, we'll be working with automobile data , where we've extracted relevant parts below. Note that CSV files can be output by any spreadsheet software, and are plain text, hence are a great way to share data. Importing data with pandas Now let's read in our automobile data as a pandas dataframe structure. In [86]: import pandas as pd In [87]: # Read in the csv files dfcars = pd . read_csv ( \"data/mtcars.csv\" ) type ( dfcars ) In [88]: dfcars . head () What we have now is a spreadsheet with indexed rows and named columns, called a dataframe in pandas. dfcars is an instance of the pd.DataFrame class , created by calling the pd.read_csv \"constructor function\". The take-away is that dfcars is a dataframe object, and it has methods (functions) belonging to it. For example, df.head() is a method that shows the first 5 rows of the dataframe. A pandas dataframe is a set of columns pasted together into a spreadsheet, as shown in the schematic below, which is taken from the cheatsheet above. The columns in pandas are called series objects. Let's look again at the first five rows of dfcars . In [89]: dfcars . head () Notice the poorly named first column: \"Unnamed: 0\". Why did that happen? The first column, which seems to be the name of the car, does not have a name. Here are the first 3 lines of the file: \"\",\"mpg\",\"cyl\",\"disp\",\"hp\",\"drat\",\"wt\",\"qsec\",\"vs\",\"am\",\"gear\",\"carb\" \"Mazda RX4\",21,6,160,110,3.9,2.62,16.46,0,1,4,4 \"Mazda RX4 Wag\",21,6,160,110,3.9,2.875,17.02,0,1,4,4 Lets clean that up: In [90]: dfcars = dfcars . rename ( columns = { \"Unnamed: 0\" : \"name\" }) dfcars . head () In the above, the argument columns = {\"Unnamed: 0\": \"name\"} of rename changed the name of the first column in the dataframe from Unnamed: 0 to name . Lets save this cleaned dataframe out to a CSV file. In [91]: # dont store the 0,1,2,3,4.. index dfcars . to_csv ( \"data/cleaned-mtcars.csv\" , index = False , header = True ) The output will look something like this: name,mpg,cyl,disp,hp,drat,wt,qsec,vs,am,gear,carb Mazda RX4,21.0,6,160.0,110,3.9,2.62,16.46,0,1,4,4 Mazda RX4 Wag,21.0,6,160.0,110,3.9,2.875,17.02,0,1,4,4 Datsun 710,22.8,4,108.0,93,3.85,2.32,18.61,1,1,4,1 Hornet 4 Drive,21.4,6,258.0,110,3.08,3.215,19.44,1,0,3,1 Hornet Sportabout,18.7,8,360.0,175,3.15,3.44,17.02,0,0,3,2 Valiant,18.1,6,225.0,105,2.76,3.46,20.22,1,0,3,1 To access a series (column), you can use either dictionary syntax or instance-variable syntax. Dictionary syntax is very useful when column names have spaces: Python variables cannot have spaces in them. In [92]: dfcars . mpg In [93]: dfcars [ 'mpg' ] You can get a numpy array of values from the Pandas Series: In [94]: dfcars . mpg . values And we can produce a histogram from these values In [95]: # the .values isnt really need, a series behaves like a list for # plotting purposes plt . hist ( dfcars . mpg . values , bins = 20 ); plt . xlabel ( \"mpg\" ); plt . ylabel ( \"Frequency\" ) plt . title ( \"Miles per Gallon\" ); But pandas is very cool: you can get a histogram directly: In [96]: dfcars . mpg . hist ( bins = 20 ); plt . xlabel ( \"mpg\" ); plt . ylabel ( \"Frequency\" ) plt . title ( \"Miles per Gallon\" ); We can also get sub-dataframes by choosing a set of series. We pass a list of the columns we want as \"dictionary keys\" to the dataframe. In [97]: dfcars [[ 'am' , 'mpg' ]] Scatter plots We often want to see co-variation among our columns, for example, miles/gallon versus weight. This can be done with a scatter plot. In [98]: plt . scatter ( dfcars . wt , dfcars . mpg ); plt . xlabel ( \"weight\" ); plt . ylabel ( \"miles per gallon\" ); You could have used plot instead of scatter . In [99]: plt . plot ( dfcars . wt , dfcars . mpg , 'o' ); plt . xlabel ( \"weight\" ); plt . ylabel ( \"miles per gallon\" ); Usually we use plt.show() at the end of every plot to display the plot. Our magical incantation %matplotlib inline takes care of this for us, and we don't have to do it in the Jupyter notebook. But if you run your Python program from a file, you will need to explicitly have a call to show. We include it for completion. In [100]: plt . plot ( dfcars . wt , dfcars . mpg , 'ko' ) #black dots plt . xlabel ( \"weight\" ); plt . ylabel ( \"miles per gallon\" ); plt . show () Suppose we'd like to save a figure to a file. We do this by including the savefig command in the same cell as the plotting commands. The file extension tells you how the file will be saved. In [101]: plt . plot ( dfcars . wt , dfcars . mpg , 'o' ) plt . xlabel ( \"weight\" ); plt . ylabel ( \"miles per gallon\" ); plt . savefig ( 'images/foo1.pdf' ) plt . savefig ( 'images/foo1.png' , bbox_inches = 'tight' ) #less whitespace around image And this is what the saved png looks like. Code in Markdown to show this is: ![](images/foo1.png) Below is a summary of the most commonly used matplotlib plotting routines. Exercise Create a scatter plot showing the co-variation between two columns of your choice. Label the axes. See if you can do this without copying and pasting code from earlier in the lab. What can you conclude, if anything, from your scatter plot? In [102]: # your code here if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab1/notebook/"},{"title":"Lecture 2: Data Engineering","text":"Slides PDF PPTX Notebooks Examples","tags":"lectures","url":"lectures/lecture2/"},{"title":"Lecture 2","text":"Lecture 2","tags":"pages","url":"pages/lecture2/"},{"title":"Lecture 1","text":"Lecture 1","tags":"pages","url":"pages/lecture1/"},{"title":"Lecture 1: Data, Summaries, and Visuals","text":"Slides PDF Notebooks Examples","tags":"lectures","url":"lectures/lecture1/"}]}