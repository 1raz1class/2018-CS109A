var tipuesearch = {"pages":[{"title":"GitHub","text":"All course contents are in: GitHup repo We suggest that you clone this repository.","tags":"pages","url":"pages/github.html"},{"title":"Projects","text":"At end of the semester, students gather in small groups (max 4 per group) and apply what they have learned. Students had the chance to select one of the following 7 projects and show us what they got. Project Guidelines Project 1 | Alzheimer's Disease and Cognitive Impairment Prediction Project 2 | Automatic Playlist Recommender for Spotify Project 3 | Dog Breed Classification Project 4 | Lending Club Investments Project 5 | Predicting the Midterm Election Project 6 | Machine Learning & Analysis for Predicting the 2018 FIFA World Cup Project 7 | Machine Learning & Analysis for Twitter Bot Detection Below is a selection of the most impressive work done by the students, check'em out! Project 1 | Alzheimer's Prediction Project 2 | Spotify Song Recommendation Project Project 3 | Dog Breed Classification Project 4 | Toward Fair AND Profitable Investment Strategies Project 5 | 2018 Midterms Predictive Model Project 6 | Predicting the 2018 FIFA World Cup Results Project 7 | Twitter Bot Detection Thanks everybody for these great results! 109A Team","tags":"pages","url":"pages/projects.html"},{"title":"Schedule","text":"Week Lecture (Mon) Lecture (Wed) Lab (Thu-Fri) Advanced Section (Wed) Assignment (R:Released Tue - D:Due Wed) 1 Lecture 0: What is Data Science? Lab 1: Introduction to Python and its Numerical Stack R:HW0 2 Lecture 1: Data, Summaries and Visuals Lecture 2: Data Engineering - The Grammar of Data Lab 2: Python for Data Collection and Cleaning R:HW1 - D:HW0 3 Lecture 3: Effective Exploratory Data Analysis and Visualization Lecture 4: Linear Regression, kNN Regression and Inference Lab 3: Scikit-learn for Regression Advanced Section 1: Linear Algebra and Hypothesis Testing R:HW2 - D:HW1 4 Lecture 5: Linear Regression, Confidence Intervals and Standard Errors Lecture 6: Multiple Linear Regression, Polynomial Regression and Model Selection Lab 4: Multiple Linear Regression and Polynomial Regression Advanced Section 2: Model Selection and Information Criteria R:HW3 - D:HW2 5 Lecture 7: Regularization Lecture 8: High Dimensionality and Principal Component Analysis (PCA) Lab 5: Regularization and Cross-Validation Advanced Section 3: Methods of Regularization and Justifications R:HW4(individual) D:HW3 6 No Lecture: Columbus Day Lecture 9: Visualization for Communication No Lab No Advanced Section No Assignment 7 Lecture 10: Logistic Regression 1 Lecture 11: Logistic Regression 2 Lab 6: Logistic Regression and Principal Component Analysis Advanced Section 4: Methods of Dimensionality Reduction - Principal Component Analysis R:HW5 - D:HW4 8 Lecture 12: Artificial Neural Networks 1 - Perceptron and Back Propagation Lecture 13: k-NN for Classification and Dealing with Missingness Lab 7: NumPy for Building Artificial Neural Networks and Dealing with Missing Values Advanced Section 5: Generalized Linear Models, Logistic Regression and Beyond R:HW6 - D:HW5 9 Lecture 14: Discriminant Analysis - Linear and Quadratic (LDA/QDA) Lecture 15: Classification Trees Lab 8: Discriminant Analysis [and Classification Trees] Advanced Section 6: Topics in Supervised Classification R:HW7 - D:HW6 10 Lecture 16: Regression Trees, Bagging and Random Forest Lecture 17: Boosting Methods Lab 9: Decision Trees, Bagged Trees, Random Forests and Boosting Advanced Section 7: Decision Trees and Ensemble Methods R:HW8 - D:HW7 11 Lecture 18: Artificial Neural Networks 2 - Anatomy of ANN Lecture 19: Artificial Neural Networks 3 - Regularization Methods for ANN Lab 10: Keras for Artificial Neural Network Advanced Section 8: Artificial Neural Networks for Image Analysis R:HW9(individual) D:HW8 12 Lecture 20: Support Vector Machines No Lecture: Thanksgiving No Lab No Advanced Section No Assignment 13 Lecture 21: Stacking Lecture 22: Responsible Data Science - Guest Lecture (Julia Stoyanovich) No Lab Advanced Section 9: Support Vector Machines D:HW9 14 Lecture 23: A/B Testing Lecture 24: Final Lecture 15 Reading Period 16 Finals Week","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"pre { background-color: #F5F5F5; display: block; font-family: monospace; font-size: 14px; white-space: pre; border-color: #999999; border-width: 1px; border-style: solid; border-radius: 6px; margin: 1em 0; padding: 5px; white-space: pre-wrap; } .containerMain { display: flex; width: 100%; height: 300px; } List of Contents Prerequisites Software Course Activities Recommended Textbook Assignments Getting Help Course Policies Prerequisites You are expected to have programming experience at the level of CS 50 or above, and statistics knowledge at the level of Stat 100 or above (Stat 110 recommended). HW0 is designed to test your knowledge on the prerequisites. Successful completion of this assignment will show that this course is suitable for you. HW0 will not be graded but you are required to submit to enroll in the class. Software We will be using Python 3 run on Jupyter Notebooks. You can access the notebook viewer either in your own machine by installing the Anaconda Platform which includes Jupyter/IPython as well all packages that will be required for the course, or by using the SEAS JupyterHub from Canvas. More details in class. Course Activities The course is structured in three different types of activities that repeat themselves each week and they are: Lectures , Labs , and Sections . Lectures are held on Mon and Wed from 1:30-2:45 pm in Northwest Building, Lecture Hall B-103. Attendance is mandatory for FAS students and for DCE students recorded lectures will be available and must be visioned within 2 days after the lectures. There will be quizzes at the end of each lecture to assess the understanding of the material that will help us identify gaps. Lectures will be recorded and made available real time for DCE students and 24 hours later for in-campus students via Canvas. Labs are held on Thur 4:30-6:00 pm and Fri 10:30-11:45 am in Pierce 301. The two labs have identical contents and you should plan to attend one of the two. Attendance is optional , however it is strongly encouraged . Labs are designed as hands-on activities and are useful to practice with problems similar to the homework. Labs will be videotaped only for distant students. Sections are supplementary activities led by teaching fellows to elaborate upon the lectures material. There are two types of sections: 3a. Standard Sections are held on Monday and will cover a blend of lecture material and practice problems. 3b. Advanced Sections are held on Wednesday and will cover advanced topics about mathematical underpinnings of the methods presented in lecture and lab. The Advanced Section material is required for AC209A students . Recommended Textbook An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani. The book is available here: Free version Hollis Amazon Assignments The final grade will be calculated using the following weights for each assignment: Assignment Final Grade Weight Homework 40% Quizzes 10% Project 50% Total 100% Homework There are 9 homework to complete. There will be an initial self-assessment homework called HW0 and 8 more graded homework assignments. Some of them will be due in a week and some of them in two weeks. You have the option to work and submit the homework in pairs for all the assignments except two which you will do individually. You will be working in Jupyter Notebooks which you can run in your own environment or in the SEAS JupyterHub cloud. The homework are graded on a scale 1 to 5, where 5 is the highest grade. Quizzes There will be a quiz at the end of the class based on what was discussed in lecture. Students will have a limited amount of time to complete the quiz (DCE students will have 72 hours). 40% of the quizzes will be dropped before calculating your final grade. Final Project There will be a final group project (2-4 students) due during Exams period encompassing all the material learned in class. Submitting an Assignment Instructions for turning in assignments will be posted when the semester starts. Grading Guidelines Homework will be graded based on: How correct your code is and whether the cells in your notebook run (we are not troubleshooting code). How you have interpreted the results (we want text not just code). How well you present the results (as you would do in a report). Getting Help For questions about homework, course content, package installation, and after you have tried to troubleshoot yourselves, the process to get help is: Post the question in Piazza and hopefully your peers will answer. Note that in Piazza questions are visible to everyone. The TFs monitor the posts. Go to Office Hours , this is the best way to get help. For private matters send an email to the Helpline: cs109a2018@gmail.com. The Helpline is monitored by TFs. For personal and confidential matters send an email to the instructors . Course Policies Collaboration Policy We encourage you to talk and discuss the assignments with your fellow students (and on Piazza), but you are not allowed to look at any other students assignment or code outside of your pair. Discussion is encouraged, copying is not allowed . Please refer to Academic Honesty in The CS109A Grade linked here The CS109A Grade . Late Day Policy Homework is due on Tuesdays. You are allowed 1 late day per homework for a total of 5 late days. Re-Grading Policy We take great care in making sure all homework are graded properly. However if you feel that your assignment was not fairly graded you may contact the grader by emailing the helpline with subject line \"Regrade HW1: Grader=johnsmith\" within 48 hours of the grade release . If still unhappy with the initial response, then submit a reason via email to the Helpline with subject line \"Regrade HW1: Second request\" within 2 days of receiving the initial response. Important Note: once regrading is done, you may receive a grade that is higher or lower than the initial grade. Communication from Staff to Students Class announcements will be through Canvas . All homework and quizzes will be posted and submitted in Canvas. Also all feedback forms. Important note : make sure you have your settings set so you can receive emails from Canvas. Academic Honesty Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109 we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to Academic Honesty section in The CS109A Grade link above. Accommodations for students with disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with Kevin by the end of the third week of the term: Friday, September 15. Failure to do so may result in us being unable to respond in a timely manner. All discussions will remain confidential.","tags":"pages","url":"pages/syllabus.html"},{"title":"CS109A: Introduction to Data Science","text":"Fall 2018 Pavlos Protopapas and Kevin A. Rader pre { background-color: #F5F5F5; display: block; font-family: monospace; font-size: 14px; white-space: pre; border-color: #999999; border-width: 1px; border-style: solid; border-radius: 6px; margin: 1em 0; padding: 5px; white-space: pre-wrap; } .containerMain { display: flex; width: 100%; height: 300px; } .contentA { flex: 1; flex-direction:column; } .contentB { flex: 3; } Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year course to data science. We will focus on the analysis of data to perform predictions using statistical and machine learning methods. Topics include data scraping, data management, data visualization, regression and classification methods, and deep neural networks (see the schedule ). You will get ample practice through weekly homework assignments. The class material integrates the five key facets of an investigation using data: 1. Data Collection ‐ data wrangling, cleaning, and sampling to get a suitable data set 2. Data Management ‐ accessing data quickly and reliably 3. Exploratory Data Analysis – generating hypotheses and building intuition 4. Prediction or Statistical Learning 5. Communication – summarizing results through visualization, stories, and interpretable summaries Only one of CS 109a, AC 209a, or Stat 121a can be taken for credit. Students who have previously taken CS 109, AC 209, or Stat 121 cannot take CS 109a, AC 209a, or Stat 121a for credit. Lectures: Mon and Wed 1:30‐2:45 pm in Harvard Northwest Building, NW B-103 Labs : Thur 4:30-6:00 pm and Fri 10:30-11:45 am in Pierce 301 (content is identical, students should only attend one) Head TFs : Eleni Kaxiras -DCE Head TF : Sol Girouard Office Hours: IACS student lobby in Maxwell-Dworkin's ground. Just follow the signs. Online Office Hours zoom link: https://harvard-dce.zoom.us/j/7607382317 Class meetings have concluded! Thank you all for a great semester! Guest Lecture on Nov 28th: Ethics and Critical Thinking, Julia Stoyanovich , Assistant Professor in the Department of Computer Science and Engineering at the Tandon School of Engineering, and the Center for Data Science. Course material can be viewed in the public GitHub repository . REGULAR SECTIONS Cover the material presented in class. All 2 sessions are identical. Standard Sections have concluded. Thank you! ADVANCED SECTIONS Cover a different topic per week and are required for 209a students. Advanced Sections have concluded. Thank you! Instructor Office Hours Pavlos : Mon. 4:00-5:00 pm Kevin : Mon. 3:00-4:00 pm. TF Office Hours See the Weekly Schedule Please be aware, that we will not publicly release the homework assignments this year. If you want to follow the course online without registering, you can use the assignments from 2013 and 2014, available at the links below. Additionally, the material from 2015 is also available. Previous Years Material 2017 2015 2014 . 2013","tags":"pages","url":"pages/cs109a-introduction-to-data-science/"},{"title":"Lecture 24: Final Lecture","text":"Slides PDF | Lecture 24: Final Lecture PPTX | Lecture 24: Final Lecture","tags":"lectures","url":"lectures/lecture-24/"},{"title":"Lecture 23: A/B Testing","text":"Slides PDF | Lecture 23: A/B Testing PPTX | Lecture 23: A/B Testing Jupyter Notebook Lecture 23: A/B Testing Demo","tags":"lectures","url":"lectures/lecture-23/"},{"title":"EDA","text":"Contents {:.no_toc} * % matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) Italian Olives I found this data set in the RGGobi book (http://www.ggobi.org/book/), from which the above diagram is taken. It has \"the percentage composition of fatty acids found in the lipid fraction of Italian olive oils', with oils from 3 regions of Italy: the North, the South, and Sardinia. The regions themselves are subdivided into areas as shown in the map above. The source for this data is: Forina, M., Armanino, C., Lanteri, S. & Tiscornia, E. (1983), Classification of Olive Oils from their Fatty Acid Composition, in Martens, H. and Russwurm Jr., H., eds, Food Research and Data Analysis, Applied Science Publishers, London, pp. 189–214. Exploratory Viz df = pd . read_csv ( \"local-olives-cleaned.csv\" ) df . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } areastring region area palmitic palmitoleic stearic oleic linoleic linolenic arachidic eicosenoic regionstring 0 North-Apulia 1 1 10.75 0.75 2.26 78.23 6.72 0.36 0.60 0.29 South 1 North-Apulia 1 1 10.88 0.73 2.24 77.09 7.81 0.31 0.61 0.29 South 2 North-Apulia 1 1 9.11 0.54 2.46 81.13 5.49 0.31 0.63 0.29 South 3 North-Apulia 1 1 9.66 0.57 2.40 79.52 6.19 0.50 0.78 0.35 South 4 North-Apulia 1 1 10.51 0.67 2.59 77.71 6.72 0.50 0.80 0.46 South exploring globally pd . crosstab ( df . areastring , df . regionstring ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } regionstring North Sardinia South areastring Calabria 0 0 56 Coast-Sardinia 0 33 0 East-Liguria 50 0 0 Inland-Sardinia 0 65 0 North-Apulia 0 0 25 Sicily 0 0 36 South-Apulia 0 0 206 Umbria 51 0 0 West-Liguria 50 0 0 pd . value_counts ( df . areastring , sort = False ) . plot ( kind = \"bar\" ); pd . value_counts ( df . regionstring , sort = False ) . plot ( kind = \"barh\" ); acidlist = [ 'palmitic' , 'palmitoleic' , 'stearic' , 'oleic' , 'linoleic' , 'linolenic' , 'arachidic' , 'eicosenoic' ] df [ acidlist ] . median () . plot ( kind = \"bar\" ); Or one can use aggregate to pass an arbitrary function of to the sub-dataframe. The function is applied columnwise. dfbymean = df . groupby ( \"regionstring\" ) . aggregate ( np . mean ) dfbymean . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } region area palmitic palmitoleic stearic oleic linoleic linolenic arachidic eicosenoic regionstring North 3.0 8.006623 10.948013 0.837351 2.308013 77.930530 7.270331 0.217881 0.375762 0.019735 Sardinia 2.0 5.336735 11.113469 0.967449 2.261837 72.680204 11.965306 0.270918 0.731735 0.019388 South 1.0 2.783282 13.322879 1.548019 2.287740 71.000093 10.334985 0.380650 0.631176 0.273220 with sns . axes_style ( \"white\" , { 'grid' : False }): dfbymean [ acidlist ] . plot ( kind = 'barh' , stacked = True ); sns . despine () Figuring the dataset by Region g = sns . FacetGrid ( df , col = \"region\" ) g . map ( plt . scatter , \"eicosenoic\" , \"linoleic\" ); Clearly, region 1 or the South can visually be separated out by eicosenoic fraction itself. with sns . axes_style ( \"white\" ): g = sns . FacetGrid ( df , col = \"region\" ) g . map ( sns . distplot , \"eicosenoic\" ) We make a SPLOM using seaborn to see in what space the regions may be separated. Note that linoleic and oleic seem promising. And perhaps arachidic paired with eicosenoic. sns . pairplot ( df , vars = acidlist , hue = \"regionstring\" , size = 2.5 , diag_kind = 'kde' ); Pandas supports conditional indexing: documentation . Lets use it to follow up on the clear pattern of Southern oils seeeming to be separable by just the eicosenoic feature. Indeed this is the case! Can also be seen using parallel co-ordinates: from pandas.tools.plotting import parallel_coordinates dfna = df [ acidlist ] #normalizing by range dfna_norm = ( dfna - dfna . mean ()) / ( dfna . max () - dfna . min ()) with sns . axes_style ( \"white\" ): parallel_coordinates ( df [[ 'regionstring' ]] . join ( dfna_norm ), 'regionstring' , alpha = 0.3 ) Figuring the South of Italy by Area dfsouth = df [ df . regionstring == 'South' ] dfsouth . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } areastring region area palmitic palmitoleic stearic oleic linoleic linolenic arachidic eicosenoic regionstring 0 North-Apulia 1 1 10.75 0.75 2.26 78.23 6.72 0.36 0.60 0.29 South 1 North-Apulia 1 1 10.88 0.73 2.24 77.09 7.81 0.31 0.61 0.29 South 2 North-Apulia 1 1 9.11 0.54 2.46 81.13 5.49 0.31 0.63 0.29 South 3 North-Apulia 1 1 9.66 0.57 2.40 79.52 6.19 0.50 0.78 0.35 South 4 North-Apulia 1 1 10.51 0.67 2.59 77.71 6.72 0.50 0.80 0.46 South We make a couple of SPLOM's, one with sicily and one without sicily, to see whats separable. Sicily seems to be a problem. As before, see the KDE's first to see if separability exists and then let the eye look for patterns. sns . pairplot ( dfsouth , hue = \"areastring\" , size = 2.5 , vars = acidlist , diag_kind = 'kde' ); sns . pairplot ( dfsouth [ dfsouth . areastring != \"Sicily\" ], hue = \"areastring\" , size = 2.5 , vars = acidlist , diag_kind = 'kde' ); Seems that combinations of oleic, palmitic, palmitoleic might be useful?","tags":"pages","url":"pages/eda/"},{"title":"Lab 11: Italian Olives","text":"title: The Case of the Italian Olives This is the home page Lets have fun here is a quote Here is emph and bold . Here is some inline math $\\alpha = \\frac{\\beta}{\\gamma}$ and, of-course, E rules: $$ G_{\\mu\\nu} + \\Lambda g_{\\mu\\nu} = 8 \\pi T_{\\mu\\nu} . $$","tags":"labs","url":"labs/lab11/"},{"title":"Lab 11 README","text":"gpages Script for the Lab: Preamble Set up a github account Create a Public repository called testgpages with a README.md and a Jekyll .gitignore . Download the github app and clone this repository down. Also fork-clone/clone the repository https://github.com/rahuldave/gpages . Open both repositories side by side in the Finder (File Manager) Make an edit to the README.md in testgpages and commit and push to understand the git flow. Web Pages, Version 1 Open a terminal from the github app for testgpages . Copy the notebooks folder from gpages into testgpages Using that terminal, convert the notebooks to html: jupyter nbconvert --output-dir . --to html notebooks/olives-eda.ipynb jupyter nbconvert --output-dir . --to html notebooks/olives-model.ipynb Copy oldondex.html from gpages to index.html in testgpages Commit and push Go to the hithub user interface for settings and enable a website on the master branch of testgpages . Go to https://username.github.io/testgpages/index.html to see your website Web Site Version 2 Delete the html files in the main testgpages folder. We will generate markdown instead From gpages copy the _layouts folder, index.md , and _config.yml . Using the testgpages terminal, convert the notebooks into markdown using our custom template. jupyter nbconvert --output-dir . --to markdown --template ../gpages/_support/markdown.tpl notebooks/olives-eda.ipynb jupyter nbconvert --output-dir . --to markdown --template ../gpages/_support/markdown.tpl notebooks/olives-model.ipynb Add YAML preambles and some TOC frontmatter to our markdown files python ../gpages/_support/nbmd.py olives-eda.md python ../gpages/_support/nbmd.py olives-model.md Edit the markdown files to add a YAML tag nav_include: 1 and nav_include: 2 respectively to the above markdown files.The 1 and 2 reflect the position on the navigation menu in the default.html template in _layouts . Commit and push everything Go to the website in a bit to check the improvements Things for you to do Hide some cells: see our nbconvert template to see how to do this. Use another theme from the github defaults ADVANCED: use your own jekyll theme. More Info http://jmcglone.com/guides/github-pages/","tags":"labs","url":"labs/lab11/readme/"},{"title":"Models","text":"Contents {:.no_toc} * #!pip install seaborn % matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) df = pd . read_csv ( \"local-olives-cleaned.csv\" ) df . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } areastring region area palmitic palmitoleic stearic oleic linoleic linolenic arachidic eicosenoic regionstring 0 North-Apulia 1 1 10.75 0.75 2.26 78.23 6.72 0.36 0.60 0.29 South 1 North-Apulia 1 1 10.88 0.73 2.24 77.09 7.81 0.31 0.61 0.29 South 2 North-Apulia 1 1 9.11 0.54 2.46 81.13 5.49 0.31 0.63 0.29 South 3 North-Apulia 1 1 9.66 0.57 2.40 79.52 6.19 0.50 0.78 0.35 South 4 North-Apulia 1 1 10.51 0.67 2.59 77.71 6.72 0.50 0.80 0.46 South acidlist = [ 'palmitic' , 'palmitoleic' , 'stearic' , 'oleic' , 'linoleic' , 'linolenic' , 'arachidic' , 'eicosenoic' ] dfsouth = df [ df . regionstring == 'South' ] dfsouth . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } areastring region area palmitic palmitoleic stearic oleic linoleic linolenic arachidic eicosenoic regionstring 0 North-Apulia 1 1 10.75 0.75 2.26 78.23 6.72 0.36 0.60 0.29 South 1 North-Apulia 1 1 10.88 0.73 2.24 77.09 7.81 0.31 0.61 0.29 South 2 North-Apulia 1 1 9.11 0.54 2.46 81.13 5.49 0.31 0.63 0.29 South 3 North-Apulia 1 1 9.66 0.57 2.40 79.52 6.19 0.50 0.78 0.35 South 4 North-Apulia 1 1 10.51 0.67 2.59 77.71 6.72 0.50 0.80 0.46 South Predicting via SVM dfnew = df [[ 'eicosenoic' , 'region' , 'regionstring' ]] dfnew [ 'linoarch' ] = ( 0.969 / 1022.0 ) * df . linoleic + ( 0.245 / 105.0 ) * df . arachidic dfnew . head () eicosenoic region regionstring linoarch 0 0.29 1 South 0.007772 1 0.29 1 South 0.008828 2 0.29 1 South 0.006675 3 0.35 1 South 0.007689 4 0.46 1 South 0.008238 dfnosouth = df [ df . regionstring != 'South' ] dfnosouth . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } areastring region area palmitic palmitoleic stearic oleic linoleic linolenic arachidic eicosenoic regionstring 323 Inland-Sardinia 2 5 11.29 1.20 2.22 72.72 11.12 0.43 0.98 0.02 Sardinia 324 Inland-Sardinia 2 5 10.42 1.35 2.10 73.76 11.16 0.35 0.90 0.03 Sardinia 325 Inland-Sardinia 2 5 11.03 0.96 2.10 73.80 10.85 0.32 0.94 0.03 Sardinia 326 Inland-Sardinia 2 5 11.18 0.97 2.21 72.79 11.54 0.35 0.94 0.02 Sardinia 327 Inland-Sardinia 2 5 10.52 0.95 2.15 73.88 11.26 0.31 0.92 0.01 Sardinia plt . scatter ( dfnosouth . linoleic , dfnosouth . arachidic , c = dfnosouth . region , s = 50 ); from sklearn.cross_validation import train_test_split from sklearn.metrics import confusion_matrix from sklearn.svm import SVC # \"Support Vector Classifier\" def plot_svc_decision_function ( clf , ax = None ): \"\"\"Plot the decision function for a 2D SVC\"\"\" if ax is None : ax = plt . gca () x = np . linspace ( plt . xlim ()[ 0 ], plt . xlim ()[ 1 ], 30 ) y = np . linspace ( plt . ylim ()[ 0 ], plt . ylim ()[ 1 ], 30 ) Y , X = np . meshgrid ( y , x ) P = np . zeros_like ( X ) for i , xi in enumerate ( x ): for j , yj in enumerate ( y ): P [ i , j ] = clf . decision_function ([[ xi , yj ]]) return ax . contour ( X , Y , P , colors = 'k' , levels = [ - 1 , 0 , 1 ], alpha = 0.5 , linestyles = [ '--' , '-' , '--' ]) X = dfnosouth [[ 'linoleic' , 'arachidic' ]] y = ( dfnosouth . regionstring . values == 'Sardinia' ) * 1 Xtrain , Xtest , ytrain , ytest = train_test_split ( X . values , y ) clf = SVC ( kernel = \"linear\" ) clf . fit ( Xtrain , ytrain ) plt . scatter ( Xtrain [:, 0 ], Xtrain [:, 1 ], c = ytrain , s = 50 , cmap = 'spring' , alpha = 0.3 ) plot_svc_decision_function ( clf , plt . gca ()) plt . scatter ( clf . support_vectors_ [:, 0 ], clf . support_vectors_ [:, 1 ], s = 200 , facecolors = 'none' ) plt . scatter ( Xtest [:, 0 ], Xtest [:, 1 ], c = ytest , s = 50 , marker = \"s\" , cmap = 'spring' , alpha = 0.5 ); clf . score ( Xtest , ytest ) 1 . 0 confusion_matrix ( clf . predict ( Xtest ), ytest ) array ([[ 31 , 0 ], [ 0 , 32 ]]) Allowing for crossovers from sklearn.model_selection import GridSearchCV def cv_optimize_svm ( X , y , n_folds = 10 , num_p = 50 ): #clf = SVC() #parameters = {\"C\": np.logspace(-4, 3, num=num_p), \"gamma\": np.logspace(-4, 3, num=10)} clf = SVC ( kernel = \"linear\" , probability = True ) parameters = { \"C\" : np . logspace ( - 4 , 3 , num = num_p )} gs = GridSearchCV ( clf , param_grid = parameters , cv = n_folds ) gs . fit ( X , y ) return gs def get_optim_classifier_svm ( indf , inacidlist , clon , clonval ): subdf = indf [ inacidlist ] subdfstd = ( subdf - subdf . mean ()) / subdf . std () X = subdfstd . values y = ( indf [ clon ] . values == clonval ) * 1 Xtrain , Xtest , ytrain , ytest = train_test_split ( X , y , train_size = 0.8 ) #Xtrain, Xtest, ytrain, ytest=X,X,y,y fitted = cv_optimize_svm ( Xtrain , ytrain ) return fitted , Xtrain , ytrain , Xtest , ytest thesvcfit , Xtr , ytr , Xte , yte = get_optim_classifier_svm ( dfnosouth , [ 'linoleic' , 'arachidic' ], 'regionstring' , \"Sardinia\" ) #thesvcfit, Xtr, ytr, Xte, yte = get_optim_classifier_binary(dfsouthns, ['palmitic','palmitoleic'],'area', 3) thesvcfit . best_estimator_ , thesvcfit . best_params_ , thesvcfit . best_score_ ( SVC ( C = 0 . 071968567300115138 , cache_size = 200 , class_weight = None , coef0 = 0 . 0 , decision_function_shape = None , degree = 3 , gamma = 'auto' , kernel = 'linear' , max_iter =- 1 , probability = True , random_state = None , shrinking = True , tol = 0 . 001 , verbose = False ), { 'C' : 0 . 071968567300115138 } , 1 . 0 ) def plot_svm_new ( clf , Xtr , ytr , Xte , yte ): plt . scatter ( Xtr [:, 0 ], Xtr [:, 1 ], c = ytr , s = 50 , cmap = 'spring' , alpha = 0.5 ) plt . scatter ( Xte [:, 0 ], Xte [:, 1 ], marker = 's' , c = yte , s = 50 , cmap = 'spring' , alpha = 0.5 ) #plt.xlim(-1, 4) #plt.ylim(-1, 6) plot_svc_decision_function ( clf , plt . gca ()) plt . scatter ( clf . support_vectors_ [:, 0 ], clf . support_vectors_ [:, 1 ], s = 100 , facecolors = None , lw = 2 , alpha = 0.4 ) print ( dict ( kernel = \"linear\" , ** thesvcfit . best_params_ )) clsvc = SVC ( ** dict ( kernel = \"linear\" , ** thesvcfit . best_params_ )) . fit ( Xtr , ytr ) plot_svm_new ( clsvc , Xtr , ytr , Xte , yte ) { 'kernel' : 'linear' , 'C' : 0 . 071968567300115138 } The best fit allows for a bigger margin by allowing some inbetween penalization. If we use the standard C=1 in scikit-learn you see that we are allowing for less penalization.","tags":"pages","url":"pages/models/"},{"title":"Advanced Section 9: Support Vector Machine","text":"Slides PDF | Advanced Section 9: Support Vector Machine PPTX | Advanced Section 9: Support Vector Machine Jupyter Notebook Advanced Section 9: Support Vector Machine Demo","tags":"a-sections","url":"a-sections/a-section-9/"},{"title":"Lecture 22: Responsible Data Science","text":"Slides PDF | Lecture 22: Responsible Data Science","tags":"lectures","url":"lectures/lecture-22/"},{"title":"Lecture 21: Stacking","text":"Slides PDF | Lecture 21: Stacking PPTX | Lecture 21: Stacking Jupyter Notebook Lecture 21: Stacking Demo","tags":"lectures","url":"lectures/lecture-21/"},{"title":"Lecture 20: Support Vector Machine (SVM)","text":"Slides PDF | Lecture 20: Support Vector Machine PPTX | Lecture 20: Support Vector Machine Jupuyter Notebook Lecture 20: Support Vector Machine Demo","tags":"lectures","url":"lectures/lecture-20/"},{"title":"Standard Section 9: Artificial Neural Networks Continued","text":"Jupyter Notebooks Standard Section 9: Artificial Neural Networks Continued","tags":"sections","url":"sections/section-9/"},{"title":"Lab 10: Keras for Artificial Neural Network","text":"Associated Materials Notebook | Lab 10: Keras for Artificial Neural Network","tags":"labs","url":"labs/lab-10/"},{"title":"Advanced Section 8: Artificial Neural Networks for Image Analysis","text":"Slides PDF | Artificial Neural Networks for Image Analysis PPTX | Artificial Neural Networks for Image Analysis","tags":"a-sections","url":"a-sections/a-section-8/"},{"title":"Lecture 19: Artificial Neural Networks 3 - Regularization Methods for ANN","text":"Slides PDF | Lecture 19: Artificial Neural Networks 3 - Regularization Methods for ANN PPTX | Lecture 19: Artificial Neural Networks 3 - Regularization Methods for ANN Associated Materials PDF | Lecture 19: Artificial Neural Networks 3 - Optimization PPTX | Lecture 19: Artificial Neural Networks 3 - Optimization Notebook | Standard Section 9: Artificial Neural Networks Continued","tags":"lectures","url":"lectures/lecture-19/"},{"title":"Lecture 18: Artificial Neural Networks 2 - Anatomy of ANN","text":"Slides PDF | Lecture 18: Artificial Neural Networks 2 - Anatomy of NN PPTX | Lecture 18: Artificial Neural Networks 2 - Anatomy of NN Associated Materials Notebook | Standard Section 9: Artificial Neural Networks Continued Notebook | Lab 10: Keras for Artificial Neural Network Notebook | Lab 7: NumPy for Building an Artificial Neural Network and Dealing with Missing Values Notebook | Standard Section 6: Feed Forward Artificial Neural Networks","tags":"lectures","url":"lectures/lecture-18/"},{"title":"Standard Section 8: Bagging and Random Forest","text":"Jupyter Notebooks Standard Section 8: Bagging and Random Forest","tags":"sections","url":"sections/section-8/"},{"title":"Lab 9: Decision Trees, Bagged Trees, Random Forests and Boosting","text":"Jupyter Notebooks Lab 9: Decision Trees, Bagged Trees, Random Forests and Boosting - Student Version Lab 9: Decision Trees, Bagged Trees, Random Forests and Boosting - Solutions","tags":"labs","url":"labs/lab-9/"},{"title":"Advanced Section 7: Decision Trees and Ensemble Methods","text":"Slides PDF | Advanced Section 7: Decision Trees and Ensemble Methods PPTX | Advanced Section 7: Decision Trees and Ensemble Methods","tags":"a-sections","url":"a-sections/a-section-7/"},{"title":"Lecture 17: Boosting Methods","text":"Slides PDF | Lecture 17: Boosting Methods PPTX | Lecture 17: Boosting Methods Jupyter Notebook Lab 9: Decision Trees, Bagged Trees, Random Forests and Boosting","tags":"lectures","url":"lectures/lecture-17/"},{"title":"Lecture 16: Regression Trees, Bagging and Random Forest","text":"Slides PDF | Lecture 16: Regression Trees, Bagging and Random Forest PPTX | Lecture 16: Regression Trees, Bagging and Random Forest Jupyter Notebook Standard Section 8: Bagging and Random Forest Lab 9: Decision Trees, Bagged Trees, Random Forests and Boosting","tags":"lectures","url":"lectures/lecture-16/"},{"title":"Standard Section 7: Multiclass Classification Methods","text":"Jupyter Notebooks Standard Section 7: Multiclass Classification Methods","tags":"sections","url":"sections/section-7/"},{"title":"Lab 8: Discriminant Analysis - A tale of two cities","text":"Jupyter Notebooks Lab 8: Discriminant Analysis - Student Version Lab 8: Discriminant Analysis - Solutions","tags":"labs","url":"labs/lab-8/"},{"title":"Lecture 15: Classification Trees","text":"Slides PDF | Lecture 15: Classification Trees PPTX | Lecture 15: Classification Trees Jupyter Notebook Lecture 15: Decision Trees Demo Standard Section 8: Bagging and Random Forest Lab 9: Decision Trees, Bagged Trees, Random Forests and Boosting","tags":"lectures","url":"lectures/lecture-15/"},{"title":"Advanced Section 6: Topics in Supervised Classification","text":"Lecture Notes PDF | Advanced Section 6: Topics in Supervised Classification","tags":"a-sections","url":"a-sections/a-section-6/"},{"title":"Lecture 14: Discriminant Analysis - Linear and Quadratic (LDA/QDA)","text":"Slides PDF | Lecture 14: Linear and Quadratic Discriminant Analysis PPTX | Lecture 14: Linear and Quadratic Discriminant Analysis Jupyter Notebook Lecture 14: Discriminant Analysis Demo Standard Section 7: Multiclass Classification Lab 8: Discriminant Analysis - A tale of two cities","tags":"lectures","url":"lectures/lecture-14/"},{"title":"Lab 7: NumPy for Building Artificial Neural Network and Dealing with Missing Values","text":"Jupyter Notebooks Prep for Lab 7: Numpy for Tensor and Artificial Neural Networks Lab 7: NumPy for Building an Artificial Neural Network and Dealing with Missing Values","tags":"labs","url":"labs/lab-7/"},{"title":"Standard Section 6: Feed Forward Artificial Neural Networks Demo","text":"Jupyter Notebook Standard Section 6: Feed Forward Artificial Neural Networks - Solutions","tags":"sections","url":"sections/section-6/"},{"title":"Lecture 13: k-NN for Classification and Dealing with Missingness","text":"Slides PDF | Lecture 13: k-NN for Classification and Dealing with Missingness PPTX | Lecture 13: k-NN for Classification and Dealing with Missingness Jupyter Notebooks Lecture 13: Classification with KNN Demo Standard Section 7: Multiclass Classification","tags":"lectures","url":"lectures/lecture-13/"},{"title":"Advanced Section 5: Generalized Linear Models, Logistic Regression and Beyond","text":"Slides PDF | Advanced Section 5: Generalized Linear Models, Logistic Regression and Beyond PPTX | Advanced Section 5: Generalized Linear Models, Logistic Regression and Beyond Section Notes PDF | Advanced Section 5: Generalized Linear Models, Logistic Regression and Beyond","tags":"a-sections","url":"a-sections/a-section-5/"},{"title":"Lecture 12: Artificial Neural Networks 1 - Perceptron and Back Propagation","text":"Slides PDF | Lecture 12: Artificial Neural Networks 1 - Perceptron and Back Propagation PPTX | Lecture 12: Artificial Neural Networks 1 - Perceptron and Back Propagation Jupyter Notebook Preparation for Lab 7: Tensor and Neural Networks Lab 7: NumPy for Building an Artificial Neural Network and Dealing with Missing Values Standard Section 6: Feed Forward Artificial Neural Networks","tags":"lectures","url":"lectures/lecture-12/"},{"title":"Standard Section 5: Logistic Regression and Principal Component Analysis (PCA)","text":"Slides PDF | Standard Section 5: Linear and Logistic Regression Recap Jupyter Notebooks [Standard Section 5: Logistic Regression and Principal Component Analysis (PCA) Solutions]({filename}notebook/solutions/section5_solutions.ipynb)","tags":"sections","url":"sections/section-5/"},{"title":"Lab 6: Classification and Dimensionality Reduction","text":"Jupyter Notebooks Lab 6: Classification and Dimensionality Reduction - Student Version","tags":"labs","url":"labs/lab-6/"},{"title":"Advanced Section 4: Methods of Dimensionality Reduction - Principal Component Analysis","text":"Slides PDF | Advanced Section 4: Methods of Dimensionality Reduction - Principal Component Analysis (PCA) PPTX | Advanced Section 4: Methods of Dimensionality Reduction - Principal Component Analysis (PCA) Section Notes PDF | Advanced Section 4: Methods of Dimensionality Reduction - Principal Component Analysis (PCA)","tags":"a-sections","url":"a-sections/a-section-4/"},{"title":"Lecture 11: Logistic Regression 2","text":"Slides PDF | Lecture 11: Logistic Regression 2 PPTX | Lecture 11: Logistic Regression 2 Jupyter Notebooks Lecture 11: Logistic Regression 2 Demo Lab 6: Classification and Dimensionality Reduction - Student Version","tags":"lectures","url":"lectures/lecture-11/"},{"title":"Lecture 10: Logistic Regression 1","text":"Slides PDF | Lecture 10: Logistic Regression 1 PPTX | Lecture 10: Logistic Regression 1 Jupyter Notebooks Lecture 10: Logistic Regression 1 Demo Standard Section 5: Logistic Regression and Principal Component Analysis (PCA) - Solutions Lab 6: Classification and Dimensionality Reduction - Student Version","tags":"lectures","url":"lectures/lecture-10/"},{"title":"Standard Section 4: Model Selection","text":"Slides HTML | Standard Section 4: Model Selection Jupyter Notebooks Standard Section 4: Model Selection Demo","tags":"sections","url":"sections/section-4/"},{"title":"Lecture 9: Visualization for Communication","text":"Slides PDF | Lecture 9: Visualization for Communication","tags":"lectures","url":"lectures/lecture-9/"},{"title":"Lab 5: Regularization and Cross-Validation","text":"Jupyter Notebooks Lab 5: Regularization and Cross-Validation - Student Version Lab 5: Regularization and Cross-Validation - Solutions","tags":"labs","url":"labs/lab-5/"},{"title":"Advanced Section 3: Methods of Regularization and Justifications","text":"Slides PPTX | Advanced Section 3: Methods of Regularization and Justifications Section Notes PDF | Advanced Section 3: Methods of Regularization and Justifications Jupyter Notebooks Advanced Section 3: Methods of Regularization and Justifications","tags":"a-sections","url":"a-sections/a-section-3/"},{"title":"Lecture 8: High Dimensionality and Principal Component Analysis (PCA)","text":"Slides PDF | Lecture 8: High Dimensionality and Principal Component Analysis (PCA) Associated Materials Notebook | Lecture 8: Principal Component Analysis Demo Notebook | Standard Section 5: Logistic Regression and Principal Component Analysis (PCA) - Solutions Notebook | Lab 6: Classification and Dimensionality Reduction","tags":"lectures","url":"lectures/lecture-8/"},{"title":"Lecture 7: Regularization","text":"Slides PDF | Lecture 7: Regularization PPTX | Lecture 7: Regularization Jupyter Notebooks Lab 5: Regularization and Cross-Validation - Student Version Lab 5: Regularization and Cross-Validation - Solutions","tags":"lectures","url":"lectures/lecture-7/"},{"title":"Standard Section 3: Predictor Types and Feature","text":"Slides PDF | Standard Section 3: Predictor Types and Feature Notebooks Standard Section 3: Predictor Types and Feature - Student Version Standard Section 3: Predictor Types and Feature - Solutions","tags":"sections","url":"sections/section-3/"},{"title":"Lab 4: Multiple and Polynomial Linear Regression","text":"Jupyter Notebooks Lab 4: Multiple Linear Regression and Polynomial Regression - Student Version Lab 4: Multiple Linear Regression and Polynomial Regression - Solutions","tags":"labs","url":"labs/lab-4/"},{"title":"Advanced Section 2: Model Selection and Information Criteria","text":"Slides PDF | Advanced Section 2: Model Selection and Information Criteria PPTX | Advanced Section 2: Model Selection and Information Criteria Section Notes PDF | Advanced Section 2: Model Selection and Information Criteria","tags":"a-sections","url":"a-sections/a-section-2/"},{"title":"Lecture 6: Multiple Linear Regression, Polynomial Regression and Model Selection","text":"Slides PDF | Lecture 6: Multiple Linear Regression, Polynomial Regression and Model Selection PPTX | Lecture 6: Multiple Linear Regression, Polynomial Regression and Model Selection PDF | Standard Section 4: Model selection Jupyter Notebooks Standard Section 4: Model Selection Demo Lab 4: Multiple Linear Regression and Polynomial Regression - Student Version Lab 4: Multiple Linear Regression and Polynomial Regression - Solutions","tags":"lectures","url":"lectures/lecture-6/"},{"title":"Lecture 5: Linear Regression, Confidence Intervals and Standard Errors","text":"Slides PDF | Lecture 5: Linear Regression, Confidence Intervals and Standard Errors PPTX | Lecture 5: Linear Regression, Confidence Intervals and Standard Errors Jupyter Notebooks Standard Section 3: Predictor Types and Feature - Student Version Standard Section 3: Predictor Types and Feature - Solutions Lab 3: Scikit-learn for Regression - Student Version Lab 3: Scikit-learn for Regression - Solutions","tags":"lectures","url":"lectures/lecture-5/"},{"title":"Standard Section 2: Prediction using kNN and Linear Regression","text":"Jupyter Notebooks Standard Section 2: Prediction using kNN and Linear Regression - Student Version Standard Section 2: Prediction using kNN and Linear Regression - Solutions","tags":"sections","url":"sections/section-2/"},{"title":"Lab 3: Scikit-learn for Regression","text":"Jupyter Notebooks Lab 3: Scikit-learn for Regression - Student Version Lab 3: Scikit-learn for Regression - Solutions","tags":"labs","url":"labs/lab-3/"},{"title":"Advanced Section 1: Linear Algebra and Hypothesis Testing","text":"Slides PDF | Advanced Section 1: Linear Algebra and Hypothesis Testing PPTX | Advanced Section 1: Linear Algebra and Hypothesis Testing","tags":"a-sections","url":"a-sections/a-section-1/"},{"title":"Lecture 4: Linear Regression, kNN Regression and Inference","text":"Slides PDF | Lecture 4: Linear Regression, kNN Regression and Inference PPTX | Lecture 4: Linear Regression, kNN Regression and Inference Jupyter Notebooks Lecture 4: Regression Demo Standard Section 2: Prediction using kNN and Linear Regression - Student Version Standard Section 2: Prediction using kNN and Linear Regression - Solutions Lab 3: Scikit-learn for Regression - Student Version Lab 3: Scikit-learn for Regression - Solutions","tags":"lectures","url":"lectures/lecture-4/"},{"title":"Lecture 3: Effective Exploratory Data Analysis and Visualization","text":"Slides PDF | Lecture 3: Effective Exploratory Data Analysis and Visualization Associated Materials Notebook | Lecture 3: EDA and Visualization Demo","tags":"lectures","url":"lectures/lecture-3/"},{"title":"Standard Section 1: Introduction to Web Scraping","text":"Jupyter Notebooks Standard Section 1: Introduction to Web Scraping - Student Version Standard Section 1: Introduction to Web Scraping - Solutions","tags":"sections","url":"sections/section-1/"},{"title":"Lab 2: Python for Data Collection and Cleaning","text":"Notebooks Lab 2: BeautifulSoup for Scraping - Student Version Lab 2: BeautifulSoup for Scraping - Solutions Lab 2: Pandas for Data Cleaning - Student Version Lab 2: Pandas for Data Cleaning - Solutions","tags":"labs","url":"labs/lab-2/"},{"title":"Lecture 2: Data Engineering - The Grammar of Data","text":"Slides PDF | Lecture 2: Data Engineering - The Grammar of Data PPTX | Lecture 2: Data Engineering - The Grammar of Data Jupyter Notebooks Lecture 2: Data Engineering Demo Standard Section 1: Introduction to Web Scraping - Student Version Standard Section 1: Introduction to Web Scraping - Solutions Lab 1: Introduction to Python and its Numerical Stack - Student Version Lab 1: Introduction to Python and its Numerical Stack - Solutions Lab 2: BeautifulSoup for Scraping - Student Version Lab 2: BeautifulSoup for Scraping - Solutions","tags":"lectures","url":"lectures/lecture-2/"},{"title":"Lecture 1: Data, Summaries and Visuals","text":"Slides PDF | Lecture 1: Data, Summaries and Visuals Jupyter Notebooks Standard Section 1: Introduction to Web Scraping - Student Version Standard Section 1: Introduction to Web Scraping - Solutions Lab 1: Introduction to Python and its Numerical Stack - Student Version Lab 1: Introduction to Python and its Numerical Stack - Solutions","tags":"lectures","url":"lectures/lecture-1/"},{"title":"Lab 1: Introduction to Python and its Numerical Stack","text":"Jupyter Notebooks Lab 1: Introduction to Python and its Numerical Stack - Student Version Lab 1: Introduction to Python and its Numerical Stack - Solutions","tags":"labs","url":"labs/lab-1/"},{"title":"Lecture 0: What is Data Science?","text":"Slides PDF | Lecture 0: What is Data Science? Jupyter Notebook Lecture 0: Data Science Demo","tags":"lectures","url":"lectures/lecture-0/"}]}